% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Intro to Statistics Notes},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Intro to Statistics Notes}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{course-details}{%
\chapter*{Course Details}\label{course-details}}
\addcontentsline{toc}{chapter}{Course Details}

\begin{itemize}
\item
  \textbf{Dates:} Mon, Jun 1 -- Mon Jul 13, 2020
\item
  \textbf{Time:} 9:00am-12:00pm
\item
  \textbf{Place:} Online! \url{https://mcgill.zoom.us/j/98513054963}
\item
  \textbf{Course website:} MyCourses
\item
  \textbf{Instructor email:} \href{mailto:nandini.dendukuri@mcgill.ca}{\nolinkurl{nandini.dendukuri@mcgill.ca}}
\item
  \textbf{Assessment}:

  \begin{itemize}
  \tightlist
  \item
    6 assignments: 60\%
  \item
    In-class quizzes: 10\%
  \item
    1 group project: 30\%
  \end{itemize}
\end{itemize}

\hypertarget{project}{%
\section*{Project}\label{project}}
\addcontentsline{toc}{section}{Project}

\begin{itemize}
\tightlist
\item
  Goal is for you to learn the methods covered in this course by applying to a real problem you are familiar with
\item
  You are required to work in groups of 3, identify a dataset, preferably related to your research and analyze it
\item
  At the end of the course, submit a short report and make a 10-minute presentation

  \begin{itemize}
  \tightlist
  \item
    Use reporting guidelines from the Equator network as relevant
  \end{itemize}
\item
  The methods used will naturally limited to those covered during this introductory course. However, you can discuss what other methods would be needed to answer the research question satisfactorily.
\end{itemize}

\hypertarget{suggested-references}{%
\section*{Suggested References}\label{suggested-references}}
\addcontentsline{toc}{section}{Suggested References}

\begin{itemize}
\tightlist
\item
  Many books available for free via McGill libraries, for example:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Biostatistics with R: An Introduction to Statistics through Biological Data}, Babak Shahbaba, Springer, 2012
  \item
    \textbf{A tiny handbook of R}, Mike Allerhand, Springer-Verlag, 2011
  \end{itemize}
\item
  Lecture material for this course is drawn from a variety of sources including:

  \begin{itemize}
  \tightlist
  \item
    Other courses:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Tim Hanson's Course (Univ of South Carolina):} \url{http://people.stat.sc.edu/hansont/stat205/stat205_spring2014.html}
    \item
      \textbf{Ingo Ruczinski's two-term course (Johns Hopkins)} \url{http://www.biostat.jhsph.edu/~iruczins/teaching/140.615/140.615.index.html}
    \item
      \textbf{Lawrence Joseph's EPIB-607 Course (Dept of Epidemiology, Biostatistics and Occupational Health, McGill University)}
      \url{http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-607/main.html}
    \end{itemize}
  \item
    Text books (available via sites like amazon.com):

    \begin{itemize}
    \tightlist
    \item
      \textbf{Statistics for the life sciences}, Samuels, Wittmer and Schaffner, 2016\\
    \item
      \textbf{Statistical data analysis for the life sciences}, Ekstrom and Sorensen, CRC press, 2010
    \item
      \textbf{Data analysis for the life sciences}, Irizarry and Love, Leanpub, 2015
    \item
      \textbf{Statistical ideas and methods}, Utts and Heckard, Thompson Brooks Cole, 2006
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{lecture-1}{%
\chapter{Lecture 1}\label{lecture-1}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{what-is-statistics}{%
\subsection{What is Statistics?}\label{what-is-statistics}}

\emph{Statistics is a collection of procedures and principles for gathering data and analyzing information in order to help people make decisions when faced with uncertainty}\\
- \href{https://www.amazon.ca/Statistical-Methods-Internet-Companion-Statistics/dp/0495122505}{Utts \& Heckard} in `Statistical Ideas \& Methods'

\hypertarget{a-motivating-example}{%
\subsection{A Motivating Example}\label{a-motivating-example}}

\includegraphics[width=0.5\linewidth]{./1_9}

\href{http://www.bbc.com/news/av/technology-41114587/selfie-app-spots-early-signs-of-pancreatic-cancer}{News Clip} \href{https://ubicomplab.cs.washington.edu/pdfs/biliscreen.pdf}{Manuscript}

\begin{itemize}
\tightlist
\item
  Pancreatic cancer has a very poor survival rate because it is often detected too late
\item
  A new app promises to detect early symptoms of jaundice that may go unnoticed typically
\item
  Should this ``test'' be adopted into routine practice?
\end{itemize}

\hypertarget{what-was-the-evidence-behind-this-optimistic-headline}{%
\subsection{What was the evidence behind this optimistic headline?}\label{what-was-the-evidence-behind-this-optimistic-headline}}

\begin{itemize}
\tightlist
\item
  In an initial study the app detected cases of ``concern'' correctly 89.7\% of the time, and classified ``negative'' cases correctly 96.8\% of the time
\item
  The reference test was based on the total serum bilirubin level
\end{itemize}

\hypertarget{what-would-a-data-detective-ask}{%
\subsection{What would a data detective ask?}\label{what-would-a-data-detective-ask}}

\includegraphics[width=0.5\linewidth]{./1_11}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are the statistical methods appropriate?
\item
  Is the study design appropriate?
\item
  Is there information external to the study that affects its interpretation?
\end{enumerate}

\hypertarget{results-reported-in-the-study}{%
\subsection{Results reported in the study}\label{results-reported-in-the-study}}

\begin{tabular}{l|l|l}
\hline
  & Borderline or Elevated Bilirubin & Normal Bilirubin\\
\hline
BiliScreen Positive & 35 (89.7\%) & 1\\
\hline
BiliScreen Negative & 4 & 30 (96.8\%)\\
\hline
Total & 39 & 31\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  The statistics of interest when evaluating a diagnostic test are

  \begin{itemize}
  \tightlist
  \item
    Sensitivity = Probability(Positive result \textbar{} Reference test positive) = 89.7\%
  \item
    Specificity = Probability(Negative result \textbar{} Reference test negative) = 96.8\%
  \end{itemize}
\item
  Do these data provide good estimates of BiliScreen accuracy?
\end{itemize}

\hypertarget{evaluating-the-quality-of-the-statistical-methods}{%
\subsection{Evaluating the quality of the statistical methods}\label{evaluating-the-quality-of-the-statistical-methods}}

\includegraphics[width=0.5\linewidth]{./1_13}

\begin{itemize}
\tightlist
\item
  Is the study large enough?
\item
  What is the uncertainty around the reported results?
\item
  Were relevant statistics recorded?
\item
  Do the statistics provided help make a decision about the next step?
\end{itemize}

\hypertarget{what-if-the-sample-size-were-smaller}{%
\subsection{What if the sample size were smaller?}\label{what-if-the-sample-size-were-smaller}}

\begin{tabular}{l|l|l}
\hline
  & Borderline or Elevated Bilirubin & Normal Bilirubin\\
\hline
BiliScreen Positive & 9 (90\%) & 0\\
\hline
BiliScreen Negative & 1 & 10 (100\%)\\
\hline
Total & 10 & 10\\
\hline
\end{tabular}

\hypertarget{what-if-the-sample-size-were-larger}{%
\subsection{What if the sample size were larger?}\label{what-if-the-sample-size-were-larger}}

\begin{tabular}{l|l|l}
\hline
  & Borderline or Elevated Bilirubin & Normal Bilirubin\\
\hline
BiliScreen Positive & 180 (90\%) & 0\\
\hline
BiliScreen Negative & 20 & 150 (100\%)\\
\hline
Total & 200 & 150\\
\hline
\end{tabular}

\hypertarget{sample-size-and-precision}{%
\subsection{Sample Size and Precision}\label{sample-size-and-precision}}

\includegraphics[width=1\linewidth]{./1_16}

\hypertarget{evaluating-the-quality-of-the-statistical-methods-1}{%
\subsection{Evaluating the quality of the statistical methods}\label{evaluating-the-quality-of-the-statistical-methods-1}}

\begin{itemize}
\tightlist
\item
  Notice that \textbf{the certainty we have in our conclusions depends on the sample size}. The extreme results were less convincing when the sample size was reduced.
\item
  What sample size is needed to draw a definitive conclusion? That needs to be determined using appropriate statistical methods to obtain the desired precision. We will study this in Lectures 3 and 6
\end{itemize}

\hypertarget{evaluating-the-quality-of-the-study-design}{%
\subsection{Evaluating the quality of the study design}\label{evaluating-the-quality-of-the-study-design}}

\begin{itemize}
\tightlist
\item
  Are the subjects in the study representative?
\item
  Is the reference standard relevant?
\item
  Are the subjects in the study representative?

  \begin{itemize}
  \tightlist
  \item
    Healthy volunteers and patients from a medical centre were used
  \item
    If the test accuracy is systematically better or worse in these patients than in patients on whom the test will be used, then the results are biased\\
  \end{itemize}
\item
  Is the reference standard relevant?

  \begin{itemize}
  \tightlist
  \item
    Bilirubin level is a measure of jaundice, but not all cases of jaundice have pancreatic cancer
  \item
    If the accuracy of the test with respect to bilirubin level is systematically different from the accuracy with respect to pancreatic cancer, then our results may be biased
  \end{itemize}
\end{itemize}

\hypertarget{the-role-of-external-or-prior-information}{%
\subsection{The role of external (or prior) information}\label{the-role-of-external-or-prior-information}}

\begin{itemize}
\tightlist
\item
  Besides the sample size and study design, our conclusions may also be affected by information external to the observed results, for example from a previous study
\item
  Statistical analyses should take into account the impact of this prior information. We will study how to do so in Lecture 6
\end{itemize}

\hypertarget{reducing-bias-in-research-studies}{%
\section{Reducing Bias in Research Studies}\label{reducing-bias-in-research-studies}}

\hypertarget{bias-vs.-precision}{%
\subsection{Bias vs.~Precision}\label{bias-vs.-precision}}

\includegraphics[width=0.3\linewidth]{./1_23}

\begin{itemize}
\tightlist
\item
  Precision results in a random departure from the true value
\item
  Bias is a systematic departure from the true value
\item
  A large sample size can improve precision but not bias. Study design and analysis could reduce bias
\end{itemize}

\hypertarget{common-study-designs-used-in-clinical-research}{%
\subsection{Common study designs used in clinical research}\label{common-study-designs-used-in-clinical-research}}

\includegraphics[width=0.3\linewidth]{./1_24}

\begin{itemize}
\tightlist
\item
  An analytical or experimental study can study the relation between an intervention and an outcome
\item
  A descriptive study, with no control group, cannot
\end{itemize}

\hypertarget{randomized-controlled-trial}{%
\subsection{Randomized Controlled Trial}\label{randomized-controlled-trial}}

\begin{itemize}
\tightlist
\item
  Advantages:

  \begin{itemize}
  \tightlist
  \item
    unbiased distribution of confounders;
  \item
    blinding more likely;
  \item
    randomisation facilitates statistical analysis.
  \end{itemize}
\item
  Disadvantages

  \begin{itemize}
  \tightlist
  \item
    expensive: time and money;
  \item
    study subjects not representative;
  \item
    ethically problematic at times.
  \end{itemize}
\end{itemize}

\hypertarget{reducing-bias-in-research-studies-1}{%
\subsection{Reducing bias in research studies}\label{reducing-bias-in-research-studies-1}}

\begin{itemize}
\tightlist
\item
  Different types of bias common in research studies have been enumerated
\end{itemize}

\begin{tabular}{l|l|l}
\hline
Type of bias & Description & Possible Remedial Measures\\
\hline
Selection bias & Sampling method results in sample not representative of the population & Random sampling Statistical modeling\\
\hline
Measurement bias & Measurement method records outcome with systematic error & Statistical modeling\\
\hline
Detection bias & Measurement method differs between groups being compared & Blinding\\
\hline
Confounding & Risk factors distributed unequally in groups being compared & Randomization Statistical modeling\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Statistical methods are often used to reduce bias, either at the planning stage of a study or at the analysis stage
\item
  In this lecture, we will look at random sampling and randomization. In Lecture 12 we will look at adjustment via regression
\end{itemize}

\hypertarget{a-second-motivating-example-renal-denervation}{%
\subsection{A second motivating example: Renal Denervation}\label{a-second-motivating-example-renal-denervation}}

\includegraphics[width=0.5\linewidth]{./1_28}

\href{https://www.terumo.com/about/pressrelease/2013/20130408.html}{Image Source}

\begin{itemize}
\tightlist
\item
  A surgical procedure called ``renal denervation'' was developed to help people with hypertension who do not respond to medication.
\end{itemize}

\hypertarget{example-4a-results-from-a-cohort-study-of-renal-denervation}{%
\subsection{\texorpdfstring{Example 4a: Results from a cohort study of renal denervation\(^*\)}{Example 4a: Results from a cohort study of renal denervation\^{}*}}\label{example-4a-results-from-a-cohort-study-of-renal-denervation}}

\begin{table}
\centering
\begin{tabular}{r|l|r|l}
\hline
\multicolumn{2}{c|}{Baseline} & \multicolumn{2}{c}{3-month follow-up} \\
\cline{1-2} \cline{3-4}
Number of patients & Blood pressure & Number of Patients & Change in blood pressure\\
\hline
153 & 176/98 [systolic/diastolic (mmHg) Mean] & 135 & -25/-11 [systolic/diastolic (mmHg) Mean]\\
\hline
\end{tabular}
\end{table}

\(^*\)Investigators Symplicity HTN-1. Catheter-based renal sympathetic denervation for resistant hypertension: durability of blood pressure reduction out to 24 months. Hypertension 2011;57(5):911-917.

\begin{itemize}
\tightlist
\item
  Can the large observed change be interpreted as being caused by renal denervation?
\item
  This is an example of a before-after design that reports on change over a period of time, typically the change after an intervention.
\item
  The primary drawback of this design is the lack of a control group.
\item
  The observed change may simply be attributable to the participation in the study (`Hawthorne effect'). If so, then the same magnitude of change in the blood pressure would be observed in the control group. This would mean that the change was not due to renal denervation at all.
\item
  Therefore this study cannot provide proof that renal denervation causes a decline in blood pressure.
\item
  Another issue in the data presented here is that the variability around the mean change is not available. So we don't know if all patients experienced this benefit.
\end{itemize}

\hypertarget{example-4b-results-compared-to-a-control-group}{%
\subsection{\texorpdfstring{Example 4b: Results compared to a control group\(^*\)}{Example 4b: Results compared to a control group\^{}*}}\label{example-4b-results-compared-to-a-control-group}}

\includegraphics[width=1\linewidth]{./1_31}

\begin{table}
\centering
\begin{tabular}{l|r|l|r|l}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{Baseline} & \multicolumn{2}{c}{3-month follow-up} \\
\cline{2-3} \cline{4-5}
  & Number of patients & Blood pressure & Number of Patients & Change in blood pressure\\
\hline
Renal Denervation & 45 & 176/98 [systolic/diastolic (mmHg) Mean] & 39 & -21/-10 [systolic/diastolic (mmHg) Mean]\\
\hline
Control group* & 5 & 173/98 [systolic/diastolic (mmHg) Mean] & 3 & +2/+3 [systolic/diastolic (mmHg) Mean]\\
\hline
\end{tabular}
\end{table}

\(^*\)Patients excluded from renal denervation arm for anatomical reasons

\href{https://www-sciencedirect-com.proxy3.library.mcgill.ca/science/article/pii/S0140673609605663}{\(^*\)Catheter-based renal sympathetic denervation for resistant hypertension: a multicentre safety and proof-of-principle cohort study}

\begin{itemize}
\tightlist
\item
  The control group was of patients who were excluded for anatomical reasons.
\item
  It is possible that, the control group may not have had the same risk of resistant hypertension as the treatment group, i.e.~the `anatomical reasons' were a confounding factor. This may explain why the control group had a worse mean change in blood pressure than the renal denervation group
\item
  Therefore, once again, we don't have a conclusive result.
\item
  Of course, the small size of the control group also does not help. Other concerns in this study include loss to follow-up. Only 18 patients completed the follow-up of 24 months.
\end{itemize}

\hypertarget{example-4c-results-from-a-randomized-controlled-trial-rct-of-renal-denervation}{%
\subsection{\texorpdfstring{Example 4c: Results from a randomized controlled trial (RCT) of renal denervation\(^*\)}{Example 4c: Results from a randomized controlled trial (RCT) of renal denervation\^{}*}}\label{example-4c-results-from-a-randomized-controlled-trial-rct-of-renal-denervation}}

\begin{table}
\centering
\begin{tabular}{l|r|l|r|l}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{Baseline} & \multicolumn{2}{c}{6-month follow-up} \\
\cline{2-3} \cline{4-5}
  & Number of patients & Blood pressure & Number of Patients & Change in blood pressure\\
\hline
Renal Denervation & 49 & 178/96 [systolic/diastolic (mmHg) Mean] & 49 & -32/-12 [systolic/diastolic (mmHg) Mean]\\
\hline
Control* & 51 & 178/97 [systolic/diastolic (mmHg) Mean] & 51 & +1/0 [systolic/diastolic (mmHg) Mean]\\
\hline
\end{tabular}
\end{table}

\(^*\)*Esler MD, Krum H, Sobotka PA, Schlaich MP, Schmieder RE, Bohm M. Renal sympathetic denervation in patients with treatment-resistant hypertension (The Symplicity HTN-2 Trial): a randomised controlled trial. Lancet 2010;376(9756):1903-1909

\begin{itemize}
\tightlist
\item
  The study concluded there was a statistically significant (p\textless0.001) difference between the intervention and control groups
\item
  The randomization procedure gives us greater confidence in these results as patients had the same risk of a change in BP at the time of randomization
\item
  However, the study was not perfect. Importantly, it was not blinded and the main outcome was office BP rather than ambulatory BP. Therefore, it is possible that the patients in the renal denervation arm reacted differently owing to the greater attention they received.
\item
  Also, the follow-up of 6-months is very short and it is unknown whether the observed drop in BP is sustained in the long term.
\end{itemize}

\hypertarget{example-4d-results-from-a-second-randomized-controlled-trial-of-renal-denervation}{%
\subsection{\texorpdfstring{Example 4d: Results from a second randomized controlled trial of renal denervation\(^*\)}{Example 4d: Results from a second randomized controlled trial of renal denervation\^{}*}}\label{example-4d-results-from-a-second-randomized-controlled-trial-of-renal-denervation}}

\includegraphics[width=0.5\linewidth]{./1_36}

\begin{itemize}
\tightlist
\item
  ``A significant change from baseline to 6 months in office systolic blood pressure was observed in both study groups.
\end{itemize}

The between-group difference (the primary efficacy end point) did not meet a test of superiority with a margin of 5 mm Hg.

The bars indicate standard deviations.''

\begin{itemize}
\tightlist
\item
  The second RCT improved on the first one by using a sham procedure in the control group. This removed the concern about blinding.
\item
  They found that there was no significant difference between the renal denervation and control groups.
\end{itemize}

\(^*\)Bhatt et al.~A controlled trial of renal denervation for resistant hypertension. N Engl J Med 2014;370:1393-401. DOI: 10.1056/NEJMoa1402670

\hypertarget{example-4-renal-denervation-as-a-treatment-for-resistant-hypertension}{%
\subsection{Example 4: Renal Denervation as a treatment for resistant hypertension}\label{example-4-renal-denervation-as-a-treatment-for-resistant-hypertension}}

\begin{itemize}
\tightlist
\item
  An early study suggested that renal denervation (which uses radiotherapy to destroy some nerves in arteries feeding the kidney) reduces blood pressure. In that experiment, patients who received surgery had an average improvement in systolic blood pressure of 33 mmHg more than did control patients who received no surgery.
\item
  Later an experiment was conducted in which patients were randomly assigned to one of two groups. Patients in the treatment group received the renal denervation surgery. Patients in the control group received a sham operation in which a catheter was inserted, as in the real operation, but 20 minutes later the catheter was removed without radiotherapy being used. These patients had no way of knowing that their operation was a sham. The rates of improvement in the two groups of patients were nearly identical.(Samuels 10-11)
\end{itemize}

\hypertarget{lessons-learnt-from-renal-denervation-example}{%
\subsection{Lessons learnt from renal denervation example}\label{lessons-learnt-from-renal-denervation-example}}

\begin{itemize}
\tightlist
\item
  A control group is necessary to draw conclusions about the effect of a variable
\item
  However, a randomized design is necessary to make a cause-effect conclusion
\item
  A randomized, controlled trial is not automatically unbiased. Blinding is necessary
\end{itemize}

\hypertarget{health-technology-assessment-of-renal-denervation}{%
\subsection{Health Technology Assessment of Renal Denervation}\label{health-technology-assessment-of-renal-denervation}}

\begin{itemize}
\tightlist
\item
  The MUHC's Technology Assessment Unit evaluated Renal Denervation in 2013. The full report is available \href{https://www.mcgill.ca/tau/files/tau/muhc_tau_2013_72_renald.pdf}{here}
\item
  We concluded:\\
  ``\ldots{} There is evidence, based mainly on observational data that this procedure results in a clinically significant reduction in blood pressure at 6 months. Weaker evidence suggests that the effect is sustained up to 2 years of follow-up. Some side-effects, none unmanageable or permanent, are reported.
\end{itemize}

It is recommended that this technology receive temporary (two-year) and conditional approval for use only in the context of a formal research study to be supported by the manufacturer as specified.''

\hypertarget{random-sampling-and-randomization}{%
\section{Random sampling and Randomization}\label{random-sampling-and-randomization}}

\hypertarget{sample-surveys}{%
\subsection{Sample surveys}\label{sample-surveys}}

\begin{itemize}
\tightlist
\item
  A sample survey is a type of observational study
\item
  In a \textbf{sample survey} a subgroup of a larger population is studied. Ideally, we wish to use methods to draw a representative sample to avoid bias
\item
  \textbf{Surveys} are preferred because they are less expensive and time consuming than a census (or complete enumeration of a population)
\end{itemize}

\hypertarget{simple-random-sample}{%
\subsection{Simple random sample}\label{simple-random-sample}}

\begin{itemize}
\tightlist
\item
  A \textbf{simple random sample} is a sample of n items in which

  \begin{itemize}
  \tightlist
  \item
    every member of the population has an equal chance of being included,
  \item
    members are chosen independently from each other
  \end{itemize}
\item
  The word random does not mean haphazard. Rather, it refers to a well-defined process whose outcomes are not fixed but are determined by a probability distribution
\end{itemize}

\hypertarget{sample-surveys-1}{%
\subsection{Sample surveys*}\label{sample-surveys-1}}

\begin{itemize}
\tightlist
\item
  Interestingly, if you use commonly accepted methods, a sample of size 1500 would be adequate to gauge the percentage of a population who have a certain trait or opinion to within ±3\%
\item
  Further, this result does not depend on the size of the population. A sample size of 1500 is adequate whether the population size is 10 million or 4 billion, as long as a proper sampling technique has been used
\end{itemize}

\hypertarget{margin-of-error}{%
\subsection{Margin of error}\label{margin-of-error}}

\begin{itemize}
\tightlist
\item
  An obvious question is: how close is a sample estimate to the true value?
\item
  The central limit theorem (which we will study in Lecture 3) we know that the margin of error around the sample mean is proportional to \(\frac{\sigma}{\sqrt n}\), where \(\sigma\) is the standard deviation and n is the sample size
\end{itemize}

\hypertarget{how-to-choose-a-simple-random-sample}{%
\subsection{How to choose a simple random sample}\label{how-to-choose-a-simple-random-sample}}

\begin{itemize}
\tightlist
\item
  Create a sampling frame by listing all members of the population
\item
  Find a method to randomly select from among these

  \begin{itemize}
  \tightlist
  \item
    e.g.~a physical method, e.g.~placing the names of members of the population in an opaque bowl and drawing the required number
  \item
    e.g.~a virtual method with a computer, e.g.~using the sample() function in R
  \end{itemize}
\item
  The chosen members constitute the sample
\end{itemize}

\hypertarget{example-drawing-a-random-sample}{%
\subsection{Example: Drawing a random sample}\label{example-drawing-a-random-sample}}

\includegraphics[width=0.5\linewidth]{./1_48}

\begin{itemize}
\tightlist
\item
  A respiratory researcher wants to estimate the amount of inflammation in the parenchyma of a mouse lung.
\item
  She takes an image of a histological slide of the lungs of the mouse with staining of the inflammatory cells of interest.
\item
  She divides the images in a grid of 100 rectangular areas, but excludes 10 areas because they include airways.
\item
  She then counts the number of inflammatory cells in 40 areas randomly selected out of the remaining 90 areas
\item
  What was the sampling frame in this study, and how did it differ from the population of interest?
\item
  Explain why ``using the wrong sampling frame'' might lead to a biased estimate.
\item
  Use R to propose to the researcher which rectangular areas she needs to study.
\end{itemize}

\hypertarget{practical-concerns-when-random-sampling}{%
\subsection{Practical concerns when random sampling}\label{practical-concerns-when-random-sampling}}

\begin{itemize}
\tightlist
\item
  For practical reasons, it may not be possible to obtain a simple random sample because it may not be possible to enumerate the entire population

  \begin{itemize}
  \tightlist
  \item
    e.g.~how would we enumerate the population of people who need to be screened by Biliscreen?
  \end{itemize}
\item
  Then, it would be important to identify the population, and scrutinize the method of selection to ensure that the resulting sample satisfies the definition of a simple random sample
\item
  Other sampling techniques such as \textbf{cluster sampling} or \textbf{stratified random sampling} may be easier to implement
\end{itemize}

\hypertarget{some-typical-biases-that-can-arise-during-a-survey}{%
\subsection{Some typical biases that can arise during a survey}\label{some-typical-biases-that-can-arise-during-a-survey}}

\begin{itemize}
\tightlist
\item
  \textbf{Selection bias}: Due to selecting non-representative sample
\item
  \textbf{Non-response (or missingness bias)}: Arises when a representative sample was chosen but a subset could not or did not provide responses, e.g.~a survey conducted during the evening would miss individuals who were working at that time
\item
  \textbf{Response bias}: Occurs when participants respond differently from how they feel, e.g.~response to sensitive questions such as smoking habits
\end{itemize}

\hypertarget{randomization}{%
\subsection{Randomization}\label{randomization}}

\begin{itemize}
\tightlist
\item
  Random sampling can also be used in the context of an experiment, such that each subject has the same probability of receiving the different treatments under study
\item
  Randomization ensures that any observed or unobserved confounding variables have a similar distribution in each treatment group
\end{itemize}

\hypertarget{simple-randomization}{%
\subsection{Simple randomization}\label{simple-randomization}}

\begin{itemize}
\tightlist
\item
  Like with random sampling, there are different techniques we can use to carry out randomization to a treatment group
\item
  In \textbf{simple randomization}, subjects are assigned to groups based on a single sequence of random assignments

  \begin{itemize}
  \tightlist
  \item
    e.g.~If there are two treatments, we can toss a coin to determine how to assign each patient recruited into the study (Heads -- Treatment, Tails -- Control)
  \item
    Instead of a coin you can use a computer to generate the random sequence
  \end{itemize}
\item
  This method is suitable when the planned sample size is relatively large and the subjects to be sampled are relatively homogenous
\end{itemize}

\hypertarget{relevance-of-statistical-methods-to-researchers-in-the-life-sciences}{%
\subsection{Relevance of statistical methods to researchers in the life sciences}\label{relevance-of-statistical-methods-to-researchers-in-the-life-sciences}}

\includegraphics[width=0.5\linewidth]{./1_53}

\href{https://www.nature.com/news/poorly-designed-animal-experiments-in-the-spotlight-1.18559}{Nandini Dendukuri, McGill University}

\begin{itemize}
\tightlist
\item
  Medical research is increasingly quantitative. Simultaneously, there is a move towards evidence-based medicine
\item
  Statistical methods are necessary for designing and analyzing research studies that can answer relevant questions
\item
  Knowledge of statistics is necessary for interpreting research publications
\end{itemize}

\hypertarget{organizations-supporting-transparent-reporting-of-biomedical-research-evidence-based-decision-making}{%
\subsection{Organizations supporting transparent reporting of biomedical research \& evidence-based decision making}\label{organizations-supporting-transparent-reporting-of-biomedical-research-evidence-based-decision-making}}

\includegraphics[width=1\linewidth]{./1_54a}

\includegraphics[width=0.3\linewidth]{./1_54b}

\includegraphics[width=0.3\linewidth]{./1_54c}

\hypertarget{biomedical-journals-are-insisting-on-appropriate-statistical-methods}{%
\subsection{Biomedical journals are insisting on appropriate statistical methods}\label{biomedical-journals-are-insisting-on-appropriate-statistical-methods}}

\includegraphics[width=1\linewidth]{./1_55}
\includegraphics[width=1\linewidth]{./1_56}

\hypertarget{fev-example-dataset}{%
\subsection{FEV Example: Dataset}\label{fev-example-dataset}}

\begin{table}
\centering
\begin{tabular}{r|r|r|r|r|r}
\hline
\multicolumn{6}{c}{First few rows of FEV dataset} \\
\cline{1-6}
id & age & fev & ht & sex & smoke\\
\hline
1 & 9 & 1.708 & 57.0 & 0 & 0\\
\hline
2 & 8 & 1.724 & 67.5 & 0 & 0\\
\hline
3 & 7 & 1.720 & 54.5 & 0 & 0\\
\hline
4 & 9 & 1.558 & 53.0 & 1 & 0\\
\hline
5 & 9 & 1.895 & 57.0 & 1 & 0\\
\hline
6 & 8 & 2.336 & 61.0 & 0 & 0\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  The variables in the dataset include the following:

  \begin{itemize}
  \tightlist
  \item
    fev (in liters)
  \item
    age (in years)
  \item
    height (in inches)
  \item
    gender (M/F)
  \item
    smoke (Y/N)
  \end{itemize}
\end{itemize}

\hypertarget{lecture-2-types-of-variables-probability-and-probability-distributions}{%
\chapter{Lecture 2: Types of Variables, Probability and Probability Distributions}\label{lecture-2-types-of-variables-probability-and-probability-distributions}}

\hypertarget{types-of-variables}{%
\section{Types of variables}\label{types-of-variables}}

\begin{itemize}
\tightlist
\item
  A variable is a characteristic of a person or a thing that can be assigned a number or a category

  \begin{itemize}
  \tightlist
  \item
    Age and sex are two variables that can be measured on a person
  \end{itemize}
\item
  Variables can be of different types
\end{itemize}

\begin{tabular}{l|l}
\hline
Qualitative & Quantitative\\
\hline
Nominal & Continuous\\
\hline
Ordinal & Discrete/Ordinal\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  We need to distinguish between different types of variables because the statistical methods employed -- whether descriptive or inferential - to study them depend on the type of variable we have studied
\end{itemize}

\hypertarget{some-questions-on-types-of-variables}{%
\subsection{Some questions on types of variables}\label{some-questions-on-types-of-variables}}

\begin{itemize}
\tightlist
\item
  Click on this \href{https://forms.office.com/Pages/ResponsePage.aspx?id=cZYxzedSaEqvqfz4-J8J6vtaSwoU68FCgvKfzwN_XcBURFJJUTFWN05JRTJGRzU0WUw3MzIwMlJEUy4u}{link} to answer a few questions on types of variables
\end{itemize}

\hypertarget{qualitative-variables}{%
\subsection{Qualitative variables}\label{qualitative-variables}}

\begin{itemize}
\tightlist
\item
  Qualitative variables are categorical and not measured on a numerical scale

  \begin{itemize}
  \tightlist
  \item
    Nominal variables do not have a particular ordering

    \begin{itemize}
    \tightlist
    \item
      Blood type of a person: A, B, AB, O
    \item
      Sex of a fish: male, female
    \item
      Research interest of students in EXMD 634:
      Cell Biology, Immunology, Cancer, \ldots{}
    \end{itemize}
  \end{itemize}
\item
  Qualitative variables are categorical and not measured on a numerical scale

  \begin{itemize}
  \tightlist
  \item
    Ordinal variables do have a particular ordering, but the gap between successive categories is not measureable and may not be equal

    \begin{itemize}
    \tightlist
    \item
      Likert-type scale:
    \end{itemize}
  \end{itemize}
\end{itemize}

\includegraphics[width=0.5\linewidth]{./2_6}
- Age in categories:
+ Infants, Toddlers, Gradeschoolers, Adolescents
* Quantitative variables are measured on a numerical scale that allows us to measure the interval between any two observations
+ Continuous variables can take decimal values
- Age of a patient
- Cholesterol concentration in a blood specimen
- Optical density of a solution
+ Discrete/Ordinal variables are reported as integers
- Number of bacteria colonies in a petri dish
- Number of cancerous lymph nodes detected in a patient
- Length of a DNA segment in basepairs
* The distinction between continuous and discrete variables is not a rigid one as measurements can be rounded off.
e.g.~age or birth weight can be reported as integers
* In practice, if the number of unique integer values observed is small (say \textless10), then we would treat the quantitative variable as discrete/ordinal

\hypertarget{probability}{%
\section{Probability}\label{probability}}

\begin{itemize}
\tightlist
\item
  The conclusions of a statistical data analysis are often stated in terms of probability
\item
  Probability models allow us to quantify how likely, or unlikely, an experimental result is, given certain modeling assumptions
\item
  We will first look at probability and probability distributions for dichotomous and discrete variables, before proceeding to continuous variables
\end{itemize}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

\begin{itemize}
\tightlist
\item
  A probability is a numerical quantity that expresses the likelihood of an event
\item
  The probability of an event E may be written as P(E)
\item
  P(E) is always a number between 0 and 1, inclusive. May also be expressed as a percentage
\item
  The higher the probability, the more certain we are that the event will occur
\item
  We can speak meaningfully about a probability P(E) only in the context of a \textbf{chance operation} or a \textbf{chance experiment}---that is, an operation whose outcome is not pre-determined
\item
  The chance operation must be defined in such a way that each time the chance operation is performed, the event E either occurs or does not occur. We refer to the event that E does not occur as E complement (\(E^c\))
\item
  The sample space enumerates all the possible events that a chance experiment gives rise to. The sum of their probabilities is 1
\end{itemize}

\hypertarget{example-1-coin-tossing}{%
\subsection{Example 1: Coin Tossing}\label{example-1-coin-tossing}}

\begin{itemize}
\tightlist
\item
  Consider the familiar chance operation of tossing a coin. The sample space is \{Heads, Tails\}. That means each time the coin is tossed, either it falls heads or Tails
\item
  Define the event E:Heads. If the coin is fair (i.e.~equally likely to fall heads or tails), then P(E) = 1/2 = 0.5
\item
  If the coin is not fair (perhaps because it is slightly bent), then P(E) will be some value other than 0.5, e.g.~P(E) = 0.6, suggesting it is more likely to see a head than a tail
\end{itemize}

\hypertarget{example-2-coin-tossing-again}{%
\subsection{Example 2: Coin Tossing again}\label{example-2-coin-tossing-again}}

\begin{itemize}
\tightlist
\item
  Consider the event
  E: 3 heads in a row
\item
  The chance operation that could give rise to this event is ``Toss a coin 3 times''
\item
  Notice that the sample space is now larger than when the operation was made of a single toss
\item
  The sample space is now
  \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}
  where H denotes Heads and T denotes Tails
\item
  Assuming we have a fair coin, the probability of each of the 8 outcomes in the sample space is equally likely
\item
  Therefore, P(E) = 1/8
\end{itemize}

\hypertarget{interpretation-of-probability}{%
\subsection{Interpretation of probability}\label{interpretation-of-probability}}

\begin{itemize}
\tightlist
\item
  How do we know P(Heads)=0.5 for a fair coin?
\item
  For one, we know that there are two possible events resulting from a coin toss both of which are equally likely
\item
  Another interpretation arises when a chance operation can be observed repeatedly. Then P(E) can be interpreted as the relative frequency of occurrence of E in an indefinitely long series of repetitions of the chance operation
\end{itemize}

\hypertarget{relative-frequency-interpretation-of-probability}{%
\subsection{Relative frequency interpretation of probability}\label{relative-frequency-interpretation-of-probability}}

\includegraphics[width=0.5\linewidth]{./2_16}

\begin{tabular}{r|l|r|r}
\hline
Number of tosses & Outcome & Cumulative number of heads & Relative frequency of heads\\
\hline
1 & H & 1 & 1.000\\
\hline
2 & H & 2 & 1.000\\
\hline
3 & T & 2 & 0.670\\
\hline
4 & T & 2 & 0.500\\
\hline
5 & T & 2 & 0.400\\
\hline
10 & H & 4 & 0.400\\
\hline
200 & H & 111 & 0.555\\
\hline
500 & H & 257 & 0.514\\
\hline
750 & H & 378 & 0.504\\
\hline
1000 & T & 487 & 0.487\\
\hline
\end{tabular}

\hypertarget{subjective-interpretation-of-probability}{%
\subsection{Subjective interpretation of probability}\label{subjective-interpretation-of-probability}}

\begin{itemize}
\tightlist
\item
  It is not always possible to observe events repeatedly. In such cases, probability may be used to represent a subjective or personal degree of belief
\end{itemize}

e.g.~There is an 80\% chance it will rain tomorrow

e.g.~It is believed that culture for M. tuberculosis in children has a \textless2\% chance of being falsely positive

\begin{itemize}
\tightlist
\item
  There are very few restrictions place on personal probabilities besides that they must be coherent

  \begin{itemize}
  \tightlist
  \item
    e.g.~If you say there is an 80\% chance it will rain tomorrow, then you should also agree that there is a 20\% chance it will not rain tomorrow
  \end{itemize}
\item
  Different individuals can have different personal probabilities, and may not necessarily agree,

  \begin{itemize}
  \tightlist
  \item
    e.g.~members on a job interview committee may different views on the probability of a client being suitable
  \end{itemize}
\end{itemize}

\hypertarget{compound-events}{%
\subsection{Compound events}\label{compound-events}}

\begin{itemize}
\tightlist
\item
  A compound event is defined by the joint occurrence of three simple events

  \begin{itemize}
  \tightlist
  \item
    e.g.~Obtaining three heads on three successive tosses of a coin
  \item
    e.g.~Obtaining a true positive diagnostic test result for tuberculosis
  \end{itemize}
\item
  The different simple events may be independent or dependent

  \begin{itemize}
  \tightlist
  \item
    Each toss of a coin is independent of the previous toss
  \item
    The probability of a positive result on a diagnostic test is dependent on whether the patient has the disease
  \end{itemize}
\end{itemize}

\hypertarget{some-questions-on-probability}{%
\subsection{Some questions on probability}\label{some-questions-on-probability}}

\begin{itemize}
\tightlist
\item
  Click on this \href{https://forms.office.com/Pages/ResponsePage.aspx?id=cZYxzedSaEqvqfz4-J8J6vtaSwoU68FCgvKfzwN_XcBUQTRINzBERUowMVJVRjVTVzVXNzdFUUY1Ti4u}{link} to answer a few questions on probability
\end{itemize}

\hypertarget{combining-probabilities-addition-rules}{%
\subsection{Combining probabilities: Addition rules}\label{combining-probabilities-addition-rules}}

\includegraphics[width=0.5\linewidth]{./2_21a}
\includegraphics[width=0.5\linewidth]{./2_21b}

\begin{itemize}
\tightlist
\item
  When two events are independent (top panel) then
\end{itemize}

\[P\{E1 \space or \space E2\} = P\{E1\} + P\{E2\}\]

\begin{itemize}
\tightlist
\item
  When two events are dependent (bottom panel)
\end{itemize}

\[P\{E1 \space or \space E2\}  = P\{E1\} + P\{E2\} - P\{E1 \space and \space E2\}\]

\hypertarget{combining-probabilities-multiplication-rules}{%
\subsection{Combining probabilities: Multiplication rules}\label{combining-probabilities-multiplication-rules}}

\includegraphics[width=0.5\linewidth]{./2_22a}
\includegraphics[width=0.5\linewidth]{./2_22b}

\begin{itemize}
\tightlist
\item
  When two events are independent (top panel) then
\end{itemize}

\[P\{E1 \space and \space E2\} = P\{E1\} × P\{E2\}\]

\begin{itemize}
\item
  When two events are dependent (bottom panel)

  \[P\{E1 \space and \space E2\} = P\{E1\} × P\{E2|E1\} = P\{E2\} × P\{E1|E2\}\]
\end{itemize}

\hypertarget{conditional-probability}{%
\subsection{Conditional probability}\label{conditional-probability}}

\begin{itemize}
\tightlist
\item
  P(E2\textbar E1) is the conditional probability of E2 given E1, it is interpreted as the probability of observing E2 given that E1 has occurred
\end{itemize}

\[P(E2|E1) = \frac{P(E_1 \space and \space E_2)}{P(E_1)}\]

\hypertarget{probability-trees}{%
\subsection{Probability Trees}\label{probability-trees}}

\begin{itemize}
\tightlist
\item
  Often it is useful to depict a probability problem using a probability tree
\item
  The following slides depict how we can enumerate the events in the sample space that arises from independent events, and how we can then calculate the corresponding probabilities of each event using probability rules
\end{itemize}

\hypertarget{example-3-independent-events}{%
\subsection{Example 3: Independent events}\label{example-3-independent-events}}

\begin{itemize}
\tightlist
\item
  If two carriers of the gene for albinism marry, each of their children has probability 1/4 of being albino.
\item
  The chance that the second child is albino is the same (1/4) whether or not the first child is albino; similarly, the outcome for the third child is independent of the first two, and so on.
\item
  We can use a probability tree to enumerate the sample space and corresponding probabilities
\end{itemize}

\hypertarget{albinism-example}{%
\subsection{Albinism example}\label{albinism-example}}

\includegraphics[width=0.5\linewidth]{./2_26}

\begin{itemize}
\tightlist
\item
  Suppose two carriers of the gene for albinism marry and have two children. Then the probability that both of their children are albino is
\end{itemize}

\[P\{AA\} 
    = 0.25 × 0.25
    = 0.0625\]

\hypertarget{albinism-example-sample-space-and-probabilities}{%
\subsection{Albinism example: Sample space and probabilities}\label{albinism-example-sample-space-and-probabilities}}

\begin{tabular}{l|l}
\hline
Number of Albino children & Probability\\
\hline
2 & \$0.25\textasciicircum{}2 = 0.0625\$\\
\hline
1 & \$2 \textbackslash{}times 0.25 \textbackslash{}times 0.75 = 0.375\$\\
\hline
0 & \$0.75\textasciicircum{}2 = 0.5625\$\\
\hline
Total & 1\\
\hline
\end{tabular}

\hypertarget{example-4-medical-testing}{%
\subsection{Example 4: Medical testing}\label{example-4-medical-testing}}

\begin{itemize}
\tightlist
\item
  The following is based on a scenario a statistician, David Eddy, (1982) posed to a 100 physicians* (see full text \href{http://personal.lse.ac.uk/robert49/teaching/mm/articles/Eddy1982_ProbReasoningInClinicalMedicine.pdf}{here}):
\item
  One of your patients has a lump in her breast. You are almost certain that it is benign, and believe there is only a 1\% chance that it is malignant. Just to be sure you have the patient undergo a mammogram. Sadly for your patient the mammogram is positive.
\item
  Suppose that the mammogram has the following characteristics

  \begin{itemize}
  \tightlist
  \item
    P\{Testing positive \textbar{} Person has disease\} = Sensitivity = 80\%
  \item
    P\{Testing negative \textbar{} Person does not have disease\} = Specificity = 90\%
  \end{itemize}
\item
  What is the probability that a randomly chosen woman will test positive on a mammogram? What are the chances the lump is truly malignant?
\end{itemize}

\hypertarget{medical-testing-example}{%
\subsection{Medical testing example}\label{medical-testing-example}}

\includegraphics[width=0.5\linewidth]{./2_29}

\begin{itemize}
\tightlist
\item
  The sample space is \{D+T+, D+T-, D-T+, D-T-\}
\item
  The probability of a positive test
\end{itemize}

= P\{true positive\} + P\{false positive\}

= Pr\{D+T+\} + P\{D-T+\}

= 0.01 × 0.8 + 0.99 × 0.1

= 0.107

\begin{itemize}
\tightlist
\item
  Probability that a person truly has the disease given they are positive
\end{itemize}

= P\{D+ \textbar{} T+\}

= P\{D+ and T+\}/P\{T+\}

= (0.01 × 0.8)/(0.01 × 0.8 + 0.99 × 0.1)

= 0.075

The above expression is formally referred to as Bayes Theorem

\hypertarget{probability-distributions}{%
\section{Probability Distributions}\label{probability-distributions}}

\begin{itemize}
\tightlist
\item
  A \textbf{probability distribution} is a mathematical function that provides the probabilities of occurrence of different possible values of a random variable

  \begin{itemize}
  \tightlist
  \item
    It follows the probability rules we studied earlier, e.g.~the sum of the probabilities of all possible values of a random variable is 1
  \end{itemize}
\item
  A very large number(100s?) of probability distributions have been described -- but we tend to use a much smaller number in common applications
\end{itemize}

\hypertarget{population-and-sample}{%
\subsection{\texorpdfstring{Population and sample\(^*\)}{Population and sample\^{}*}}\label{population-and-sample}}

\includegraphics[width=0.5\linewidth]{./2_33}

\(^*\)From text by Ekstrom and Sorensen

\hypertarget{notation}{%
\subsection{Notation}\label{notation}}

\includegraphics[width=0.5\linewidth]{./2_34}

\hypertarget{parameters-statistics-probability-distributions}{%
\subsection{Parameters, Statistics, Probability Distributions}\label{parameters-statistics-probability-distributions}}

\begin{itemize}
\tightlist
\item
  A \textbf{parameter} is a number that describes the \textbf{population}. A parameter is a fixed number; but in practice we do not know its value.
\item
  A \textbf{statistic} is a number that describes a \textbf{sample}. The value of a statistic is known when we have taken a sample, but it can change from sample to sample. We often use a statistic to estimate an unknown parameter
\item
  The \textbf{probability distributions} we will study in this lecture are examples of \textbf{probability models} that help us to make inference about the population based on observed statistics
\end{itemize}

\hypertarget{binomial-distribution}{%
\subsection{Binomial Distribution}\label{binomial-distribution}}

\begin{itemize}
\tightlist
\item
  Both the coin toss and the albinism examples were examples of random variables following a Binomial distribution. These variables are characterized by:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Binary outcomes}: There are two possible outcomes for each trial (success and failure).
  \item
    \textbf{Independent trials}: The outcomes of the trials are independent of each other.
  \item
    \textbf{n is fixed}: The number of trials, n, is fixed in advance.
  \item
    \textbf{Same value of \(\pi\)}: The probability of a success on a single trial is the same for all trials.
  \end{itemize}
\end{itemize}

\hypertarget{binomial-distribution-function}{%
\subsection{Binomial Distribution Function}\label{binomial-distribution-function}}

Whereas both examples we looked at had n=2 trials, and were easy to illustrate with a probability tree, we can write a more general expression for the probability of k successes in n independent trials as follows

\[Pr\{k|n,\pi\} = \frac{n!}{k!(n-k)!}\pi^k(1-\pi)^{n-k}\]

\hypertarget{albinism-example-for-a-couple-with-5-children-sample-space-and-probabilities}{%
\subsection{Albinism example for a couple with 5 children: Sample space and probabilities}\label{albinism-example-for-a-couple-with-5-children-sample-space-and-probabilities}}

\begin{tabular}{r|l|r}
\hline
Number of Albino children & Probability expression & Probability rounded value\\
\hline
0 & \$(1-\textbackslash{}pi)\textasciicircum{}5\$ & 0.24\\
\hline
1 & \$5\textbackslash{}pi(1-\textbackslash{}pi)\textasciicircum{}4\$ & 0.40\\
\hline
2 & \$10\textbackslash{}pi\textasciicircum{}2(1-\textbackslash{}pi)\textasciicircum{}3\$ & 0.26\\
\hline
3 & \$10\textbackslash{}pi\textasciicircum{}3(1-\textbackslash{}pi)\textasciicircum{}2\$ & 0.09\\
\hline
4 & \$5\textbackslash{}pi\textasciicircum{}4(1-\textbackslash{}pi)\$ & 0.01\\
\hline
5 & \$\textbackslash{}pi\textasciicircum{}5\$ & 0.00\\
\hline
\end{tabular}

These probabilities may be obtained from R with the following command:

dbinom(x=seq(0,5),size=5,prob=0.25)

\hypertarget{probability-distributions-in-r}{%
\subsection{Probability distributions in R}\label{probability-distributions-in-r}}

\begin{itemize}
\tightlist
\item
  One of the main advantages of R is that it has several functions related to statistical probability distributions that can be used to:

  \begin{itemize}
  \tightlist
  \item
    Obtain a random sample from a distribution
  \item
    Calculate the density function
  \item
    Calculate the cumulative probability
  \item
    Obtain quantiles of the distribution
  \end{itemize}
\end{itemize}

\begin{tabular}{l|l|l|l|l}
\hline
Distribution & Random Sample & Density function & Cumulative probability function & Quantiles\\
\hline
Binomial & rbinom & dbinom & pbinom & qbinom\\
\hline
Gamma & rgamma & dgamma & pgamma & qgamma\\
\hline
Poisson & rpois & dpois & ppois & qpois\\
\hline
Normal & rnorm & dnorm & pnorm & qnorm\\
\hline
\end{tabular}

\hypertarget{example-binomial-distribution-in-practice}{%
\subsection{\texorpdfstring{Example: Binomial distribution in practice\(^*\)}{Example: Binomial distribution in practice\^{}*}}\label{example-binomial-distribution-in-practice}}

\begin{itemize}
\tightlist
\item
  The assumptions behind the use of the Binomial distribution may not always be satisfied in practice. For example:
\item
  Let X represent the number of females in four children, among all couples in Canada with exactly four children
\item
  The ``Real World'' data and the data predicted by a Binomial distribution model with n=4 and \(\pi=0.5\) are given on the following page, i.e.~the predicted proportion was given by
\end{itemize}

\[Pr\{X=k\} = \frac{4!}{k!(4-k)!}\pi^k(1-\pi)^{4-k}\]

\begin{tabular}{r|r|r}
\hline
X & Predicted proportion obtained using dbinom in R & Observed proportion\\
\hline
0 & 0.0625 & 0.08\\
\hline
1 & 0.2500 & 0.26\\
\hline
2 & 0.3750 & 0.31\\
\hline
3 & 0.2500 & 0.27\\
\hline
4 & 0.0625 & 0.08\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Why do you think the observed values differ (slightly) from those predicted by a Binomial model?
\item
  Which assumptions of the Binomial model may be violated here?
\end{itemize}

\(^*\)From Lawrence Joseph's notes

\hypertarget{mean-and-variance-of-random-variables}{%
\subsection{Mean and variance of random variables}\label{mean-and-variance-of-random-variables}}

Let X be a discrete random variable taking values \(\{x_1, x_2, …, x_n\}\) with probabilities \(\{p_1, p_2, …, p_n\}\), respectively

\begin{itemize}
\tightlist
\item
  Population mean (or expectation) of a discrete random variable \(=E(X)=\sum_{i=1}^nx_ip_i\)
\item
  Population variance of a discrete random variable \(=\sum_{i=1}^n(x_i-E(X))^2p_i\)
\end{itemize}

\hypertarget{mean-and-variance-for-a-binomial-distribution}{%
\subsection{Mean and variance for a Binomial distribution}\label{mean-and-variance-for-a-binomial-distribution}}

\includegraphics[width=0.5\linewidth]{./2_44}

\begin{itemize}
\tightlist
\item
  Let X be a Binomial variable with n trials and probability of success \(\pi\)
\item
  \(E(X)=n\pi=5\times 0.25=1.25\)
\item
  \(Var(X)=n\pi(1-\pi)=5\times 0.25\times 0.75 = 0.9375\)
\end{itemize}

Therefore, standard deviation of X = 0.968

\hypertarget{probability-of-a-continuous-variable}{%
\subsection{Probability of a continuous variable}\label{probability-of-a-continuous-variable}}

\includegraphics[width=0.5\linewidth]{./2_45}

\begin{itemize}
\tightlist
\item
  We can think of the relative frequency histogram of a continuous variable as an approximation of the underlying true population distribution from which the data came.
\item
  A smooth curve representing a frequency distribution is called a probability density function
\item
  On the x-axis we have different possible values of the variable (i.e.~the sample space). On the y-axis we have the probability density corresponding to each value of the variable.
\end{itemize}

\hypertarget{probability-density-function-for-a-continuous-variable}{%
\subsection{Probability density function for a continuous variable}\label{probability-density-function-for-a-continuous-variable}}

\includegraphics[width=0.5\linewidth]{./2_46a}
\includegraphics[width=0.5\linewidth]{./2_46b}

\begin{itemize}
\tightlist
\item
  If a variable is continuous, then we find probabilities by using the density curve for the variable.
\item
  The probability that a continuous variable lies in a certain range equals the area under the density curve for the variable between two points
\item
  This means the probability of a single value, say Pr\{Y=a\}=0. But the \(Pr\{a-\delta<Y<a+\delta\}\), where \(\delta\) is an infinitesimal quantity is non-zero and equal to the height of the density function at Y=a.
\item
  The area under the entire curve is 1
\end{itemize}

\hypertarget{normal-distribution}{%
\subsection{Normal Distribution}\label{normal-distribution}}

\begin{itemize}
\tightlist
\item
  The most well know continuous distribution is the Normal (or Gaussian) distribution that is recognizable by its characteristic bell shape
\item
  Probability density function of a continuous variable Y that follows a normal distribution with mean \(\mu\) and variance \(\sigma^2\)
\end{itemize}

\[f(y|\mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}}exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right),-\infty<y<\infty\]

\hypertarget{normal-probability-density-function}{%
\subsection{Normal probability density function}\label{normal-probability-density-function}}

\includegraphics[width=0.5\linewidth]{./2_48}

\begin{itemize}
\tightlist
\item
  The density function is symmetric about the mean \(\mu\) (which also happens to be the median and mode of this distribution
\item
  Though it is defined all the way from \(-\infty\) to \(\infty\), most of the probability lies in the range \(\mu\pm3\sigma\)
\end{itemize}

\hypertarget{three-normal-curves-with-different-means-and-standard-deviations}{%
\subsection{Three normal curves with different means and standard deviations}\label{three-normal-curves-with-different-means-and-standard-deviations}}

\includegraphics[width=0.5\linewidth]{./2_49}

\hypertarget{area-under-the-normal-curve}{%
\subsection{Area under the normal curve}\label{area-under-the-normal-curve}}

\includegraphics[width=0.5\linewidth]{./2_50}

\begin{itemize}
\tightlist
\item
  \(Pr\{a<Y<b\}=\int_a^b\frac{1}{\sigma\sqrt{2\pi}}exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)\)
\item
  The values of the areas under the standard normal distribution (denoted \(N(\mu=0, \sigma^2=1))\) were typically published as tables in statistics books
\item
  Today you can use a program like R to calculate this integral
\end{itemize}

\hypertarget{example-distribution-of-serum-cholesterol-values}{%
\subsection{Example: Distribution of serum cholesterol values}\label{example-distribution-of-serum-cholesterol-values}}

\begin{itemize}
\tightlist
\item
  The serum cholesterol levels of 12- to 14-year-olds follow a normal distribution with mean 155 mg/dl and standard deviation 27 mg/dl. What percentage of 12 to 14-year-olds have serum cholesterol values
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  137 or less?
\item
  186 or less?
\item
  164 or more?
\item
  100 or more?
\item
  between 159 and 186?
\item
  between 100 and 132?
\item
  between 132 and 159?
\end{enumerate}

\includegraphics[width=0.5\linewidth]{./2_52}

Let Y denote the variable serum cholesterol.

We know the distribution is symmetric about 155mg/dl and that most values of Y will lie between (74, 236)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  P(Y ≤ 137) = pnorm(q=137,mean=155,sd=27) = 0.252
\item
  P(Y≥164) = 1 -- Pr(Y \textless{} 164) = 1 -- pnorm(q=164,mean=155,sd=27) = 0.369
\item
  P(Y≤186) = 0.875
\item
  P (Y ≥100) = 1 -- 0.02 = 0.98
\end{enumerate}

\includegraphics[width=0.5\linewidth]{./2_54}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  between 159 and 186?
\end{enumerate}

P(159 ≤ Y ≤ 186)
= P(Y ≤ 186) -- Pr(Y ≤ 159)
= pnorm(186,155,27)
- pnorm(159,155,27)

= 0.875 -- 0.559
= 0.316

Another way to answer this question is via the z-transformation of Y into a standard normal variable with mean=0 and standard deviation=1

P(159 ≤ Y ≤ 186)
= P( (159-155)/27 ≤ (Y-155)/27 ≤ (186-155)/27 )

= Pr(0.148 ≤ Z ≤ 1.148)
= Pr(Z ≤ 1.148) -- Pr(Z ≤ 0.148)

= pnorm(1.148) - pnorm(0.148)
= 0.875 -- 0.559
= 0.316

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  between 100 and 132?
\end{enumerate}

P(100 ≤ Y ≤ 132) = 0.176

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  between 132 and 159?
\end{enumerate}

P(132 ≤ Y ≤ 159) = 0.362

\hypertarget{area-under-the-normal-curve-1}{%
\subsection{Area under the normal curve}\label{area-under-the-normal-curve-1}}

\includegraphics[width=0.5\linewidth]{./2_57}

\begin{itemize}
\tightlist
\item
  We can show that the probability of lying within 1 standard deviation of the mean is 0.68, within 2 standard deviations is 95\% and within 3 standard deviations is 99.7\%
\end{itemize}

\hypertarget{mean-and-variance-of-the-normal-distribution}{%
\subsection{Mean and variance of the normal distribution}\label{mean-and-variance-of-the-normal-distribution}}

\begin{itemize}
\tightlist
\item
  The expressions for the mean and variance are similar to those for discrete variables, except that the summation sign is replaced by an integral sign
\item
  \(Expectation(Y)=\int_{-\infty}^\infty\frac{y}{\sigma\sqrt{2\pi}}exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)dy = \mu\)
\item
  \(Variance(Y)=\int_{-\infty}^\infty\frac{(y-\mu)^2}{\sigma\sqrt{2\pi}}exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right)dy = \sigma^2\)
\end{itemize}

\hypertarget{examples-of-discrete-distributions}{%
\subsection{Examples of discrete distributions}\label{examples-of-discrete-distributions}}

\begin{itemize}
\tightlist
\item
  Bernoulli distribution
\item
  Binomial distribution
\item
  Poisson distribution
\item
  Negative binomial distribution
\end{itemize}

\hypertarget{bernoulli-distribution}{%
\subsection{Bernoulli distribution}\label{bernoulli-distribution}}

\includegraphics[width=0.5\linewidth]{./2_60}

\begin{itemize}
\tightlist
\item
  X is dichotomous
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Single coin toss
  \item
    Observation on an individual patient in a longitudinal study of survival following a treatment
  \end{itemize}
\end{itemize}

Probability density function

\[f(x|\pi)=\pi^x(1-\pi)^{1-x},x=0,1\]

Mean = \(\pi\)

Variance = \(\pi(1-\pi)\)

\hypertarget{binomial-distribution-1}{%
\subsection{Binomial distribution}\label{binomial-distribution-1}}

\includegraphics[width=0.5\linewidth]{./2_61}

\begin{itemize}
\tightlist
\item
  X is the sum of successes in n independent Bernoulli trials
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    three coin tosses
  \item
    number of patients who will survive 1 year following a treatment
  \end{itemize}
\end{itemize}

Probability density function

\[f(x|n,\pi)={n\choose x}\pi^x(1-\pi)^{1-x},x=0,1,...,n\]

Mean = \(n\pi\)

Variance = \(n\pi(1-\pi)\)

\hypertarget{poisson-distribution}{%
\subsection{Poisson distribution}\label{poisson-distribution}}

\includegraphics[width=0.5\linewidth]{./2_62}

\begin{itemize}
\tightlist
\item
  X takes discrete taking values 0, 1, to \(\infty\) within a unit of time or space
\item
  Examples

  \begin{itemize}
  \tightlist
  \item
    Number of downloads of an app in 1 minute
  \item
    Number of cases of cancer reported in a square kilometre
  \end{itemize}
\end{itemize}

Probability density function

\[f(x\space events|time=t,rate=\lambda)=\frac{exp^{-\mu}\mu^x}{x!},x=0,1,...,\infty\]

Mean = \(\mu\)

Variance = \(\mu\)

\hypertarget{example-transcriptomic-analyses}{%
\subsection{\texorpdfstring{Example: Transcriptomic Analyses\(^*\)}{Example: Transcriptomic Analyses\^{}*}}\label{example-transcriptomic-analyses}}

\includegraphics[width=0.5\linewidth]{./2_63}

\begin{itemize}
\tightlist
\item
  RNA sequencing is a powerful and commonly used tool to analysis expression data
\item
  The goal of most sequencing experiments is to identify differences in gene expression between biological conditions such as the influence of a disease-linked genetic mutation or drug treatment.
\end{itemize}

\hypertarget{underlying-statistical-principles-of-commonly-used-packages}{%
\subsection{Underlying statistical principles of commonly used packages}\label{underlying-statistical-principles-of-commonly-used-packages}}

\includegraphics[width=0.5\linewidth]{./2_64}

\hypertarget{how-it-works}{%
\subsection{How it works}\label{how-it-works}}

\begin{itemize}
\tightlist
\item
  In a standard sequencing experiment (RNA-Seq), we map the sequencing reads to the reference genome and count how many reads fall within a given gene (or exon).
\end{itemize}

\includegraphics[width=0.5\linewidth]{./2_65}

=\textgreater{} This means that the input for the statistical analysis are discrete non-negative integers (``counts'') for each gene in each sample.

\hypertarget{what-would-be-a-suitable-probability-distribution}{%
\subsection{What would be a suitable probability distribution?}\label{what-would-be-a-suitable-probability-distribution}}

\begin{itemize}
\tightlist
\item
  The total number of reads for each sample tends to be in the millions, while the counts per gene vary considerably but tend to be in the tens, hundreds or thousands.
\item
  The chance of a given read to be mapped to any specific gene is rather small.
\item
  Discrete events that are sampled out of a large pool with low probability - sounds like a Poisson distribution would be suitable
\end{itemize}

Problem: The \textbf{variability of read counts} in sequencing experiments tends to be \textbf{larger than the Poisson distribution allows}.

\includegraphics[width=0.5\linewidth]{./2_67}

\begin{itemize}
\tightlist
\item
  It is obvious that the variance of counts is generally greater than their mean, especially for genes expressed at a higher level. This phenomenon is called ``\textbf{overdispersion}``.
\item
  The negative binomial distribution can model the greater variance
\end{itemize}

\hypertarget{poisson-distribution-is-limiting}{%
\subsection{Poisson Distribution is limiting}\label{poisson-distribution-is-limiting}}

\begin{itemize}
\tightlist
\item
  The Poisson distribution makes the restrictive assumption that the mean of the distribution is equal to its variance
\item
  In terms of RNA-seq, Poisson distribution implies that for a certain gene, its expression profile follows a distribution with a mean expression equal to the variance in expression
\item
  Empirical observations show that for highly expressed genes at least, this is not the case even in biological replicates
\item
  Another degree of variation that removes this restriction
\end{itemize}

\hypertarget{negative-binomial-distribution}{%
\subsection{Negative Binomial Distribution}\label{negative-binomial-distribution}}

\begin{itemize}
\tightlist
\item
  The NB distribution is similar to a Poisson distribution but has an extra parameter (α) called the ``clumping'' or ``dispersion'' parameter =\textgreater{} \textbf{More variance}
\end{itemize}

\[\sigma^2=\mu+\alpha\mu^2\]

\begin{itemize}
\tightlist
\item
  The NB distribution can be defined as a \textbf{Poisson-Gamma mixture distribution}
\item
  This means that the NB distribution is a weighted mixture of Poisson distributions where the rate parameter (i.e.~the expected counts) is itself associated with uncertainty following a Gamma distribution
\end{itemize}

\hypertarget{conceptual-justification}{%
\subsection{Conceptual Justification}\label{conceptual-justification}}

\begin{itemize}
\tightlist
\item
  When comparing samples of different conditions we usually have multiple independent replicates of each condition.
\item
  Such replicates are called \textbf{``biological''} replicates because they come from independent animals, dishes, or cultures.
\item
  Splitting a sample in two and running it through the sequencer twice would be a \textbf{``technical''} replicate.
\item
  In general, there is more variance associated with biological replicates than technical replicates.
\item
  As a result, the Poisson process in each biological replicate has a slightly different expected count parameter.
\end{itemize}

\hypertarget{additional-notes-and-practical-implications}{%
\subsection{Additional Notes and Practical Implications}\label{additional-notes-and-practical-implications}}

\begin{itemize}
\tightlist
\item
  In a standard sequencing experiments, we have to be content with few biological replicates per condition due to the high costs associated with sequencing experiments and the large amount of time that goes into library preparations.
\item
  Modern RNA-Seq analysis tools such as \href{https://bioconductor.org/packages/release/bioc/html/DESeq2.html}{DESeq2} and \href{https://bioconductor.org/packages/release/bioc/html/edgeR.html}{edgeR} combine the gene-wise dispersion estimate with an estimate of the expected dispersion rate based on all genes.
\item
  This \textbf{Bayesian ``shrinkage''} of the variance has emerged as a powerful technique to mitigate the shortcomings of having few replicates.
\end{itemize}

\hypertarget{examples-of-continuous-distributions}{%
\subsection{Examples of continuous distributions}\label{examples-of-continuous-distributions}}

\begin{itemize}
\tightlist
\item
  Normal distribution
\item
  Uniform distribution
\item
  Student's t-distribution
\item
  Gamma distribution
\item
  Beta distribution
\end{itemize}

\hypertarget{normal-distribution-1}{%
\subsection{Normal distribution}\label{normal-distribution-1}}

\includegraphics[width=0.5\linewidth]{./2_73}

\begin{itemize}
\tightlist
\item
  X is continuous and symmetrically distributed over a range that lies between \(-\infty\) to \(\infty\)
\item
  Example:

  \begin{itemize}
  \tightlist
  \item
    blood pressure
  \item
    body mass index
  \end{itemize}
\end{itemize}

Probability density function

\(f(x|\mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}}exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),-\infty<x<\infty\)

Mean = \(\mu\)

Variance = \(\sigma^2\)

\hypertarget{uniform-distribution}{%
\subsection{Uniform distribution}\label{uniform-distribution}}

\includegraphics[width=0.5\linewidth]{./2_74}

\begin{itemize}
\tightlist
\item
  X is continuous and equally likely to take values in the range (a,b)
\item
  In the standard Uniform distribution, a=0, b=1
\item
  Example:

  \begin{itemize}
  \tightlist
  \item
    X is a probability, such as disease prevalence or sensitivity of a test
  \end{itemize}
\end{itemize}

\hypertarget{lecture-3-central-limit-theorem-and-inference-for-means}{%
\chapter{Lecture 3: Central Limit Theorem and Inference for Means}\label{lecture-3-central-limit-theorem-and-inference-for-means}}

\hypertarget{mean-and-standard-deviation}{%
\section{Mean and Standard Deviation}\label{mean-and-standard-deviation}}

\hypertarget{descriptive-statistics-vs.-inferential-statistics}{%
\subsection{Descriptive statistics vs.~Inferential Statistics}\label{descriptive-statistics-vs.-inferential-statistics}}

\includegraphics[width=0.5\linewidth]{./3_3}

\begin{itemize}
\tightlist
\item
  Descriptive statistics help to describe the characteristics of the sample gathered
\item
  Inferential statistics help to use these characteristics to draw conclusions about the target population
\end{itemize}

\hypertarget{some-commonly-encountered-shapes-of-distributions-of-a-variable}{%
\subsection{Some commonly encountered shapes of distributions of a variable}\label{some-commonly-encountered-shapes-of-distributions-of-a-variable}}

\includegraphics[width=0.5\linewidth]{./3_4}

\hypertarget{descriptive-statistics-notation}{%
\subsection{Descriptive statistics: Notation}\label{descriptive-statistics-notation}}

\begin{itemize}
\tightlist
\item
  We use capital letters to denote a variable, and small letters to denote the values it takes. For example,

  \begin{itemize}
  \tightlist
  \item
    X = FEV (the variable),
  \item
    x = 0.793 litres (an observed value)
  \end{itemize}
\item
  \(\sum_{i=1}^nx_i\) means the sum of the observed values x on a sample of size n.~\(x_i\) is the observed value for the \(i^{th}\) subject in the sample
\item
  The next few slides list common measures of central tendency and spread
\end{itemize}

\hypertarget{histogram-of-fev}{%
\subsection{Histogram of FEV}\label{histogram-of-fev}}

\includegraphics[width=0.5\linewidth]{./3_6}

\hypertarget{measures-of-central-tendency}{%
\subsection{Measures of central tendency}\label{measures-of-central-tendency}}

\includegraphics[width=1\linewidth]{./3_7}

\hypertarget{summary-of-fev-variable}{%
\subsection{Summary of FEV variable}\label{summary-of-fev-variable}}

\includegraphics[width=1\linewidth]{./3_8}

For a symmetric distribution, the median=mean.

The values above suggest that the distribution of FEV may be slightly skewed to the right as the mean is higher than the mode

\hypertarget{robustness}{%
\subsection{Robustness}\label{robustness}}

\begin{itemize}
\tightlist
\item
  A statistic is said to be \textbf{robust} if the value of the statistic is relatively unaffected by changes in a small portion of the data, even if the changes are dramatic ones. The median is a robust statistic, but the mean is not robust because it can be greatly shifted by changes in even one
\item
  \textbf{Example:} In the FEV dataset, I replaced the last observation in the dataset of 3.211 by 6.211, an extreme value. This resulted in increasing the mean from 2.637 to 2.641 but the median remained at 2.548
\item
  If the frequency distribution is skewed, both measures are pulled toward the longer tail, but the mean is usually pulled farther than the median
\end{itemize}

\hypertarget{mean-vs.-median}{%
\subsection{Mean vs.~Median}\label{mean-vs.-median}}

\begin{itemize}
\tightlist
\item
  In some situations the mean makes very little sense. Suppose, for example, that the observations are survival times of cancer patients on a certain treatment protocol, and that most patients survive less than 1 year, while a few respond well and survive for 5 or even 10 years. In this case, the mean survival time might be greater than the survival time of most patients; the median would more nearly represent the experience of a ``typical'' patient. Note also that the mean survival time cannot be computed until the last patient has died; the median does not share this disadvantage. Situations in which the median can readily be computed, but the mean cannot, are not uncommon in bioassay, survival, and toxicity studies
\item
  An advantage of the mean is that in some circumstances it is more efficient than the median. Efficiency is a technical notion in statistical theory; roughly speaking, a method is efficient if it takes full advantage of all the information in the data. Partly because of its efficiency, the mean has played a major role in classical methods in statistics
\end{itemize}

\hypertarget{quantiles}{%
\subsection{Quantiles}\label{quantiles}}

\begin{itemize}
\tightlist
\item
  Quantiles (also known as percentiles) help to demarcate different points of the distribution of a continuous variable
\item
  The q\% quantile is the number below which q\% of observed values lie
\item
  For example

  \begin{itemize}
  \tightlist
  \item
    The 10\% quantile of FEV is the value below which 10\% of FEV values lie = 1.612\\
    = \(0.1n^{th}\) lowest value of FEV
  \end{itemize}
\end{itemize}

\hypertarget{measures-of-spread}{%
\subsection{Measures of spread}\label{measures-of-spread}}

\includegraphics[width=1\linewidth]{./3_12}

\hypertarget{summary-of-fev-variable-1}{%
\subsection{Summary of FEV variable}\label{summary-of-fev-variable-1}}

\includegraphics[width=1\linewidth]{./3_13}

\hypertarget{comparison-of-measures-of-spread}{%
\subsection{Comparison of measures of spread}\label{comparison-of-measures-of-spread}}

\includegraphics[width=1\linewidth]{./3_14}

\hypertarget{variance-and-standard-deviation}{%
\subsection{Variance and Standard Deviation}\label{variance-and-standard-deviation}}

\begin{itemize}
\tightlist
\item
  The standard deviation is more commonly reported than the variance because it is in the same units as the variable X and the mean of X
\item
  Notice that we use the sum of the squared deviations. This is because the sum of the deviations themselves will always be 0. We need a way to get rid of the signs of the deviations. Alternatives to taking the squares include taking the absolute value. But squares are more popular because of their mathematical properties
\item
  Why do we divide by n-1 rather than n? We do so because we are measuring the deviation from a quantity that is also defined using the sample, i.e.~\(\bar x\). It is as if we must penalize the sample size to correct for this. If we knew the true population mean (µ), then we would divide by n instead:
\end{itemize}

\[Population\space variance = \frac{\sum_{i=1}^n(x_i-\mu)^2}{n}\]

\hypertarget{why-n-1-rather-than-n}{%
\subsection{\texorpdfstring{Why n-1 rather than n?\(^*\)}{Why n-1 rather than n?\^{}*}}\label{why-n-1-rather-than-n}}

\begin{itemize}
\tightlist
\item
  Suppose the population has only 4 members \{1,2,3,4\}

  \begin{itemize}
  \tightlist
  \item
    The true mean is \(\frac{1+2+3+4}{4} = 2.5\)
  \item
    The true variance is \(\frac{(1−2.5)^2+(2−2.5)^2+(3−2.5)^2+(4−2.5)^2}{4}=1.25\)
  \end{itemize}
\item
  Now suppose we cannot view the whole population, but instead take a sample of size two. On the next slide, all possible samples are listed together with mean, the correct calculation for the sample variance dividing by n-1 and the incorrect calculation dividing by n.~Each sample is equally likely to occur, assuming we are sampling with replacement from the population
\item
  Notice that the incorrect expression for the sample variance results in an underestimate on the average across samples
\end{itemize}

\begin{tabular}{l|r|r|r}
\hline
Sample & Sample mean & Correct Sample variance & Underestimated Sample variance\\
\hline
(1,2) & 1.5 & 0.50 & 0.250\\
\hline
(1,3) & 2.0 & 2.00 & 1.000\\
\hline
(1,4) & 2.5 & 4.50 & 2.250\\
\hline
(2,3) & 2.5 & 0.50 & 0.250\\
\hline
(2,4) & 3.0 & 2.00 & 1.000\\
\hline
(3,4) & 3.5 & 0.50 & 0.250\\
\hline
(2,1) & 1.5 & 0.50 & 0.250\\
\hline
(3,1) & 2.0 & 2.00 & 1.000\\
\hline
(4,1) & 2.5 & 4.50 & 2.250\\
\hline
(3,2) & 2.5 & 0.50 & 0.250\\
\hline
(4,2) & 3.0 & 2.00 & 1.000\\
\hline
(4,3) & 3.5 & 0.50 & 0.250\\
\hline
(1,1) & 1.0 & 0.00 & 0.000\\
\hline
(2,2) & 2.0 & 0.00 & 0.000\\
\hline
(3,3) & 3.0 & 0.00 & 0.000\\
\hline
(4,4) & 4.0 & 0.00 & 0.000\\
\hline
Average across samples & 2.5 & 1.25 & 0.625\\
\hline
\end{tabular}

\(^*\)Lawrence Joseph's notes

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

\hypertarget{example-1-serum-cholesterol-in-children}{%
\subsection{Example 1: Serum cholesterol in children}\label{example-1-serum-cholesterol-in-children}}

\begin{itemize}
\tightlist
\item
  Though we are more conscious of the relationship between cholesterol level and heart disease in adults, high levels of cholesterol are also a concern in children, particularly if they have risk factors like family history or obesity
\item
  The American Academy of Pediatrics now recommends cholesterol testing in certain age groups
\item
  To determine if a child is at risk of heart disease, we would need to compare the observed cholesterol level with the standard expected in a normal child. How large a sample size do we need to determine the normal level?
\item
  The serum cholesterol levels (Y) of 12- to 14-year-olds follow a normal distribution with mean μ=155mg/dl and standard deviation σ=27 mg/dl
\item
  You wish to estimate the true mean serum cholesterol in this population by using a sample of observations:

  \begin{itemize}
  \tightlist
  \item
    Should you prefer a sample of n=10, 30 or 100 observations?
  \end{itemize}
\end{itemize}

\includegraphics[width=1\linewidth]{./3_21}

\hypertarget{the-sampling-distribution-of-bar-y}{%
\subsection{\texorpdfstring{The sampling distribution of \(\bar Y\)}{The sampling distribution of \textbackslash bar Y}}\label{the-sampling-distribution-of-bar-y}}

\begin{itemize}
\tightlist
\item
  The sample mean can be used, not only as a description of the data in the sample, but also as an estimate of the population mean μ.
\item
  It is natural to ask, ``How close to μ is \(\bar y\)?'' We cannot answer this question for the mean \(\bar y\) of a particular sample, but we can answer it if we think in terms of the random sampling model and regard the sample mean as a random variable \(\bar Y\).
\item
  The question then becomes: ``How close to μ is \(\bar Y\) likely to be?'' and the answer is provided by the \textbf{sampling distribution of \(\bar Y\)} - that is, the probability distribution that describes sampling variability in \(\bar Y\)
\item
  In order to visualize the sampling distribution of \(\bar Y\), imagine repeated samples of size n are drawn from a population with fixed mean µ and standard deviation σ. The variation of the \(\bar y's\) among the samples is specified by the sampling distribution of \(\bar Y\)
\end{itemize}

\includegraphics[width=0.5\linewidth]{./3_23}

\hypertarget{example-sampling-distribution-of-bar-y-when-n10}{%
\subsection{\texorpdfstring{Example: Sampling distribution of \(\bar Y\) when n=10}{Example: Sampling distribution of \textbackslash bar Y when n=10}}\label{example-sampling-distribution-of-bar-y-when-n10}}

\includegraphics[width=0.5\linewidth]{./3_24}

\hypertarget{example-sampling-distribution-of-bar-y-when-n30}{%
\subsection{\texorpdfstring{Example: Sampling distribution of \(\bar Y\) when n=30}{Example: Sampling distribution of \textbackslash bar Y when n=30}}\label{example-sampling-distribution-of-bar-y-when-n30}}

\includegraphics[width=0.5\linewidth]{./3_25}

\hypertarget{example-sampling-distribution-of-bar-y-when-n100}{%
\subsection{\texorpdfstring{Example: Sampling distribution of \(\bar Y\) when n=100}{Example: Sampling distribution of \textbackslash bar Y when n=100}}\label{example-sampling-distribution-of-bar-y-when-n100}}

\includegraphics[width=0.5\linewidth]{./3_26}

\hypertarget{example-sampling-distribution-of-bar-y-when-n1000}{%
\subsection{\texorpdfstring{Example: Sampling distribution of \(\bar Y\) when n=1000}{Example: Sampling distribution of \textbackslash bar Y when n=1000}}\label{example-sampling-distribution-of-bar-y-when-n1000}}

\includegraphics[width=0.5\linewidth]{./3_27}

\hypertarget{example-1}{%
\subsection{Example 1}\label{example-1}}

\begin{itemize}
\tightlist
\item
  We notice that the mean of the sampling distribution gets very close to µ even with smaller sample sizes. This only improves as n increases
\item
  As n increases, there is a very clear decrease in the standard deviation of the means across a 100 samples
\item
  Finally, we notice that the shape of the sampling distribution is increasingly like a normal distribution as n increases
\end{itemize}

\hypertarget{the-sampling-distribution-of-bar-y-1}{%
\subsection{\texorpdfstring{The sampling distribution of \(\bar Y\)}{The sampling distribution of \textbackslash bar Y}}\label{the-sampling-distribution-of-bar-y-1}}

\begin{itemize}
\tightlist
\item
  \textbf{Mean}: The mean of the sampling distribution of \(\bar Y\) is equal to the population mean, i.e.~\(E(\bar Y)=\mu_{\bar Y}=\mu\)
\item
  \textbf{Standard deviation}: The standard deviation of the sampling distribution is equal to the population standard deviation divided by the square root of the sample size, i.e.~\(SD(\bar Y)=\sigma_{\bar Y}=\frac{\sigma}{\sqrt n}\). Note that this implies the \(Variance(\bar Y)=\sigma^2_{\bar Y}=\frac{\sigma^2}{n}\)
\item
  \textbf{Shape}

  \begin{itemize}
  \tightlist
  \item
    If the population distribution of Y is normal, then the sampling distribution is normal, regardless of the sample size n.~
  \item
    \emph{Central Limit Theorem}: If n is large, then the sampling distribution is approximately normal, even if the population distribution of Y is not normal
  \end{itemize}
\end{itemize}

\hypertarget{central-limit-theorem-1}{%
\subsection{Central Limit Theorem}\label{central-limit-theorem-1}}

\begin{itemize}
\tightlist
\item
  From the text by Moore and McCabe:
\end{itemize}

``The sampling distribution of \(\bar Y\) is normal if the underlying population itself is normal.

What happens when the population distribution is not normal? It turns out that as the \emph{sample size increases, the distribution of \(\bar Y\) becomes closer to a normal distribution}. This is true no matter what the population distribution may be, as long as the population has a finite standard deviation σ. This famous fact of probability theory is called the \emph{central limit theorem}. For large sample size n, we can regard \(\bar Y\) as having the \(N\left(\mu,\frac{\sigma}{\sqrt n}\right)\) distribution''

\hypertarget{example-1-1}{%
\subsection{Example 1}\label{example-1-1}}

\begin{itemize}
\tightlist
\item
  Applying the Central Limit Theorem, we can say that the sampling distribution of the mean serum cholesterol is:

  \begin{itemize}
  \tightlist
  \item
    \(N\left(\mu_{\bar Y}=155,\sigma_{\bar Y}=\frac{27}{\sqrt {10}}=8.54\right)\) when n=10
  \item
    \(N\left(\mu_{\bar Y}=155,\sigma_{\bar Y}=\frac{27}{\sqrt {30}}=4.93\right)\) when n=30
  \item
    \(N\left(\mu_{\bar Y}=155,\sigma_{\bar Y}=\frac{27}{\sqrt {100}}=2.7\right)\) when n=100
  \item
    \(N\left(\mu_{\bar Y}=155,\sigma_{\bar Y}=\frac{27}{\sqrt {1000}}=0.85\right)\) when n=1000
  \end{itemize}
\end{itemize}

Therefore, applying the rules pertaining to the normal distribution, we know that roughly 95\% of the sampling distribution lies in the following ranges depending on the size of n:

\includegraphics[width=0.5\linewidth]{./3_32}

\hypertarget{theory-related-to-the-sums-of-random-variables}{%
\subsection{Theory related to the sums of random variables}\label{theory-related-to-the-sums-of-random-variables}}

\begin{itemize}
\tightlist
\item
  These two slides help to see how \(\frac{\sigma}{\sqrt n}\) arises.
\item
  Let X and Y be two arbitrary, independent random variables. Then from probability theory we know that:

  \begin{itemize}
  \tightlist
  \item
    E(X+Y) = E(X) + E(Y)
  \item
    Var(X + Y) = Var(X) + Var(Y)
  \item
    E(aX+bY) = aE(X) + bE(Y), where a and b are constants
  \item
    \(Var(aX+bY) = a^2 Var(X) + b^2 Var(Y)\)
  \item
    If X \textasciitilde{} \(N(\mu_X,\sigma_X^2)\) and Y \textasciitilde{} \(N(\mu_Y,\sigma_Y^2)\) then (X+Y) \textasciitilde{} \(N(\mu_X+\mu_Y,\sigma_X^2+\sigma_Y^2)\)
  \end{itemize}
\end{itemize}

\hypertarget{some-examples-related-to-the-sums-of-independent-random-variables}{%
\subsection{Some examples related to the sums of independent random variables}\label{some-examples-related-to-the-sums-of-independent-random-variables}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If X \textasciitilde{} \(N(\mu_X=0,\sigma_X^2=1)\) and Y \textasciitilde{} \(N(\mu_Y=3,\sigma_Y^2=4)\),\\
  then X+Y \textasciitilde{} N(mean=3, variance=5)
\item
  If \(X_1,X_2,...,X_n\) \textasciitilde{} N(0,1), then \(\sum_{i=1}^nX_i\) \textasciitilde{} N(0,n)
\item
  \ldots{} and then, \(\frac{1}{n}\sum_{i=1}^nX_i\) \textasciitilde{} \(N(0,\frac{1}{n})\)
\item
  If \(X_1,X_2,...,X_n\) \textasciitilde{} \(N(\mu,\sigma^2)\), then \(\sum_{i=1}^nX_i\) \textasciitilde{} \(N(n\mu,n\sigma^2)\)
\item
  \ldots{} and then, \(\frac{1}{n}\sum_{i=1}^nX_i\) \textasciitilde{} \(N(\mu,\frac{\sigma^2}{n})\)
\end{enumerate}

\hypertarget{excerpt-from-lawrence-josephs-notes}{%
\subsubsection{Excerpt from Lawrence Joseph's notes}\label{excerpt-from-lawrence-josephs-notes}}

\includegraphics[width=0.5\linewidth]{./3_35}

\hypertarget{example-2-central-limit-theorem-in-action}{%
\subsection{Example 2: Central Limit Theorem in action}\label{example-2-central-limit-theorem-in-action}}

\begin{itemize}
\tightlist
\item
  What is the average time taken across the 50 students in the class?
\item
  R code to replicate
\end{itemize}

x1 = rnorm(50,4,1) \# walk to bus stop
x2 = runif(50,4,16) \# wait for bus
x3 = rnorm(50,20,2) \# bus ride
x4 = rgamma(50,shape=3/2,scale=2) \# trudge up hill

par(mfrow=c(2,3))
hist(x1);hist(x2);hist(x3);hist(x4)
hist(x1+x2+x3+x4,xlab=``Sum for 50 students'',main=````)
hist((x1+x2+x3+x4)/4,xlab=''Mean for 50 students'',main=``\,``)

\hypertarget{confidence-intervals-for-means}{%
\section{Confidence intervals for means}\label{confidence-intervals-for-means}}

\hypertarget{confidence-interval-estimation-for-a-single-mean}{%
\subsection{Confidence interval estimation for a single mean}\label{confidence-interval-estimation-for-a-single-mean}}

\includegraphics[width=0.5\linewidth]{./3_38}

\begin{itemize}
\tightlist
\item
  The construction of a confidence interval relies on the principal of the central limit theorem
\item
  If, we can reasonably assume that the sample mean follows a normal distribution with mean µ and standard deviation \(\frac{\sigma}{\sqrt n}\)
\item
  Then, across repeated samples, 95\% of samples' means \((\bar x's)\) lie in the interval \(\left(\mu-2\frac{\sigma}{\sqrt n},\mu+2\frac{\sigma}{\sqrt n}\right)\)
\item
  This implies that 95\% of the intervals \(\left(\bar x-2\frac{\sigma}{\sqrt n},\bar x+2\frac{\sigma}{\sqrt n}\right)\) will include \(\mu\). This interval is called the 95\% confidence interval for µ
\item
  More generally, \((1-\alpha)\)\% of the intervals \(\left(\bar x-Z_{(1-\frac{\alpha}{2})}\frac{\sigma}{\sqrt n},\bar x+Z_{(1-\frac{\alpha}{2})}\frac{\sigma}{\sqrt n}\right)\) will include \(\mu\).
\item
  This interval is called the (1-α)\% equal-tailed confidence interval for µ, where \(Z_{(1-\frac{\alpha}{2})}\) is the (1- α/2) quantile of the standard normal distribution
\item
  Equal-tailed refers to the fact that the probability of (1-α) is divided equally in the two tails of the distribution
\item
  Notice that the 95\% or (1-α)\% in the definition refers to a percentage across repeated experiments
\item
  We cannot say whether the 95\% confidence interval estimated from the sample at hand is one of the ones that captured the true value of µ or not
\item
  The population standard deviation (σ) is seldom known and must be substituted by the sample standard deviation (s)
\item
  Does the assumption of 95\% confidence still hold? It turns out that it does but we must replace the quantile \(Z_{(1-\frac{\alpha}{2})}\) from the normal distribution by the \(t_{(1-\frac{\alpha}{2})}\) quantile from the Student's t-distribution (or t-distribution for short)
\item
  The resulting expression for the confidence interval is given by:
\end{itemize}

\[\left(\bar x-t_{(1-\frac{\alpha}{2}),n-1}\frac{s}{\sqrt n},\bar x+t_{(1-\frac{\alpha}{2}),n-1}\frac{s}{\sqrt n}\right)\]

where \(t_{(1-\frac{\alpha}{2}),n-1}\) is the (1- α/2) quantile of the t-distribution with n-1 degrees of freedom

\hypertarget{t-distribution}{%
\subsection{t-distribution}\label{t-distribution}}

\includegraphics[width=0.5\linewidth]{./3_41}

Image from Wikipedia

\begin{itemize}
\tightlist
\item
  The t-distribution was discovered by the British scientist W. S. Gossett who was employed by the Guiness Brewery.

  \begin{itemize}
  \tightlist
  \item
    He published his work in 1908 under the pseudonym Student
  \end{itemize}
\item
  The t-distribution is a bell-shaped, symmetrically distribution over the range -∞ to ∞. It resembles the normal distribution, but has a higher standard deviation.
\item
  The exact shape of the distribution depends on a quantity called the degrees of freedom (ν in the illustration). The higher the value of ν the closer it is to a normal distribution
\end{itemize}

Probability density function centred at 0

\[f(x|v) = \frac{\Gamma\left(\frac{v+1}{2}\right)}{\sqrt {v\pi}\Gamma\left(\frac{v}{2}\right)}\left(1+\frac{x^2}{v}\right)^{-\frac{v+1}{2}},-\infty<x<\infty\]

Mean=0

Variance=\(\frac{v}{v-2}\)

\hypertarget{example-1-serum-potassium-concentration}{%
\subsection{Example 1: Serum Potassium Concentration}\label{example-1-serum-potassium-concentration}}

\includegraphics[width=0.5\linewidth]{./3_42}

\begin{itemize}
\tightlist
\item
  As part of a study of natural variation in blood chemistry, serum potassium concentrations were measured in 84 healthy women.
\item
  The mean concentration was 4.36 mEq/l, and the standard deviation was 0.42 mEq/l.
\item
  The table presents a frequency distribution of the data
\item
  Calculate the standard error of the mean
\item
  Construct a histogram of the data and indicate the intervals mean ± SD and mean ± SE
\item
  Construct a 95\% confidence interval for the population mean. Interpret this confidence interval
\item
  Would this interval be suitable to define ``reference limits'' for serum potassium in healthy women, i.e.~the limits within which we would expect to find 95\% of healthy people?
\item
  Suppose a similar study is to be conducted the following year among 200 women. What would you predict would be

  \begin{itemize}
  \tightlist
  \item
    the SD of the new measurements?
  \item
    the SE of the new measurements?
  \end{itemize}
\end{itemize}

\hypertarget{example-1-histogram-of-the-data}{%
\subsection{Example 1: Histogram of the data}\label{example-1-histogram-of-the-data}}

\includegraphics[width=0.5\linewidth]{./3_44}

\hypertarget{verifying-assumptions-behind-the-t-distribution-confidence-interval}{%
\subsection{Verifying assumptions behind the t-distribution confidence interval}\label{verifying-assumptions-behind-the-t-distribution-confidence-interval}}

\begin{itemize}
\tightlist
\item
  Does the central limit theorem hold?
\item
  In other words, do at least one of the following conditions hold

  \begin{itemize}
  \tightlist
  \item
    the data follow an approximately normal distribution?
  \item
    the sample size is large
  \end{itemize}
\item
  For the serum potassium example both conditions appear to hold
\end{itemize}

\hypertarget{example-1-standard-error-and-95-confidence-interval}{%
\subsection{Example 1: Standard Error and 95\% confidence interval}\label{example-1-standard-error-and-95-confidence-interval}}

\begin{itemize}
\tightlist
\item
  The standard error of the mean (SE)\\
  \(=\frac{SD}{\sqrt n}=\frac{0.42}{\sqrt {84}}=0.05\) mEq/l, after rounding
\item
  The 95\% confidence interval\\
  \(=\left(\bar x-t_{(1-\frac{\alpha}{2}),n-1}\frac{s}{\sqrt n},\bar x+t_{(1-\frac{\alpha}{2}),n-1}\frac{s}{\sqrt n}\right)\)\\
  \(=(4.36 – t_{0.975,84-1} 0.05, 4.36 + t_{0.975,84-1} 0.05)\)\\
  = (4.36 -- 1.98 × 0.05, 4.36 + 1.98 × 0.05)\\
  = (4.26, 4.46) mEq/l
\end{itemize}

\hypertarget{interpretation-of-the-95-confidence-interval}{%
\subsection{Interpretation of the 95\% confidence interval}\label{interpretation-of-the-95-confidence-interval}}

\begin{itemize}
\tightlist
\item
  Assuming that the sample at hand is a random sample, there is a 95\% probability that the procedure used to calculate the interval (4.26, 4.46) will capture the population mean serum potassium concentration
\item
  It would \textbf{not} be correct to say: There is a 95\% probability that the population mean serum concentration lies between 4.26 and 4.46 mEq/l
\end{itemize}

\hypertarget{confidence-level}{%
\subsection{Confidence level}\label{confidence-level}}

\begin{itemize}
\tightlist
\item
  The higher the confidence level, the wider the confidence interval would be
\end{itemize}

\includegraphics[width=1\linewidth]{./3_48}

\begin{itemize}
\tightlist
\item
  qt(prob,df) is the R function that returns the t-distribution quantile

  \begin{itemize}
  \tightlist
  \item
    Arguments provided are the cumulative probability and the degrees of freedom
  \end{itemize}
\end{itemize}

\hypertarget{example-1-distribution-of-the-data-with-intervals}{%
\subsection{Example 1: Distribution of the data (with intervals)}\label{example-1-distribution-of-the-data-with-intervals}}

\includegraphics[width=0.5\linewidth]{./3_49}

\hypertarget{interpreting-the-confidence-interval}{%
\subsection{Interpreting the confidence interval}\label{interpreting-the-confidence-interval}}

\begin{itemize}
\tightlist
\item
  \emph{Would the 95\% confidence interval be suitable to define ``reference limits'' for serum potassium in healthy women, i.e.~the limits within which we would expect to find 95\% of healthy people?}
\item
  No.~The 95\% interval attempts to captures the uncertainty in the \textbf{mean} of the distribution.
\item
  In the expression for the confidence interval, if we replaced the standard error by the standard deviation, we would get the desired reference limits
\end{itemize}

\hypertarget{standard-error-vs-standard-deviation}{%
\subsection{Standard error vs Standard deviation}\label{standard-error-vs-standard-deviation}}

\begin{itemize}
\tightlist
\item
  \emph{Suppose a similar study is to be conducted the following year among 200 women. What would you predict would be}

  \begin{itemize}
  \tightlist
  \item
    \emph{the SD of the new measurements?}
  \item
    \emph{the SE of the new measurements?}
  \end{itemize}
\item
  Our best prediction for the SD would be the value in the smaller sample of 84, namely 0.42 mEq/l
\item
  However, the SE of the new measurements would decrease from 0.05 to \(\frac{0.42}{\sqrt {200}}\) = 0.03 mEq/l
\end{itemize}

\hypertarget{confidence-interval-for-the-difference-between-two-means}{%
\section{Confidence interval for the difference between two means}\label{confidence-interval-for-the-difference-between-two-means}}

\hypertarget{example-2-nck1-deficiency-and-adipogenesis}{%
\subsection{Example 2: Nck1 deficiency and adipogenesis}\label{example-2-nck1-deficiency-and-adipogenesis}}

\includegraphics[width=0.5\linewidth]{./3_53}

\begin{itemize}
\tightlist
\item
  Obesity results from an excessive expansion of white adipose tissue (WAT), which is still poorly understood from an etiologic-mechanistic perspective
\item
  A study from the MUHC-RI reported on the role of the Nck1 adaptor protein during WAT expansion and in vitro adipogenesis
\item
  Two outcomes of interest were body weight and adipose weight
\end{itemize}

\includegraphics[width=0.5\linewidth]{./3_54}

\begin{itemize}
\tightlist
\item
  Nck1 wild type (Nck1+/+) and knock-out mice (Nck1-/-) were compared at baseline and at 16 weeks
\item
  Two research questions of interest: Is there a difference in wild-type and knock-out mice in terms of

  \begin{itemize}
  \tightlist
  \item
    Body weight
  \item
    Adipose weight
  \end{itemize}
\item
  What would be considered a meaningful change on these two outcomes?
\item
  In order to apply the Central Limit Theorem we would ask:

  \begin{itemize}
  \tightlist
  \item
    Is it reasonable to assume that body weight and adipose weight follow an approximately normal distribution?
  \item
    If not, is the sample size sufficiently large?
  \end{itemize}
\item
  The sample size is not large, so the approximate normality must hold to construct a t-distribution-based confidence interval
\end{itemize}

\includegraphics[width=1\linewidth]{./3_55}

\hypertarget{confidence-interval-for-the-difference-between-means-from-two-independent-samples}{%
\subsection{Confidence interval for the difference between means from two independent samples}\label{confidence-interval-for-the-difference-between-means-from-two-independent-samples}}

The (1-α)\% confidence interval comparing two means from independent samples is given by

\[\bar x_1-\bar x_2-t_{(1-\alpha/2),df}s_{diff},\bar x_1-\bar x_2+t_{(1-\alpha/2),df}s_{diff}\]

where

\includegraphics[width=0.5\linewidth]{./3_56}

\hypertarget{variance-of-the-difference-in-means}{%
\subsection{Variance of the difference in means}\label{variance-of-the-difference-in-means}}

\includegraphics[width=0.5\linewidth]{./3_57}

\hypertarget{calculating-degrees-of-freedom-of-the-t-distribution-when-variances-are-not-equal}{%
\subsection{Calculating degrees of freedom of the t-distribution when variances are not equal}\label{calculating-degrees-of-freedom-of-the-t-distribution-when-variances-are-not-equal}}

\begin{itemize}
\tightlist
\item
  The degrees of freedom can be set to min(n1-1, n2-1), which is a conservative value. This is a useful approach if you are doing the t-test by hand
\item
  Alternatively, a computer program may use a more complex method called the Welch's method or Satterthwaite's method to calculate the degrees of freedom as follows:
\end{itemize}

\[\frac{(se_1^2+se_2^2)^2}{\frac{se_1^4}{n1-1}+\frac{se_2^4}{n2-1}},\]

where \(se_1 = se_1/\sqrt{n1}\) and \(se_2 = se_2/\sqrt{n1}\)

\hypertarget{example-2-nck1-deficiency-and-adipogenesis-1}{%
\subsection{Example 2: Nck1 deficiency and adipogenesis}\label{example-2-nck1-deficiency-and-adipogenesis-1}}

\begin{itemize}
\tightlist
\item
  Based on the sample estimates, and perhaps from information gathered previously, it may be reasonable to assume that the variance is the same in both groups being compared
\item
  Since we are assuming that the variance is the same, it is reasonable to calculate a pooled variance that averages across both groups.
\end{itemize}

\hypertarget{calculating-the-pooled-variance-for-body-weight}{%
\subsection{Calculating the pooled variance for body weight}\label{calculating-the-pooled-variance-for-body-weight}}

\begin{itemize}
\tightlist
\item
  The pooled variance is given by
\end{itemize}

\[s_p^2=\frac{(n1-1)s_1^2+(n2-1)s_2^2}{n1+n2-2}=\frac{15*5.4*5.4+8*5.6*5.6}{16+9-2}=29.9\]

\begin{itemize}
\tightlist
\item
  Therefore the pooled standard deviation is given by the square root of 29.9 or \(s_p=5.5\)
\item
  The value of \(s_{diff}=s_p\sqrt{\frac{1}{n1}+\frac{1}{n2}}=5.5\sqrt{\frac{1}{16}+\frac{1}{9}}=2.3\)
\end{itemize}

\hypertarget{confidence-interval-for-difference-in-body-weight}{%
\subsection{Confidence interval for difference in body weight}\label{confidence-interval-for-difference-in-body-weight}}

\begin{itemize}
\tightlist
\item
  The difference in mean body weight between Nck1+/+ and Nck1-/- mice is \(\bar y_1-\bar y_2=38.2-35.7=2.5\)
\item
  95\% confidence interval for the difference in means is
\end{itemize}

\(\bar y_1-\bar y_2-t_{(1-\alpha/2),n1+n2-2}s_{diff},\bar y_1-\bar y_2+t_{(1-\alpha/2),n1+n2-2}s_{diff}\)

= (2.5 -- 2.07 × 2.3, 2.5 + 2.07 × 2.3)

= (-2.3, 7.3)

\hypertarget{confidence-intervals-comparing-the-two-groups}{%
\subsection{Confidence intervals comparing the two groups}\label{confidence-intervals-comparing-the-two-groups}}

\includegraphics[width=1\linewidth]{./3_62}

\begin{itemize}
\tightlist
\item
  The assumption of unequal variance results in a lower value for the degrees of freedom and would typically be more conservative
\end{itemize}

\hypertarget{interpreting-the-confidence-interval-1}{%
\subsection{Interpreting the confidence interval}\label{interpreting-the-confidence-interval-1}}

\begin{itemize}
\tightlist
\item
  As in the case of a single mean, we have 95\% confidence in the procedure used to construct the interval.

  \begin{itemize}
  \tightlist
  \item
    We cannot say if this interval based on our sample includes the true mean difference between Nck1 +/+ and Nck1 -/- mice
  \end{itemize}
\item
  Say we consider 5g to be a meaningful difference in body weight

  \begin{itemize}
  \tightlist
  \item
    This implies, though the confidence interval includes 0, the upper limit crosses 5g suggesting we cannot eliminate the possibility there is a meaningful difference. Ideally, the study should be repeated to obtain a more precise estimate
  \end{itemize}
\item
  Say we consider a 0.5g to be a clinically meaningful difference in adipose weight

  \begin{itemize}
  \tightlist
  \item
    The interval provides evidence for a statistically significant difference, but does eliminate the possibility that the difference may not be clinically meaningful difference as the lower limit lies below 0.5g
  \end{itemize}
\end{itemize}

\hypertarget{sample-size-calculations}{%
\section{Sample size calculations}\label{sample-size-calculations}}

\begin{itemize}
\tightlist
\item
  Before collecting data for a research study, it is wise to consider in advance whether the estimates generated from the data will be sufficiently precise.
\item
  It can be painful indeed to discover after a long and expensive study that the standard errors are so large that the primary questions addressed by the study cannot be answered.
\end{itemize}

\hypertarget{an-illustration}{%
\subsection{An illustration}\label{an-illustration}}

\includegraphics[width=0.5\linewidth]{./3_66}

\begin{itemize}
\tightlist
\item
  \url{https://www.youtube.com/watch?v=PbODigCZqL8}
\end{itemize}

\hypertarget{sample-size-calculation}{%
\subsection{Sample size calculation}\label{sample-size-calculation}}

\begin{itemize}
\tightlist
\item
  The method one uses for the sample size calculation depends on the plan for the statistical inference
\item
  Accordingly, depending on whether you intend to report a hypothesis test, or a confidence interval or a Bayesian analysis, your method for sample size calculation may change
\end{itemize}

\hypertarget{sample-size-calculation-for-reporting-a-confidence-interval}{%
\subsection{Sample size calculation for reporting a confidence interval}\label{sample-size-calculation-for-reporting-a-confidence-interval}}

\begin{itemize}
\tightlist
\item
  This approach is relevant when we want to estimate a parameter within a certain precision, with a high level of confidence.
\item
  For example, we might want to estimate

  \begin{itemize}
  \tightlist
  \item
    mean change in body weight in mice within ± 2.5g of the true value with 99\% confidence
  \item
    mean serum cholesterol in middle-aged men within ± 6mg/dL of its true value with 90\% confidence
  \end{itemize}
\end{itemize}

\hypertarget{example-method-for-a-single-mean}{%
\subsection{Example: Method for a single mean}\label{example-method-for-a-single-mean}}

\begin{itemize}
\tightlist
\item
  A medical researcher proposes to estimate the mean serum cholesterol level of a certain population of middle-aged men, based on a random sample of the population.
\item
  He asks a statistician for advice. The ensuing discussion reveals that the researcher wants to estimate the population mean to within δ = ±6 mg/dl or less, with 95\% confidence.
\item
  Also, the researcher believes that the standard deviation of serum cholesterol in the population is probably about s=40 mg/dl.
\item
  How large a sample does the researcher need to take?
\end{itemize}

\hypertarget{the-desired-precision-is-much-smaller-than-the-standard-deviation-of-the-variable}{%
\subsection{The desired precision is much smaller than the standard deviation of the variable}\label{the-desired-precision-is-much-smaller-than-the-standard-deviation-of-the-variable}}

\includegraphics[width=0.5\linewidth]{./3_70}

\hypertarget{example-method-for-a-single-mean-1}{%
\subsection{Example: Method for a single mean}\label{example-method-for-a-single-mean-1}}

\begin{itemize}
\tightlist
\item
  The research question can be re-expressed as
\end{itemize}

``What is the sample size required to calculate a 95\% confidence interval for the mean serum cholesterol which has half-width 6mg / dL?''

\begin{itemize}
\tightlist
\item
  Recall that the general expression for the (1- α)\% confidence interval is
\end{itemize}

\[\bar x-t_{(1-\alpha/2),n-1}\frac{s}{\sqrt n},\bar x+t_{(1-\alpha/2),n-1}\frac{s}{\sqrt n}\]

\begin{itemize}
\tightlist
\item
  In other words, we need to find out how large n should be so that
\end{itemize}

\[t_{(1-\alpha/2),n-1}\frac{s}{\sqrt n}=\delta=6\]

\begin{itemize}
\tightlist
\item
  To solve this expression for n, we need to know the values of \(t_{(1-\alpha/2),n-1}\) and the value of s, the standard deviation
\item
  Since \(t_{(1-\alpha/2),n-1}\) itself depends on n, we cannot know its value without n! We therefore, replace it by the normal quantile \(Z_{(1-\alpha/2)}\). In our example, \(Z_{(1-\alpha/2)}=1.96\)
\item
  The value of s could be a guess value or determined from the literature or an earlier pilot study. In our example, s=40
\item
  Therefore, we wish to solve
\end{itemize}

\[Z_{(1-\alpha/2)}\frac{s}{\sqrt n}=1.96\frac{40}{\sqrt n}=\delta=6\]

\begin{itemize}
\tightlist
\item
  This implies \(\sqrt n = Z_{(1-\alpha/2)}\frac{s}{\delta}=1.96\frac{40}{6}\)
\item
  Or \(n = (1.96\frac{40}{6})^2 \approx 171\)
\end{itemize}

\hypertarget{alternative-values-of-ux3b1-s-and-ux3b4}{%
\subsection{Alternative values of α, s and δ}\label{alternative-values-of-ux3b1-s-and-ux3b4}}

\begin{tabular}{r|r|r|r}
\hline
alpha & s & delta & n\\
\hline
0.05 & 40 & 6 & 171\\
\hline
0.01 & 40 & 6 & 240\\
\hline
0.05 & 30 & 6 & 96\\
\hline
0.01 & 30 & 6 & 135\\
\hline
0.05 & 40 & 12 & 43\\
\hline
0.01 & 40 & 12 & 60\\
\hline
0.05 & 30 & 12 & 24\\
\hline
0.01 & 30 & 12 & 24\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  By varying the values of α, s and δ we can see how they impact the sample size
\item
  n increases if:\\
  α decreases, s increases or δ decreases\\
\item
  In practice, the sample size may be constrained by feasibility or cost. Using a table like this allows us to see how much precision we can `buy' with the available sample size
\end{itemize}

\hypertarget{example-sample-size-calculation-for-comparing-two-means}{%
\subsection{Example: Sample size calculation for comparing two means}\label{example-sample-size-calculation-for-comparing-two-means}}

\begin{itemize}
\tightlist
\item
  Consider the study on body weight in Nck+/+ vs Nck-/- mice
\item
  Lets say we wish to repeat the earlier study so that we can show more convincingly that there is a clinically meaningful difference
\item
  Earlier in the lecture we found that the \textbf{pooled} standard deviation of the difference was \(s_p=5.5g\)
\item
  We desire to ensure that the observed mean change lies within δ = ± 2.5 g of the true mean change with 95\% confidence.
\item
  What is the sample size required in each group (assuming the sample size is equal in both groups)?
\end{itemize}

\hypertarget{example-comparison-of-two-means}{%
\subsection{Example: Comparison of two means}\label{example-comparison-of-two-means}}

\begin{itemize}
\tightlist
\item
  To calculate the sample size required to estimate a 95\% CI with adequate precision we need to solve\\
  \(Z_{(1-\alpha/2)}s_{diff}=Z_{(1-\alpha/2)}s_p\sqrt{\frac{1}{n}+\frac{1}{n}}=\delta\)\\
  or \(1.96\times 5.5\times \sqrt{\frac{1}{n}+\frac{1}{n}}=2.5\)
\item
  This implies \(\sqrt n =1.96\frac{5.5\times\sqrt 2}{2.5}\)
\item
  Or \(n = 2(1.96\frac{5.5}{2.5})^2 \approx 37\) mice in each group
\end{itemize}

\hypertarget{lecture-4-inference-for-means-continued}{%
\chapter{Lecture 4: Inference for means continued}\label{lecture-4-inference-for-means-continued}}

\hypertarget{hypothesis-testing}{%
\section{Hypothesis testing}\label{hypothesis-testing}}

\hypertarget{example-1-nck1-and-adipogenesis-continued}{%
\subsection{Example 1: Nck1 and adipogenesis continued}\label{example-1-nck1-and-adipogenesis-continued}}

\begin{itemize}
\tightlist
\item
  Whereas in the previous lecture we saw how to carry out statistical inference about the differences between Nck1 wild type and Nck1 knock out mice using confidence intervals, the manuscript relied on hypothesis testing
\end{itemize}

\hypertarget{hypothesis-testing-1}{%
\subsection{Hypothesis testing}\label{hypothesis-testing-1}}

\begin{itemize}
\tightlist
\item
  Hypothesis testing is an alternative approach to statistical inference that also relies on the Central Limit Theorem
\item
  The research question takes the form of a decision making problem, e.g.~

  \begin{itemize}
  \tightlist
  \item
    Does mobility improve 3-months after a stroke?
  \item
    Is there a difference in the improvement in mobility between men and women after a stroke?
  \item
    Does Treatment A improve life-expectancy compared to Treatment B?
  \end{itemize}
\item
  Each of these questions can be answered yes or no. Each response can be expressed as a specific statement
\item
  We can view the response to the stroke mobility problem as a choice between the following two statements or hypotheses

  \begin{itemize}
  \tightlist
  \item
    \(H_0\): There is no improvement in mobility 3 months after stroke
  \item
    \(H_A\): There is improvement in mobility 3 months after stroke
  \end{itemize}
\item
  In general, a decision-making problem can be framed in terms of a null hypothesis \((H_0)\) and an alternative hypothesis \((H_A)\). The \(H_A\) is the complement of the null hypothesis
\item
  We typically focus on the null hypothesis, which is usually simpler than the alternative hypothesis, and decide whether or not to reject it.
\item
  To this end, we examine the evidence that the observed data provide against the null hypothesis \(H_0\)
\item
  If the evidence against \(H_0\) is strong, \textbf{we reject \(H_0\)}
\item
  If not, we state that the evidence provided by the data is not strong enough, and \textbf{we fail to reject \(H_0\)}.
\end{itemize}

\hypertarget{hypothesis-testing-for-a-single-mean}{%
\subsection{Hypothesis testing for a single mean}\label{hypothesis-testing-for-a-single-mean}}

\begin{itemize}
\tightlist
\item
  The hypothesis test may be set up with

  \begin{itemize}
  \tightlist
  \item
    a two-sided alternative
  \item
    or a one-sided alternative
  \end{itemize}
\item
  resulting in 3 different possibilities mentioned in
  the following slides
\end{itemize}

\hypertarget{mobility-after-stroke-two-sided-alternative-hypothesis}{%
\subsection{Mobility after stroke: two-sided alternative hypothesis}\label{mobility-after-stroke-two-sided-alternative-hypothesis}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is 0 units** & \$H\_A\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is not 0 units**\\
\hline
\end{tabular}

\hypertarget{mobility-after-stroke-one-sided-alternative-hypothesis-i}{%
\subsection{Mobility after stroke: one-sided alternative hypothesis I}\label{mobility-after-stroke-one-sided-alternative-hypothesis-i}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is less than or equal to 0 units** & \$H\_A\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is greater than 0 units**\\
\hline
\end{tabular}

\hypertarget{mobility-after-stroke-one-sided-alternative-hypothesis-ii}{%
\subsection{Mobility after stroke: one-sided alternative hypothesis II}\label{mobility-after-stroke-one-sided-alternative-hypothesis-ii}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is greater than or equal to 0 units** & \$H\_A\$: The **true mean** change in the STREAM score between 3-days and 3 months post stroke **is less than 0 units**\\
\hline
\end{tabular}

\hypertarget{more-generally-the-hypothesis-test-for-a-single-mean-may-be-stated-as-follows}{%
\subsection{More generally, the hypothesis test for a single mean may be stated as follows}\label{more-generally-the-hypothesis-test-for-a-single-mean-may-be-stated-as-follows}}

\begin{itemize}
\tightlist
\item
  The null and alternative hypotheses for a two-sided test may be stated as
\end{itemize}

\(H_0: µ = µ_0\space\space\space\space\space vs \space\space\space\space\space H_A: µ ≠ µ_0\)

where µ denotes the true population mean \(µ_0\) is a known constant

\begin{itemize}
\tightlist
\item
  The null and alternative hypotheses for a one-sided test can be stated as follows
\end{itemize}

\(H_0: µ ≤ µ_0 \space\space\space\space\space vs \space\space\space\space\space H_A: µ > µ_0\)

OR

\(H_0: µ ≥ µ_0\space\space\space\space\space vs \space\space\space\space\space H_A: µ < µ_0\)

\hypertarget{example-mobility-after-stroke}{%
\subsection{Example: Mobility after stroke}\label{example-mobility-after-stroke}}

\begin{tabular}{l|r|r|l}
\hline
  & Three days after stroke & Three months after stroke & Difference\\
\hline
Number of cases & 235.00 & 235.00 & 235\\
\hline
Minimum & 0.00 & 0.00 & -22.22\\
\hline
Maximum & 100.00 & 100.00 & 91.67\\
\hline
Mean & 68.30 & 83.75 & \$\textbackslash{}bar y = 15.45\$\\
\hline
Standard deviation & 30.12 & 22.74 & s = 18.97\\
\hline
\end{tabular}

\hypertarget{defining-the-test-statistic-and-the-rejection-region}{%
\subsection{Defining the test statistic and the rejection region}\label{defining-the-test-statistic-and-the-rejection-region}}

\begin{itemize}
\item
  Recall that based on the Central Limit Theorem,\\
  \(\bar Y\) \textasciitilde{} \(N(\mu,\sigma^2/n)\) or \(\bar Y\) \textasciitilde{} \(N(\mu,\sigma^2/235)\)
\item
  We can also express this as \(\frac{\bar Y-\mu}{\frac{\sigma}{\sqrt n}}\) follow a standard normal distribution
\item
  We can use our knowledge of the sampling distribution of \(\frac{\bar Y-\mu}{\frac{\sigma}{\sqrt n}}\) (the test statistic) to determine which values are likely under the null hypothesis
\item
  We define a rejection region such that if test statistic falls in this region we reject the null hypothesis
\end{itemize}

\hypertarget{defining-the-t-test-statistic}{%
\subsection{Defining the t-test statistic}\label{defining-the-t-test-statistic}}

\begin{itemize}
\tightlist
\item
  As in the case of the construction of a confidence interval, we are faced, with the problem that we seldom know the true standard deviation.
\item
  We can \textbf{estimate} the value of the unknown population standard deviation using the sample standard deviation \(\hat{\sigma}=s=18.97\)
\item
  The standardized test statistic is then \(\frac{\bar Y-\mu}{\frac{\sigma}{\sqrt n}}=\frac{15.45}{\frac{18.97}{\sqrt{235}}}=12.49\)
\item
  This statistic is referred to as the \textbf{t-statistic} as it follows a t-distribution with n-1 degrees of freedom
\item
  The corresponding hypothesis test is called the \textbf{t-test}.
\end{itemize}

\hypertarget{rejection-region-for-the-t-test}{%
\subsection{Rejection region for the t-test}\label{rejection-region-for-the-t-test}}

\includegraphics[width=0.5\linewidth]{./4_15}

\begin{itemize}
\tightlist
\item
  Our goal is to select a rejection region such that it covers values that are unlikely under the null hypothesis
\item
  The form of rejection region depends on the statement of the alternative hypothesis. * We first consider the two-sided alternative. Under this alternative hypothesis, the rejection region covers the extremes of the distribution on both sides
\item
  The two areas each covering with 0.025 probability in the extremes are unlikely under the null hypothesis as illustrated by the diagram. They correspond to a \textbf{Type I error of 0.025+0.025 = 0.05}, which we will define shortly
\item
  Under the t-distribution with degrees of freedom = n-1 = 234, these areas may be identified by the quantiles \(Q_{0.025} = -1.97\) and \(Q_{0.975} = 1.97\)

  \begin{itemize}
  \tightlist
  \item
    Therefore, if the t-statistic is above 1.97 or less than -1.97 we reject the null hypothesis
  \end{itemize}
\item
  In our example, 12.49 is well above 1.97 so we \textbf{reject the null hypothesis}
\end{itemize}

\hypertarget{comparison-to-confidence-interval}{%
\subsection{Comparison to confidence interval}\label{comparison-to-confidence-interval}}

\begin{itemize}
\tightlist
\item
  The hypothesis testing approach resulted in a similar conclusion to the equal-tailed confidence interval derived earlier in that we concluded that mobility improves 3 months after stroke
\item
  In fact, the equal-tailed 95\% confidence interval derived previously gives the range of possible values of the null hypothesis that cannot be rejected.

  \begin{itemize}
  \tightlist
  \item
    All values outside that interval will be rejected
  \item
    That happens to include the value of 5 units which defines a clinically meaningful improvement
  \end{itemize}
\end{itemize}

\hypertarget{determining-the-rejection-region-using-r}{%
\subsection{Determining the rejection region using R}\label{determining-the-rejection-region-using-r}}

We use the qt() function to obtain the quantiles of a t-distribution corresponding to the desired tail-area probability

\begin{quote}
qt(0.025,234)\\
{[}1{]} -1.970154
\end{quote}

\begin{quote}
qt(0.975,234)\\
{[}1{]} 1.970154
\end{quote}

The sample size is very large. Therefore, for all practical purposes the t-distribution with degrees of freedom n-1 = 234 is like a normal distribution

\begin{quote}
qnorm(0.025)\\
{[}1{]} -1.959964
\end{quote}

\begin{quote}
qnorm(0.975)\\
{[}1{]} 1.959964
\end{quote}

\hypertarget{type-i-and-type-ii-errors}{%
\subsection{Type I and Type II errors}\label{type-i-and-type-ii-errors}}

\begin{itemize}
\tightlist
\item
  With respect to our decision regarding the null hypothesis we can make two types of errors

  \begin{itemize}
  \tightlist
  \item
    Type I error (α): We reject \(H_0\) when it is true
  \item
    Type II error (β): We fail to reject \(H_0\) when it is not true (i.e.~when \(H_A\) is true)
  \end{itemize}
\item
  Clearly, we wish to minimize the chance of these errors. Typical values are α=0.05 and β=0.2
\end{itemize}

\hypertarget{hypothesis-testing-a-summary}{%
\subsection{Hypothesis testing: A summary}\label{hypothesis-testing-a-summary}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define null and alternative hypotheses
\item
  Define test statistic
\item
  Define rejection region with suitably selected Type I error α
\item
  If test statistic lies in the rejection region then reject null hypothesis, otherwise conclude that you do not have enough evidence to reject the null hypothesis
\end{enumerate}

\hypertarget{similarity-between-diagnostic-testing-and-hypothesis-testing}{%
\subsection{Similarity between diagnostic testing and hypothesis testing}\label{similarity-between-diagnostic-testing-and-hypothesis-testing}}

\includegraphics[width=0.5\linewidth]{./4_21a}

\begin{itemize}
\tightlist
\item
  Sensitivity = A / (A+C)
\item
  Specificity = D / (B+D)
\item
  A, B, C and D are numbers of individuals tested
\end{itemize}

\includegraphics[width=0.5\linewidth]{./4_21b}

\begin{itemize}
\tightlist
\item
  1-Type II error (Power) = A / (A+C)
\item
  1-Type I error = D / (B+D)
\item
  A, B, C and D are values of the test statistic observed across repeated experiments
\end{itemize}

\hypertarget{defining-the-t-test-statistic-for-a-one-sided-test}{%
\subsection{Defining the t-test statistic, for a one-sided test}\label{defining-the-t-test-statistic-for-a-one-sided-test}}

\begin{itemize}
\item
  Consider the situation where we pose the null and alternative hypotheses as follows\\
  \(H_0: µ ≤ µ_0 \space\space\space\space\space vs \space\space\space\space\space H_A: µ > µ_0\)
\item
  The test statistic is still evaluated at \(µ = µ_0 = 0\) as before
\item
  However, the rejection region is one-sided. In order to ensure that the rejection region has a 5\% probability as in our previous example, we will define it as the region above \(Q_{0.95} = 1.65\)
\item
  For our example, the t-statistic would remain unchanged at 12.49 and therefore would lie in the rejection region once again, leading to the same conclusion as before
\end{itemize}

\hypertarget{why-did-the-test-statistic-not-change-for-the-one-sided-hypothesis-test}{%
\subsection{Why did the test statistic not change for the one-sided hypothesis test?}\label{why-did-the-test-statistic-not-change-for-the-one-sided-hypothesis-test}}

\begin{itemize}
\tightlist
\item
  Notice that though our null hypothesis was \(H_0: µ ≤ 0\), we calculated the test-statistic at µ=0
\item
  This is because we know that rejection region under smaller values of µ below zero will be shifted to the left compared to \(Q_{0.95} = 1.65\)
\item
  Therefore, if our test statistic results in rejecting µ=0, it will certainly result in rejecting values of µ less than 0
\end{itemize}

\hypertarget{what-is-statistical-significance}{%
\subsection{What is statistical significance?}\label{what-is-statistical-significance}}

\begin{itemize}
\tightlist
\item
  As mentioned earlier, the rejection region is selected so that it is unlikely under the null hypothesis
\item
  Therefore, when the test-statistic falls in the rejection region, we say it is statistically significant
\item
  Traditionally, this region is selected to have 5\% probability under the null hypothesis. However, 5\% is arbitrary
\item
  Note that in setting up the test statistic, only the null hypothesis came into play. The alternative hypothesis did not matter
\end{itemize}

\hypertarget{what-is-a-p-value}{%
\subsection{What is a p-value?}\label{what-is-a-p-value}}

\begin{itemize}
\tightlist
\item
  The p-value is defined as the probability of being more extreme than the test statistic under the null hypothesis\\
  = \textbf{P(Test statistic is more extreme than its observed value \textbar{} H0)}
\item
  In our example, involving a \textbf{one-sided} test\\
  \(p-value = P(T_{234} > 12.49 |H_0) = 1-pt(12.49,234) = 0\)
\item
  Clearly, when the test statistic is statistically significant, the p-value is less than 5\% or more generally it is less than the Type I error
\item
  This explains why the p-value is often compared to 5\% to determine statistical significance
\end{itemize}

\hypertarget{p-value-illustrated}{%
\subsection{p-value illustrated}\label{p-value-illustrated}}

\includegraphics[width=0.5\linewidth]{./4_26}

\begin{itemize}
\tightlist
\item
  Notice the difference between the p-value and the rejection region for a \textbf{two-sided} test

  \begin{itemize}
  \tightlist
  \item
    The red lines mark off the rejection region of α=0.05 at ±1.97
  \item
    The blue line is a hypothetical observed t-statistic=2.3
  \item
    The shaded area marks off the p-value
  \item
    The green line is at -2.3, was not observed. Yet, we use the area beyond it to obtain a two-sided p-value
  \end{itemize}
\end{itemize}

\includegraphics[width=0.5\linewidth]{./4_27}

\begin{itemize}
\tightlist
\item
  This figure illustrates the p-value for a one-sided test with \(H_A: µ > µ_0\)

  \begin{itemize}
  \tightlist
  \item
    Once again, the red line marks off the rejection region of α=0.05 at 1.65
  \item
    The blue line is the observed t-statistic=2.3 in this illustration
  \item
    The shaded area marks off the p-value. Note that the p-value is half that of the one-sided test by definition
  \end{itemize}
\end{itemize}

\includegraphics[width=0.5\linewidth]{./4_28}

\begin{itemize}
\tightlist
\item
  Finally, this figure illustrates the p-value for a one-sided test with \(H_A: µ < µ_0\)

  \begin{itemize}
  \tightlist
  \item
    This time, the red line marks off the rejection region of α=0.05 at -1.65
  \item
    The blue line is the observed t-statistic=-2.3 in this illustration
  \item
    The shaded area marks off the p-value
  \end{itemize}
\end{itemize}

\hypertarget{type-i-and-type-ii-errors-1}{%
\subsection{Type I and Type II errors}\label{type-i-and-type-ii-errors-1}}

\includegraphics[width=0.5\linewidth]{./4_29}

\begin{itemize}
\tightlist
\item
  Recall

  \begin{itemize}
  \tightlist
  \item
    Type I error is the probability of rejecting the null hypothesis when it is true
  \item
    Type II error is the probability of not rejecting the null hypothesis when the alternative is true
  \end{itemize}
\end{itemize}

\hypertarget{t-test}{%
\subsection{t-test}\label{t-test}}

\includegraphics[width=0.5\linewidth]{./4_30}

\begin{itemize}
\tightlist
\item
  The t.test function in R tells us that that we can reject the null hypothesis of no difference in the mean log10 interleukin levels in the two groups at the Type I error level of 0.05
\end{itemize}

\hypertarget{a-bit-of-history-pearson-vs.-fisher}{%
\subsection{A bit of history: Pearson vs.~Fisher}\label{a-bit-of-history-pearson-vs.-fisher}}

\begin{itemize}
\tightlist
\item
  The hypothesis test and p-value were proposed by Karl Pearson and Ronald Fisher, respectively, who were contemporaries who strongly disagreed with each other
\item
  It is ironic that today we use these two techniques together!
\item
  As we will discuss in greater detail in later lectures, there has been a backlash against both these approaches and a move towards usage of confidence intervals or Bayesian methods
\end{itemize}

\hypertarget{inference-for-comparing-two-means}{%
\subsection{Inference for comparing two means}\label{inference-for-comparing-two-means}}

\begin{itemize}
\tightlist
\item
  The hypothesis test for comparing two means resembles the structure of the hypothesis test for a single mean

  \begin{itemize}
  \tightlist
  \item
    it can be two-sided or one-sided
  \item
    the form of the test-statistics depends on the study design and assumptions, e.g.

    \begin{itemize}
    \tightlist
    \item
      Whether the study design involves paired or unpaired means
    \item
      Assuming the variance in the two groups is equal or not
    \item
      Assuming the variance is known or not
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{stroke-study-question-2-two-sided-alternative-hypothesis}{%
\subsection{Stroke study: Question 2, two-sided alternative hypothesis}\label{stroke-study-question-2-two-sided-alternative-hypothesis}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The true mean change in the STREAM score between 3-days and 3 months post stroke **is the same for men and women** & \$H\_A\$: The true mean change in the STREAM score between 3-days and 3 months post stroke **is not the same for men and women**\\
\hline
\end{tabular}

\hypertarget{stroke-study-question-2-one-sided-hypothesis-i}{%
\subsection{Stroke study: Question 2, one-sided hypothesis I}\label{stroke-study-question-2-one-sided-hypothesis-i}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The true mean change in the STREAM score between 3-days and 3 months post stroke **is at most as great in men as in women** & \$H\_A\$: The **true mean change** in the STREAM score between 3-days and 3 months post stroke **is greater in men than in women**\\
\hline
\end{tabular}

\hypertarget{stroke-study-question-2-one-sided-hypothesis-ii}{%
\subsection{Stroke study: Question 2, one-sided hypothesis II}\label{stroke-study-question-2-one-sided-hypothesis-ii}}

\begin{tabular}{l|l}
\hline
Null Hypothesis & Alternative Hypothesis\\
\hline
\$H\_0\$: The true mean change in the STREAM score between 3-days and 3 months post stroke **is at least as great in men as in women** & \$H\_A\$: The **true mean change** in the STREAM score between 3-days and 3 months post stroke **is lesser in men than in women**\\
\hline
\end{tabular}

\hypertarget{example-one-sided-or-two-sided-test}{%
\subsection{Example: One-sided or two-sided test?}\label{example-one-sided-or-two-sided-test}}

\begin{itemize}
\tightlist
\item
  We return to the second research question based on the stroke dataset. It is of interest to compare the change in mobility (from baseline to 3 months) between men and women
\item
  One way to do this is to carry out a hypothesis test.
\item
  We will begin with a two-sided hypothesis test:
\end{itemize}

\(H_0: \mu_1=\mu_2\space\space\space\space\space vs. \space\space\space\space\space H_A:\mu_1\neq\mu_2\)

where \(µ_1\) is the true mean change in mobility in men and \(µ_2\) is the true mean change in women

\hypertarget{difference-in-change-in-mobility-between-men-and-women}{%
\subsection{Difference in change in mobility between men and women}\label{difference-in-change-in-mobility-between-men-and-women}}

\begin{tabular}{l|l|l}
\hline
  & Change in Men & Change in Women\\
\hline
Number of cases & 144 & 91\\
\hline
Mean & \$\textbackslash{}bar y\_1 = 17.09\$ & \$\textbackslash{}bar y\_2 = 12.86\$\\
\hline
Standard deviation & \$s\_1 = 19.25\$ & \$s\_2 = 18.31\$\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Recall that we had assumed that the variance is the same in both groups being compared and calculate a pooled variance that averages across both groups of \(s_{diff}=2.53\)
\end{itemize}

\hypertarget{comparing-change-in-mobility-between-men-and-women}{%
\subsection{Comparing change in mobility between men and women}\label{comparing-change-in-mobility-between-men-and-women}}

\begin{itemize}
\tightlist
\item
  The t-statistic is given by
\end{itemize}

\[\frac{\bar Y_1-\bar Y_2-(\mu_1-\mu_2)}{s_{diff}}=\frac{\bar Y_1-\bar Y_2}{s_{diff}}=\frac{17.09-12.86}{2.53}=1.67\]

\begin{itemize}
\tightlist
\item
  Since we are working under the assumption that the variances are equal, the t-distribution used to define the rejection region has degrees of freedom n1+n2-2 (as we saw previously when defining a confidence interval for comparing two means)
\item
  If we use a Type I error value of α=0.05, we \textbf{would} reject the null hypothesis if it lies below -1.96 or above 1.96
\item
  In our case, the t-statistic falls within this \textbf{region} so we say ``we do not have enough evidence to reject the null hypothesis''
\item
  The p-value is 0.09, which exceeds 0.05
\item
  The p-value can be calculated as follows in R
\end{itemize}

\begin{quote}
2*(1-pt((17.09-12.86)/2.53,233))\\
{[}1{]} 0.09587913
\end{quote}

\hypertarget{what-if-our-alternative-hypothesis-was-one-sided-instead}{%
\subsection{What if our alternative hypothesis was one-sided instead?}\label{what-if-our-alternative-hypothesis-was-one-sided-instead}}

\begin{itemize}
\tightlist
\item
  It is to be expected that men may experience a greater improvement in mobility than women. Therefore, we can restate our hypothesis test as:
\end{itemize}

\(H_0: µ_1 ≤ µ_2 \space\space\space\space\space vs \space\space\space\space\space H_A: µ_1 > µ_2\)

where µ1 is the true mean change in men and µ2 is the true mean change in women

\begin{itemize}
\tightlist
\item
  As in the case of hypothesis testing for a single mean, the test statistic remains the same
\item
  However the rejection region is one-sided. Using the quantiles of the t-distribution with n1+n2-2=233 degrees of freedom, we can determine that the rejection region includes the region above \(Q_{0.95} = 1.65\). Therefore, our test statistic of 1.67 lies in the rejection region
\item
  In comparison with this rejection region, we would conclude that we have enough evidence to reject the null hypothesis that the mean change in mobility in men is less than or equal to that of women
\end{itemize}

\hypertarget{what-if-our-alternative-hypothesis-was-one-sided-in-the-other-direction}{%
\subsection{What if our alternative hypothesis was one-sided in the other direction?}\label{what-if-our-alternative-hypothesis-was-one-sided-in-the-other-direction}}

\begin{itemize}
\tightlist
\item
  Only for the purpose of illustrating how the rejection region is defined, let us restate our hypothesis test as:
\end{itemize}

\(H_0: µ_1 \geq µ_2 \space\space\space\space\space vs \space\space\space\space\space H_A: µ_1 < µ_2\)

where µ1 is the true mean change in men and µ2 is the true mean change in women

\begin{itemize}
\tightlist
\item
  Once again, the test statistic remains the same
\item
  However the rejection region is now the region below \(Q_{0.95} = -1.65\). Therefore, our test statistic of 1.67 does not lie in the rejection region
\item
  This would lead us to conclude we do not have enough evidence to reject the null hypothesis that the mean change in mobility men is less than or equal to that in women
\end{itemize}

\hypertarget{what-if-our-null-hypothesis-was-one-sided-instead}{%
\subsection{What if our null hypothesis was one-sided instead?}\label{what-if-our-null-hypothesis-was-one-sided-instead}}

\begin{itemize}
\tightlist
\item
  The p-value for this situation is\\
  P(Test statistic \textgreater{} 1.67\textbar{} H0)
\item
  In R this can be calculated as (1-pt((17.09-12.86)/2.53,233)) = 0.04793956,
  which falls below the Type I error level of α=0.05
\end{itemize}

\hypertarget{why-did-our-conclusion-change-when-we-moved-from-a-two-sided-to-a-one-sided-hypothesis}{%
\subsection{Why did our conclusion change when we moved from a two-sided to a one-sided hypothesis?}\label{why-did-our-conclusion-change-when-we-moved-from-a-two-sided-to-a-one-sided-hypothesis}}

\begin{itemize}
\tightlist
\item
  The two-sided test is a more stringent test, which makes it more difficult to reject the null hypothesis
\item
  Under a two-sided alternative we have to consider the probability of being more extreme than the observed value on both sides of the null
\item
  This would be relevant only if we thought that it were possible that the difference \(\bar Y_1-\bar Y_2\) could be either positive or negative
\item
  If we have reason to believe that men are unlikely to have worse mobility than women, the one-sided test would make more sense in the context of our example
\end{itemize}

\hypertarget{hypothesis-testing-vs.-confidence-interval-estimation}{%
\section{Hypothesis testing vs.~confidence interval estimation}\label{hypothesis-testing-vs.-confidence-interval-estimation}}

\hypertarget{what-is-statistics-1}{%
\subsection{What is statistics?}\label{what-is-statistics-1}}

\emph{Statistics is a collection of procedures and principles for gathering data and analyzing information in order to help people make decisions when faced with uncertainty}\\
\href{https://www.amazon.ca/Statistical-Methods-Internet-Companion-Statistics/dp/0495122505}{Utts \& Heckard} in `Statistical Ideas \& Methods'

\hypertarget{quantifying-uncertainty-vs.-decision-making}{%
\subsection{Quantifying uncertainty vs.~decision making}\label{quantifying-uncertainty-vs.-decision-making}}

\begin{itemize}
\tightlist
\item
  The hypothesis testing framework is designed to support decision making, e.g.~

  \begin{itemize}
  \tightlist
  \item
    Whether to take an umbrella to work
  \item
    Whether the observed association between a predictor and an outcome is real
  \end{itemize}
\item
  Confidence interval estimation, on the other hand, conveys the uncertainty in our knowledge about a statistic, e.g.

  \begin{itemize}
  \tightlist
  \item
    There is a 60\%-80\% chance it will rain today
  \item
    The difference in survival associated with treatment A vs.~treatment B is 60\%-80\%
  \end{itemize}
\end{itemize}

\hypertarget{interpreting-confidence-intervals-vs.-hypothesis-tests}{%
\subsection{\texorpdfstring{Interpreting Confidence Intervals vs.~Hypothesis Tests\(^*\)}{Interpreting Confidence Intervals vs.~Hypothesis Tests\^{}*}}\label{interpreting-confidence-intervals-vs.-hypothesis-tests}}

\begin{itemize}
\tightlist
\item
  Suppose that you have just calculated a confidence interval for a certain parameter. There are five possible conclusions that can be drawn, depending on where the upper and lower confidence interval limits fall in relation to the upper and lower limits of the region of clinical equivalence.
\item
  The region of clinical equivalence, sometimes called the region of indifference, is the region inside of which both treatments would be considered to be the same for all practical purposes.
\end{itemize}

\(^*\)From Lawrence Joseph's notes

\hypertarget{interpreting-confidence-intervals-5-possible-conclusions}{%
\subsubsection{Interpreting confidence intervals: 5 possible conclusions}\label{interpreting-confidence-intervals-5-possible-conclusions}}

\includegraphics[width=0.5\linewidth]{./4_48}

\hypertarget{notes-on-significance-tests}{%
\subsection{\texorpdfstring{Notes on significance tests\(^*\)}{Notes on significance tests\^{}*}}\label{notes-on-significance-tests}}

\begin{itemize}
\tightlist
\item
  We saw that there are two ways of reporting the results of a hypothesis test -- either we can report \textbf{the decision} (reject vs.~not reject which is the same thing as significant vs.~not significant) or \textbf{the p-value}
\item
  Reporting the p-value is more informative than merely reporting whether a test was ``significant'' or ``not significant''.
\item
  The level of significance, \(\alpha\), is often set to 0.05, but it should be chosen according to the problem. There is nothing magical about \(\alpha\) = 0.05. There is no practical difference if p = 0.049 or p = 0.051.
\item
  Even a very small p-value does not guarantee \(H_0\) is false. Repeating the study is usually necessary for further proof, or to vary the conditions or population.
\item
  Statistical significance (small p-value) is not the same as practical significance.
\item
  The p-value is not everything. Must also examine your data carefully, data cleaning for outliers, etc. Remember -- all tests carry assumptions that can be thrown off by outliers.
\item
  Reporting a confidence interval for an effect is more informative than reporting a p-value.
\item
  P-values are often misinterpreted. A p-value is not the probability of the null hypothesis.
\item
  It is also not the probability that a result occurred by chance. . . .
\item
  The p-value only tells you something about the probability of seeing your results given a particular hypothesis---it cannot tell you the probability that the results are true or whether they're due to random chance.
\end{itemize}

\(^*\)From Lawrence Joseph's notes

\hypertarget{sample-size-calculations-for-studies-of-one-or-two-means}{%
\section{Sample size calculations for studies of one or two means}\label{sample-size-calculations-for-studies-of-one-or-two-means}}

\hypertarget{sample-size-for-hypothesis-tests}{%
\subsection{Sample size for hypothesis tests}\label{sample-size-for-hypothesis-tests}}

\begin{itemize}
\tightlist
\item
  This approach is relevant when we want to test a certain hypothesis
\item
  For example, we might want to test

  \begin{itemize}
  \item
    \(H_0\): mean change in stroke mobility ≤ 10 points vs.\\
  \item
    \(H_a\): mean change in stroke mobility \textgreater{} 10 points
  \item
    \(H_0\): mean serum cholesterol ≤ 200 vs.
  \item
    \(H_a\): mean serum cholesterol \textgreater{} 200
  \end{itemize}
\end{itemize}

\hypertarget{example}{%
\subsection{Example}\label{example}}

\begin{itemize}
\tightlist
\item
  In the United States, appropriate levels of serum cholesterol in adults have been defined by the National Heart, Lung, and Blood Institute as follows:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Good:} 200 mg/dL or lower
  \item
    \textbf{Borderline:} 200 to 239 mg/dL
  \item
    \textbf{High:} 240 mg/dL or higher
  \end{itemize}
\item
  Let's say the researcher in our earlier example posed the question differently.
\item
  He or she wants to test the hypothesis that the mean cholesterol level in the population has fallen to 195 mg/dL such that it is now within the ``Good'' range
\end{itemize}

\[H_0: µ ≤ 195\space\space vs.\space H_a: µ > 195\]

\begin{itemize}
\tightlist
\item
  How large a sample size is required to test this hypothesis such that

  \begin{itemize}
  \tightlist
  \item
    Type I error (α)= \(P(Rejecting\space H_0 | H_0\space is\space true)\) = 1\%, and
  \item
    Type II error (β)= \(P(Not\space rejecting\space H_0 | H_A\space is\space true)\) = 5\%
  \end{itemize}
\item
  The researcher wishes to design the study such that the test is sufficiently sensitive to detect difference of 6 mg/dL or more (i.e.~when µ=201 or more)
\end{itemize}

\hypertarget{we-are-interested-in-detecting-a-shift-in-the-mean-of-the-distribution}{%
\subsection{We are interested in detecting a shift in the mean of the distribution}\label{we-are-interested-in-detecting-a-shift-in-the-mean-of-the-distribution}}

\includegraphics[width=0.5\linewidth]{./4_55}

\hypertarget{sample-size-required-for-a-hypothesis-test-of-a-single-mean}{%
\subsection{Sample size required for a hypothesis test of a single mean}\label{sample-size-required-for-a-hypothesis-test-of-a-single-mean}}

\begin{itemize}
\tightlist
\item
  Again, we rely on the quantiles of the normal distribution rather than the t-distribution
\item
  The required sample size for a two-sided test is given by this expression:
\end{itemize}

\[n = \frac{s^2(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(\mu_0-\mu_A)^2}\]

\begin{itemize}
\tightlist
\item
  The required sample size for a one-sided test is given by this expression:
\end{itemize}

\[n = \frac{s^2(Z_{1-\alpha}+Z_{1-\beta})^2}{(\mu_0-\mu_A)^2}\]

\begin{itemize}
\tightlist
\item
  From the expressions on the previous slide we can see that n increases as:

  \begin{itemize}
  \tightlist
  \item
    s increases
  \item
    α decreases or β decreases
  \item
    \(\mu_0-\mu_A\) decreases
  \end{itemize}
\item
  Once again, you may wish to calculate sample size under several different scenarios
\end{itemize}

\hypertarget{sample-size-required-under-different-scenarios}{%
\subsection{Sample size required under different scenarios}\label{sample-size-required-under-different-scenarios}}

\includegraphics[width=1\linewidth]{./4_58}

\hypertarget{example-serum-cholesterol}{%
\subsection{Example: Serum cholesterol}\label{example-serum-cholesterol}}

\begin{itemize}
\tightlist
\item
  The sample size required for a one-sided test is
\end{itemize}

\[n = \frac{s^2(Z_{1-0.01}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(2.33+1.65)^2}{(195-201)^2}=704\]

\begin{itemize}
\tightlist
\item
  \textbf{Impact of increasing α to 0.05:}

  \begin{itemize}
  \tightlist
  \item
    If the type I error was increased to 0.05, we would replace \(Z_{1-0.01} = 2.33\) by \(Z_{1-0.05} = 1.65\).
  \item
    \(n = \frac{s^2(Z_{1-0.05}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+1.65)^2}{(195-201)^2}=484\)
  \end{itemize}
\item
  \textbf{Impact of increasing β:}

  \begin{itemize}
  \tightlist
  \item
    If in addition to the above change, the type II error was increased to 0.2, as is commonly done in practice. Then, \(Z_{1-0.2} = 1.65\) in the expression above would be replace by \(Z_{1-0.2} = 0.84\)
  \item
    \(n = \frac{s^2(Z_{1-0.01}+Z_{1-0.2})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+0.84)^2}{(195-201)^2}=276\)
  \end{itemize}
\item
  \textbf{Impact of increasing \(\mu_0-\mu_A\)}

  \begin{itemize}
  \tightlist
  \item
    If \(µ_0\) were set to 190, then the difference between the two groups increases to 11
  \item
    \(n = \frac{s^2(Z_{1-0.01}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+0.84)^2}{(190-201)^2}=82\)
  \end{itemize}
\end{itemize}

\hypertarget{summary-what-do-you-need-to-calculate-the-sample-size-required}{%
\subsection{Summary: What do you need to calculate the sample size required?}\label{summary-what-do-you-need-to-calculate-the-sample-size-required}}

\begin{tabular}{l|l}
\hline
Confidence interval & Hypothesis test\\
\hline
Confidence level 1-α & Type I error α\\
\hline
 & Type II error β\\
\hline
Guess value for standard deviation (s) & Guess value for standard deviation (s)\\
\hline
Desired precision (or half-width of interval) (δ) & The minimum important difference to detect \$(\textbackslash{}mu\_0-\textbackslash{}mu\_A)\$\\
\hline
\end{tabular}

\hypertarget{sample-size-calculation-comparing-two-means}{%
\subsection{Sample size calculation: Comparing two means}\label{sample-size-calculation-comparing-two-means}}

\begin{itemize}
\tightlist
\item
  Once again, we can define different methods depending on whether we plan to report confidence intervals or hypothesis tests
\end{itemize}

\hypertarget{example-2}{%
\subsection{Example}\label{example-2}}

\begin{itemize}
\item
  Consider the study on in-vivo efficacy of the single domain antibody P1.40 in Tg+ mice
\item
  Lets say we wish to repeat the earlier randomized controlled trial.
\item
  The authors reported that the mean change in cholesterol at 4 days after the intervention was 20 mg/dL and I guessed that the \textbf{pooled} standard deviation of the difference was \(s_p\)=9 mg/dL
\item
  We desire to ensure that the observed mean change lies within δ = ±5 mg/dL of the true mean change with 95\% confidence.
\item
  What is the sample size required in each arm of the RCT (assuming the sample size is equal in both arms)?
\item
  Alternatively, we may wish to carry out a one-sided hypothesis test of the difference between the two groups
\end{itemize}

\[H_0: µ_{P1.40} - µ_{PBS} ≤ 0\space vs. H_a:  µ_{P1.40} - µ_{PBS} > 0\]

\begin{itemize}
\tightlist
\item
  Recall, that the previous study reported that the mean change in cholesterol at 4 days after the intervention was 20 mg/dL and that the standard deviation was assumed to be \(s_p\)=9 mg/dL
\item
  We desire to ensure that the test is sensitive enough to detect a difference greater than \(µ_1 - µ_2 =15\) mg/dL with Type II error = 20\%. The Type I error is fixed at the traditional value of 5\%.
\item
  What is the sample size required in each arm of the RCT (assuming the sample size is equal in both arms)?
\end{itemize}

\hypertarget{sample-size-required-to-test-h_0mu_1mu_2}{%
\subsection{\texorpdfstring{Sample size required to test \(H_0:\mu_1=\mu_2\)}{Sample size required to test H\_0:\textbackslash mu\_1=\textbackslash mu\_2}}\label{sample-size-required-to-test-h_0mu_1mu_2}}

\begin{itemize}
\tightlist
\item
  In the expressions below n = total sample size. If the sample size is the same in both groups, it is n/2 in each group
\item
  The required sample size for a two-sided test is given by this expression:
\end{itemize}

\[n = \frac{4s_p^2(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(\mu_1-\mu_2)^2}\]

\begin{itemize}
\tightlist
\item
  The required sample size for a one-sided test is given by this expression:
\end{itemize}

\[n = \frac{4s_p^2(Z_{1-\alpha}+Z_{1-\beta})^2}{(\mu_1-\mu_2)^2}\]

\hypertarget{example-in-vivo-efficacy-of-p1.40}{%
\subsection{Example: In-vivo efficacy of P1.40}\label{example-in-vivo-efficacy-of-p1.40}}

\begin{itemize}
\tightlist
\item
  The sample size required for a one-sided test is
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 9^2(1.65+0.84)^2}{(15)^2}\]]
\textbf{80 mice (or 40 mice in each group)}
\end{description}

\begin{itemize}
\tightlist
\item
  If the standard deviation was 4 mg /dL instead, then
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 4^2(1.65+0.84)^2}{(15)^2}\]]
\textbf{16 (or 8 mice in each group)}
\end{description}

\begin{itemize}
\tightlist
\item
  If we used a calculation for a two-sided test instead (with the standard deviation of 9), then
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 9^2(1.96+0.84)^2}{(15)^2}\]]
\textbf{102 (or 51 mice per group)}
\end{description}

\hypertarget{power}{%
\subsection{Power}\label{power}}

\begin{itemize}
\tightlist
\item
  The expressions for sample size can be rearranged to calculate the power for a given sample size
\item
  For a single mean
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_0-\mu_A|-s\times z_{1-\alpha/2}}{s}\]

\begin{itemize}
\tightlist
\item
  For comparing two means
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_1-\mu_2|-2s_pz_{1-\alpha/2}}{2s_p}\]

\hypertarget{calculating-the-power-in-r}{%
\subsection{Calculating the power in R}\label{calculating-the-power-in-r}}

\begin{verbatim}
power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
             power = NULL,
             type = c("two.sample", "one.sample", "paired"),
             alternative = c("two.sided", "one.sided"),
             strict = FALSE)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The power.t.test function can be used to either take the sample size in each group (n) as an input and return the power, or vice-versa
\item
  Whereas the expressions we studied so far were based on normal quantiles, this R function uses the t-distribution quantiles and should therefore provide a more precise answer
\end{itemize}

\hypertarget{example-power-function}{%
\subsection{Example: Power function}\label{example-power-function}}

\begin{itemize}
\item
  A proposed study wishes to investigate the effects of a new hypertensive drug (experimental group) compared to a conventional treatment (control group).
\item
  The outcome of interest is the difference in the mean blood pressure in each group. Previous studies show that the pooled standard deviation (SD) across the two groups is 20mmHg
\item
  Assuming that the desired Type I error is 5\% and that the feasible sample size is 25 in each group, and that the minimum clinically important difference is 15mmHg.
\item
  What is the power of a two-sided test to detect the minimum important difference? Plot the function relating the difference between the two groups to the power of the test
\item
  First, we will calculate the power using the expression in your notes
\item
  Then we will use the power.t.test function to plot the power function
\item
  Using the expression for calculating the normal distribution quantile corresponding to the power we have
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_1-\mu_2|-2s_pz_{1-\alpha/2}}{2s_p}=\frac{\sqrt{50}|15|-2\times 20\times 1.96}{2\times 20}=0.6916504\]

\begin{itemize}
\tightlist
\item
  The power of the test is given by\\
  \(P(Z\leq Z_{1-\beta})\) = pnorm(0.6916504) = 0.754216
\item
  Using the function in R instead we would obtain the following result
\end{itemize}

\begin{verbatim}
power.t.test(n = 25, delta = 15, sd = 20, sig.level = 0.05)
= 0.7383646
\end{verbatim}

which is slightly lower than the result based on the approximation using the normal quantile
* To find the power for a series of different values for the minimum difference we can use the following R code ({in red})

\(\#\) a vector of possible values for the difference\\
{min.diff = c(5, 10, 15, 20, 25)}

\(\#\) create an object for the result of the power function\\
{result = power.t.test(n = 25, delta = min.diff, sd = 20, sig.level = 0.05)}

{names(result)} \# to examine the contents of the object\\
{[}1{]} ``n'' ``delta'' ``sd'' ``sig.level'' ``power''\\
{[}6{]} ``alternative'' ``note'' ``method''

{output=result\$power} \# create another object to extract the power

\(\#\) scatter and line plot\\
{plot(min.diff,output,type=``b'',xlab=``Difference between group means'',ylab=``Power'')}

\(\#\) red reference line at Power=80\%\\
{abline(h=0.8,col=2)}

Notice that with 25 patients in each group, we would have 80\% or higher power to detect differences greater than about 17mmHg

\includegraphics[width=0.7\linewidth]{./4_73}

\hypertarget{what-is-the-impact-of-equal-or-unequal-group-sizes-on-the-precision}{%
\subsection{What is the impact of equal or unequal group sizes on the precision?}\label{what-is-the-impact-of-equal-or-unequal-group-sizes-on-the-precision}}

\begin{itemize}
\tightlist
\item
  If we write the standard error of an estimated difference in mean responses as \(\sigma\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\) where \(\sigma\) is the standard deviation and n1 and n2 are the sample size in each group, then we can establish the following principles (which would apply to both means and proportions:

  \begin{itemize}
  \tightlist
  \item
    \textbf{If costs and other factors (including unit variability) are equal, and if both types of units are equally scarce or equally plentiful}, then for a given total sample size of \(n = n_1 + n_2\) , an equal division of n i.e.~\(n_1 = n_2\) is preferable since it yields a smaller standard error than any non-symmetric division.
  \item
    \textbf{If one type of unit is much scarcer, and thus the limiting factor}, then it makes sense to choose all (say \(n_1\)) of the available scarcer units, and some \(n_2 \geq n_1\) of the other type. The greater is \(n_2\), the smaller the standard error of the estimated difference.
  \end{itemize}
\end{itemize}

\hypertarget{effect-of-different-numbers-in-each-sample-on-precision-when-both-groups-are-equally-difficult-to-sample-from}{%
\subsubsection{Effect of different numbers in each sample on precision, when both groups are equally difficult to sample from}\label{effect-of-different-numbers-in-each-sample-on-precision-when-both-groups-are-equally-difficult-to-sample-from}}

The following table gives the value of the standard error (SE) for various combinations of \(n_1\) and \(n_2\) adding to 100 and assuming \(\sigma = 1\) (the values of \(n_1 + n_2 =100\) and \(\sigma = 1\) also arbitrary). Notice that, the standard error is relatively unaffected until the ratio exceeds 70:30.

\includegraphics[width=0.5\linewidth]{./4_75}

\(^*\)if sample sizes are \(\pi:(1-\pi)\), the \% increase is \(50/\sqrt{\pi(1-\pi)}\)

\hypertarget{effect-of-different-numbers-in-each-sample-on-precision-when-group-1-is-more-scarce-than-group-2}{%
\subsubsection{Effect of different numbers in each sample on precision, when group 1 is more scarce than group 2}\label{effect-of-different-numbers-in-each-sample-on-precision-when-group-1-is-more-scarce-than-group-2}}

There is a `law of diminishing returns' once \(n_2\) is more than a few multiples of \(n_1\) as seen in the following table where \(n_1\) is fixed (arbitrarily) at 100 and \(n_2\) ranges from \(kn=1 \times n_1\) to \(kn=100 \times n_1\); again, we assume \(\sigma=1\).

\includegraphics[width=1\linewidth]{./4_76}

\hypertarget{an-r-package-for-sample-size-and-power-calculations}{%
\subsection{An R package for sample size and power calculations}\label{an-r-package-for-sample-size-and-power-calculations}}

\begin{itemize}
\tightlist
\item
  There are a number of user-contributed packages that can be added to R
\item
  Once such package is the pwr package that includes functions for calculating the sample size required for a hypothesis test when the two groups being compared have unequal sample size
\end{itemize}

\hypertarget{installing-a-package-in-r}{%
\subsection{Installing a package in R}\label{installing-a-package-in-r}}

User-contributed packages are not part of the base R installation. They need to be installed with the install.packages() function and then read into R with the library() function as below:

\begin{quote}
install.packages(``pwr'')
library(pwr)
help(package=``pwr'')
\end{quote}

\hypertarget{sample-size-for-comparing-two-means}{%
\subsection{Sample size for comparing two means}\label{sample-size-for-comparing-two-means}}

The function pwr.t2n.test() can be used

\begin{verbatim}
pwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL,  
 alternative = c("two.sided", "less","greater"))
\end{verbatim}

Notice that it takes the effect size \(d=\frac{|\mu_1-\mu_2|}{s_p}\) as an argument, whereas the power.t.test() function we saw earlier takes arguments delta and sd instead

This function can be used to calculate either the power or the sample size

\hypertarget{example-1-2}{%
\subsubsection{Example 1:}\label{example-1-2}}

Calculate the sample size in group 2 when:\\
effect size=15/20=0.75, Type I error=0.05, Power=0.8, and feasible sample size in group 1 is 25

\begin{quote}
pwr.t2n.test(n1=25,d=0.75,power=0.8)
\end{quote}

\begin{verbatim}
 t test power calculation 

         n1 = 25
         n2 = 34.17153
          d = 0.75
  sig.level = 0.05
      power = 0.8
alternative = two.sided
\end{verbatim}

\hypertarget{example-2-1}{%
\subsubsection{Example 2:}\label{example-2-1}}

Calculate the power available when:\\
effect size=15/20=0.75, Type I error=0.05, Sample size = 25 in both groups

Notice we now have the same result we obtained previously with power.t.test

\begin{quote}
pwr.t2n.test(n1=25,n2=25,d=0.75)
\end{quote}

\begin{verbatim}
 t test power calculation 

        n1 = 25
        n2 = 25
        d = 0.75
        sig.level = 0.05
        power = 0.7383671
        alternative = two.sided
\end{verbatim}

\hypertarget{extra-problems}{%
\section{Extra Problems}\label{extra-problems}}

\hypertarget{section}{%
\subsection{1.}\label{section}}

During a weight loss study, each of nine subjects was given (1) the active drug m-chlorophenylpiperazine (mCPP) for 2 weeks and then a placebo for another 2 weeks, or (2) the placebo for the first 2 weeks and then mCPP for the second 2 weeks. The following table shows the amount of weight loss (kg) for the nine subjects when taking the drug mCPP and when taking placebo (Note that if a subject gained weight, then the recorded weight loss is negative, as is the case for the subject 2, who gained 0.3 kg when on the placebo.) Use a t-test to investigate the claim that mCPP affects weight loss. Let \(H_A\) be non-directional, and let α=0.01.

\begin{table}
\centering
\begin{tabular}{l|r|r|r}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{2}{c|}{Weight Change (kg)} & \multicolumn{1}{c}{ } \\
\cline{2-3}
Subject & mcPP & Placebo & Difference in kg\\
\hline
1 & 1.10 & 0.00 & 1.10\\
\hline
2 & 1.30 & -0.30 & 1.60\\
\hline
3 & 1.00 & 0.60 & 0.40\\
\hline
4 & 1.70 & 0.30 & 1.40\\
\hline
5 & 1.40 & -0.70 & 2.10\\
\hline
6 & 0.10 & -0.20 & 0.30\\
\hline
7 & 0.50 & 0.60 & -0.10\\
\hline
8 & 1.60 & 0.90 & 0.70\\
\hline
9 & -0.50 & -2.00 & 1.50\\
\hline
Mean & 0.91 & -0.09 & 1.00\\
\hline
SD & 0.74 & 0.88 & 0.72\\
\hline
\end{tabular}
\end{table}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  What is the value of the t-test statistic for assessing whether weight change when taking mCPP is different from a placebo?\\
\item
  In the context of this study, state the null and alternative hypotheses.\\
\item
  The p-value for the t-test is 0.003. If α=0.10, what is your conclusion regarding the hypothesis in (b)?\\
\item
  Construct a 99\% confidence interval for the mean difference.\\
\item
  Assume that a 1kg difference in weight loss between the two regimens is considered important. Interpret the confidence interval in the context of this assumption.
\end{enumerate}

\hypertarget{section-1}{%
\subsection{2.}\label{section-1}}

A study was undertaken to compare the respiratory responses of hypnotized and non-hypnotized subjects to certain instructions. The 16 male volunteers were allocated at random to an experimental group to be hypnotized or to a control group. Baseline measurements were taken at the start of the experiment. In analyzing the data, the baseline breathing patterns of the two groups were different; this was surprising, since all the subjects had been treated the same up to that time. One explanation proposed for this unexpected difference was that the experimental group were more excited in anticipation of the experience of being hypnotized. The accompanying table presents a summary of the baseline measurements of total ventilation (liters of air per minute per square meter of body area).

\begin{tabular}{l|r|r}
\hline
  & Experimental & Control\\
\hline
1 & 5.320 & 4.500\\
\hline
2 & 5.600 & 4.780\\
\hline
3 & 5.740 & 4.790\\
\hline
4 & 6.060 & 4.860\\
\hline
5 & 6.320 & 5.410\\
\hline
6 & 6.340 & 5.700\\
\hline
7 & 6.790 & 6.080\\
\hline
8 & 7.180 & 6.210\\
\hline
N & 8.000 & 8.000\\
\hline
Mean & 6.169 & 5.291\\
\hline
SD & 0.621 & 0.652\\
\hline
\end{tabular}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Create a dotplot to illustrate the distribution of observations in the two groups.\\
\item
  Use a t-test to test the hypothesis of no difference against a non-directional alternative. Let α=0.05\\
\item
  Use a t-test to test the hypothesis of no difference against the alternative that the experimental conditions produce a larger mean than the control conditions. Let α=0.05.\\
\item
  Which of the two tests ((b) or (c)) is more appropriate? Explain
\end{enumerate}

\hypertarget{lecture-5-sample-size-calculations-to-plan-for-hypothesis-tests-of-means}{%
\chapter{Lecture 5: Sample size calculations to plan for hypothesis tests of means}\label{lecture-5-sample-size-calculations-to-plan-for-hypothesis-tests-of-means}}

\hypertarget{sample-size-calculation-1}{%
\section{Sample size calculation}\label{sample-size-calculation-1}}

\begin{itemize}
\tightlist
\item
  The method one uses for the sample size calculation depends on the plan for the statistical inference
\item
  Accordingly, depending on whether you intend to report a hypothesis test, or a confidence interval or a Bayesian analysis, your method for sample size calculation may change
\end{itemize}

\hypertarget{type-i-and-type-ii-errors-2}{%
\subsection{Type I and Type II errors}\label{type-i-and-type-ii-errors-2}}

\includegraphics[width=0.5\linewidth]{./5_3}

\begin{itemize}
\tightlist
\item
  When carrying out a hypothesis test we can in fact make two types of errors

  \begin{itemize}
  \tightlist
  \item
    Type I error (α): We reject \(H_0\) when it is true
  \item
    Type II error (β): We fail to reject \(H_0\) when it is not true (i.e.~when \(H_A\) is true)
  \end{itemize}
\end{itemize}

\hypertarget{similarity-between-diagnostic-testing-and-hypothesis-testing-1}{%
\subsection{Similarity between diagnostic testing and hypothesis testing}\label{similarity-between-diagnostic-testing-and-hypothesis-testing-1}}

\includegraphics[width=0.5\linewidth]{./5_4a}

\begin{itemize}
\tightlist
\item
  Sensitivity = A / (A+C)
\item
  Specificity = D / (B+D)
\item
  A, B, C and D are numbers of individuals in the study
\end{itemize}

\includegraphics[width=0.5\linewidth]{./5_4b}

\begin{itemize}
\tightlist
\item
  1-Type II error (Power) = A / (A+C)
\item
  1-Type I error = D / (B+D)
\item
  A, B, C and D are number of repetitions of the study
\end{itemize}

\hypertarget{example-3}{%
\subsection{Example}\label{example-3}}

\begin{itemize}
\tightlist
\item
  In the United States, appropriate levels of serum cholesterol in adults have been defined by the National Heart, Lung, and Blood Institute as follows:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Good:} 200 mg/dL or lower
  \item
    \textbf{Borderline:} 200 to 239 mg/dL
  \item
    \textbf{High:} 240 mg/dL or higher
  \end{itemize}
\item
  Let's say the researcher in our earlier example posed the question differently.
\item
  He or she wants to test the hypothesis that the mean cholesterol level in the population has fallen to 195 mg/dL such that it is now within the ``Good'' range
\end{itemize}

\[H_0: µ ≤ 195\space\space vs.\space H_a: µ > 195\]

\begin{itemize}
\tightlist
\item
  How large a sample size is required to test this hypothesis such that

  \begin{itemize}
  \tightlist
  \item
    Type I error (α)= \(P(Rejecting\space H_0 | H_0\space is\space true)\) = 1\%, and
  \item
    Type II error (β)= \(P(Not\space rejecting\space H_0 | H_A\space is\space true)\) = 5\%
  \end{itemize}
\item
  The researcher wishes to design the study such that the test is sufficiently sensitive to detect difference of 6 mg/dL or more (i.e.~when µ=201 or more)
\end{itemize}

\hypertarget{we-are-interested-in-detecting-a-shift-in-the-mean-of-the-distribution-1}{%
\subsection{We are interested in detecting a shift in the mean of the distribution}\label{we-are-interested-in-detecting-a-shift-in-the-mean-of-the-distribution-1}}

\includegraphics[width=0.5\linewidth]{./4_55}

\hypertarget{sample-size-required-for-a-hypothesis-test-of-a-single-mean-1}{%
\subsection{Sample size required for a hypothesis test of a single mean}\label{sample-size-required-for-a-hypothesis-test-of-a-single-mean-1}}

\begin{itemize}
\tightlist
\item
  Again, we rely on the quantiles of the normal distribution rather than the t-distribution
\item
  The required sample size for a two-sided test is given by this expression:
\end{itemize}

\[n = \frac{s^2(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(\mu_0-\mu_A)^2}\]

\begin{itemize}
\tightlist
\item
  The required sample size for a one-sided test is given by this expression:
\end{itemize}

\[n = \frac{s^2(Z_{1-\alpha}+Z_{1-\beta})^2}{(\mu_0-\mu_A)^2}\]

\begin{itemize}
\tightlist
\item
  From the expressions on the previous slide we can see that n increases as:

  \begin{itemize}
  \tightlist
  \item
    s increases
  \item
    α decreases or β decreases
  \item
    \(\mu_0-\mu_A\) decreases
  \end{itemize}
\item
  Once again, you may wish to calculate sample size under several different scenarios
\end{itemize}

\hypertarget{sample-size-required-under-different-scenarios-for-a-one-sided-test}{%
\subsection{Sample size required under different scenarios for a one-sided test}\label{sample-size-required-under-different-scenarios-for-a-one-sided-test}}

\includegraphics[width=1\linewidth]{./4_58}

\hypertarget{example-serum-cholesterol-1}{%
\subsection{Example: Serum cholesterol}\label{example-serum-cholesterol-1}}

\begin{itemize}
\tightlist
\item
  The sample size required for a one-sided test is
\end{itemize}

\[n = \frac{s^2(Z_{1-0.01}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(2.33+1.65)^2}{(195-201)^2}=704\]

\begin{itemize}
\tightlist
\item
  \textbf{Impact of increasing α to 0.05:}

  \begin{itemize}
  \tightlist
  \item
    If the type I error was increased to 0.05, we would replace \(Z_{1-0.01} = 2.33\) by \(Z_{1-0.05} = 1.65\).
  \item
    \(n = \frac{s^2(Z_{1-0.05}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+1.65)^2}{(195-201)^2}=484\)
  \end{itemize}
\item
  \textbf{Impact of increasing β:}

  \begin{itemize}
  \tightlist
  \item
    If in addition to the above change, the type II error was increased to 0.2, as is commonly done in practice. Then, \(Z_{1-0.2} = 1.65\) in the expression above would be replace by \(Z_{1-0.2} = 0.84\)
  \item
    \(n = \frac{s^2(Z_{1-0.01}+Z_{1-0.2})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+0.84)^2}{(195-201)^2}=276\)
  \end{itemize}
\item
  \textbf{Impact of increasing \(\mu_0-\mu_A\)}

  \begin{itemize}
  \tightlist
  \item
    If \(µ_0\) were set to 190, then the difference between the two groups increases to 11
  \item
    \(n = \frac{s^2(Z_{1-0.01}+Z_{1-0.05})^2}{(\mu_0-\mu_A)^2}=\frac{40^2(1.65+0.84)^2}{(190-201)^2}=82\)
  \end{itemize}
\end{itemize}

\hypertarget{summary-input-needed-to-calculate-sample-size-for-single-mean}{%
\subsection{Summary: Input needed to calculate sample size for single mean}\label{summary-input-needed-to-calculate-sample-size-for-single-mean}}

\begin{tabular}{l|l}
\hline
Confidence interval & Hypothesis test\\
\hline
Confidence level 1-α & Type I error α\\
\hline
 & Type II error β\\
\hline
Guess value for standard deviation (s) & Guess value for standard deviation (s)\\
\hline
Desired precision (or half-width of interval) (δ) & The minimum important difference to detect \$(\textbackslash{}mu\_0-\textbackslash{}mu\_A)\$\\
\hline
\end{tabular}

\hypertarget{example-planning-a-study-to-compare-means-using-a-hypothesis-test}{%
\subsection{Example: Planning a study to compare means using a hypothesis test}\label{example-planning-a-study-to-compare-means-using-a-hypothesis-test}}

\begin{itemize}
\tightlist
\item
  Lets say we wish to repeat the earlier study on the association of adapter protein Nck1 with body weight and that we wish to carry out a one-sided hypothesis test of the difference between the two groups stated as follows:
\end{itemize}

\[H_0:\mu_{WT}-\mu_{KO}\geq 5\space vs. \space H_a:\mu_{WT}-\mu_{KO}<5\]

\begin{itemize}
\tightlist
\item
  The authors reported that the mean difference in body weight at 16 weeks between the two groups was 2.5g and we estimated that the pooled standard deviation of this difference was \(s_p=5.5g\)
\item
  We desire to ensure that the test is sensitive enough to detect a difference greater than \(µ_1 - µ_2 = 6g\) with Type II error = 20\%. The Type I error is fixed at the traditional value of 5\%.
\item
  What is the sample size required in each arm of the RCT (assuming the sample size is equal in both arms)?
\end{itemize}

\hypertarget{sample-size-required-to-compare-means}{%
\subsection{Sample size required to compare means}\label{sample-size-required-to-compare-means}}

\begin{itemize}
\tightlist
\item
  In the expressions below n = total sample size. If the sample size is the same in both groups, it is n/2 in each group
\item
  The required sample size for a two-sided test is given by this expression:
\end{itemize}

\[n = \frac{4s_p^2(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(\mu_1-\mu_2)^2}\]

\begin{itemize}
\tightlist
\item
  The required sample size for a one-sided test is given by this expression:
\end{itemize}

\[n = \frac{4s_p^2(Z_{1-\alpha}+Z_{1-\beta})^2}{(\mu_1-\mu_2)^2}\]

\hypertarget{example-sample-size-required-to-compare-nck1-wt-and-ko-mice}{%
\subsection{Example: Sample size required to compare Nck1 WT and KO mice}\label{example-sample-size-required-to-compare-nck1-wt-and-ko-mice}}

\begin{itemize}
\tightlist
\item
  The sample size required for a one-sided test is
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 5.5^2(1.65+0.84)^2}{(6)^2}\]]
\textbf{21 mice (or 11 mice in each group)}
\end{description}

\begin{itemize}
\tightlist
\item
  If the standard deviation was 10 mg/dL instead, then
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 10^2(1.65+0.84)^2}{(6)^2}\]]
\textbf{69 (or 35 mice in each group)}
\end{description}

\begin{itemize}
\tightlist
\item
  If we used a calculation for a two-sided test instead (with the standard deviation of 5.5), then
\end{itemize}

\begin{description}
\item[\[n = \frac{4s_p^2(Z_{1-0.05/2}+Z_{1-0.2})^2}{(\mu_1-\mu_2)^2}=\frac{4\times 5.5^2(1.96+0.84)^2}{(6)^2}\]]
\textbf{26 (or 13 mice per group)}
\end{description}

\hypertarget{power-1}{%
\subsection{Power}\label{power-1}}

\begin{itemize}
\tightlist
\item
  The expressions for sample size can be rearranged to calculate the power for a given sample size
\item
  For a single mean
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_0-\mu_A|-s\times z_{1-\alpha/2}}{s}\]

\begin{itemize}
\tightlist
\item
  For comparing two means
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_1-\mu_2|-2s_pz_{1-\alpha/2}}{2s_p}\]

\hypertarget{calculating-the-power-in-r-1}{%
\subsection{Calculating the power in R}\label{calculating-the-power-in-r-1}}

\begin{verbatim}
power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
             power = NULL,
             type = c("two.sample", "one.sample", "paired"),
             alternative = c("two.sided", "one.sided"),
             strict = FALSE)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The power.t.test function can be used to either take the sample size in each group (n) as an input and return the power, or vice-versa
\item
  Whereas the expressions we studied so far were based on normal quantiles, this R function uses the t-distribution quantiles and should therefore provide a more precise answer
\end{itemize}

\hypertarget{example-power-function-1}{%
\subsection{Example: Power function}\label{example-power-function-1}}

\begin{itemize}
\item
  A proposed study wishes to investigate the effects of a new hypertensive drug (experimental group) compared to a conventional treatment (control group).
\item
  The outcome of interest is the difference in the mean blood pressure in each group. Previous studies show that the pooled standard deviation (SD) across the two groups is 20mmHg
\item
  Assuming that the desired Type I error is 5\% and that the feasible sample size is 25 in each group, and that the minimum clinically important difference is 15mmHg.
\item
  What is the power of a two-sided test to detect the minimum important difference? Plot the function relating the difference between the two groups to the power of the test
\item
  First, we will calculate the power using the expression in your notes
\item
  Then we will use the power.t.test function to plot the power function
\item
  Using the expression for calculating the normal distribution quantile corresponding to the power we have
\end{itemize}

\[Z_{1-\beta}=\frac{\sqrt N|\mu_1-\mu_2|-2s_pz_{1-\alpha/2}}{2s_p}=\frac{\sqrt{50}|15|-2\times 20\times 1.96}{2\times 20}=0.6916504\]

\begin{itemize}
\tightlist
\item
  The power of the test is given by\\
  \(P(Z\leq Z_{1-\beta})\) = pnorm(0.6916504) = 0.754216
\item
  Using the function in R instead we would obtain the following result
\end{itemize}

power.t.test(n = 25, delta = 15, sd = 20, sig.level = 0.05)
= 0.7383646

which is slightly lower than the result based on the approximation using the normal quantile
* To find the power for a series of different values for the minimum difference we can use the following R code ({in red})

\(\#\) a vector of possible values for the difference\\
{min.diff = c(5, 10, 15, 20, 25)}

\(\#\) create an object for the result of the power function\\
{result = power.t.test(n = 25, delta = min.diff, sd = 20, sig.level = 0.05)}

{names(result)} \# to examine the contents of the object\\
{[}1{]} ``n'' ``delta'' ``sd'' ``sig.level'' ``power''\\
{[}6{]} ``alternative'' ``note'' ``method''

{output=result\$power} \# create another object to extract the power

\(\#\) scatter and line plot\\
{plot(min.diff,output,type=``b'',xlab=``Difference between group means'',ylab=``Power'')}

\(\#\) red reference line at Power=80\%\\
{abline(h=0.8,col=2)}

Notice that with 25 patients in each group, we would have 80\% or higher power to detect differences greater than about 17mmHg

\includegraphics[width=0.7\linewidth]{./4_73}

\hypertarget{what-is-the-impact-of-equal-or-unequal-group-sizes-on-the-precision-1}{%
\subsection{\texorpdfstring{What is the impact of equal or unequal group sizes on the precision?\(^*\)}{What is the impact of equal or unequal group sizes on the precision?\^{}*}}\label{what-is-the-impact-of-equal-or-unequal-group-sizes-on-the-precision-1}}

\begin{itemize}
\tightlist
\item
  \textbf{If costs and other factors (including unit variability) are equal, and if both types of units are equally scarce or equally plentiful}, then for a given total sample size of \(n = n_1 + n_2\) , an equal division of n i.e.~\(n_1 = n_2\) is preferable since it yields a smaller standard error than any non-symmetric division.
\item
  \textbf{If one type of unit is much scarcer, and thus the limiting factor}, then it makes sense to choose all (say \(n_1\)) of the available scarcer units, and some \(n_2 \geq n_1\) of the other type. The greater is \(n_2\), the smaller the standard error of the estimated difference.
\item
  These principles apply to both means and proportions
\end{itemize}

\(^*\)\href{http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/2\%20MeansQuantitativeVariable.pdf}{From James Hanley's notes}

\hypertarget{effect-of-different-numbers-in-each-sample-on-precision-when-both-groups-are-equally-difficult-to-sample-from-1}{%
\subsubsection{Effect of different numbers in each sample on precision, when both groups are equally difficult to sample from}\label{effect-of-different-numbers-in-each-sample-on-precision-when-both-groups-are-equally-difficult-to-sample-from-1}}

\begin{itemize}
\tightlist
\item
  Let the standard error of an estimated difference in mean responses as \(SE\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\) where SE is the standard deviation and n1 and n2 are the sample size in each group.
\item
  The following table gives the value of the standard error (SE) for various combinations of \(n_1\) and \(n_2\) adding to 100 and assuming SE=1
\item
  Notice that, the standard error is relatively unaffected until the ratio exceeds 70:30
\end{itemize}

\includegraphics[width=0.5\linewidth]{./4_75}

\(^*\)if sample sizes are \(\pi:(1-\pi)\), the \% increase is \(50/\sqrt{\pi(1-\pi)}\)

\hypertarget{effect-of-different-numbers-in-each-sample-on-precision-when-group-1-is-more-scarce-than-group-2-1}{%
\subsubsection{Effect of different numbers in each sample on precision, when group 1 is more scarce than group 2}\label{effect-of-different-numbers-in-each-sample-on-precision-when-group-1-is-more-scarce-than-group-2-1}}

There is a `law of diminishing returns' once \(n_2\) is more than a few multiples of \(n_1\) as seen in the following table where \(n_1\) is fixed (arbitrarily) at 100 and \(n_2\) ranges from \(kn=1 \times n_1\) to \(kn=100 \times n_1\); again, we assume SE=1

\includegraphics[width=1\linewidth]{./4_76}

\hypertarget{an-r-package-for-sample-size-and-power-calculations-1}{%
\subsection{An R package for sample size and power calculations}\label{an-r-package-for-sample-size-and-power-calculations-1}}

\begin{itemize}
\tightlist
\item
  There are a number of user-contributed packages that can be added to R
\item
  Once such package is the pwr package for sample size calculations for hypothesis testing
\item
  It includes functions for calculating the sample size required for a hypothesis test when the two groups being compared have unequal sample size
\end{itemize}

\hypertarget{installing-a-package-in-r-1}{%
\subsection{Installing a package in R}\label{installing-a-package-in-r-1}}

\begin{itemize}
\tightlist
\item
  User-contributed packages are not part of the base R installation.
\item
  They can be installed by clicking on the Install tab in R Studio. Once installed, they must be read into R using the library() function\\
  \textgreater{} library(pwr)
\item
  Click on the name of the library in the list of libraries within R Studio to access the help file
\end{itemize}

\hypertarget{sample-size-for-comparing-two-means-1}{%
\subsection{Sample size for comparing two means}\label{sample-size-for-comparing-two-means-1}}

The function pwr.t2n.test() can be used

pwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL,\\
alternative = c(``two.sided'', ``less'',``greater''))

Notice that it takes the effect size \(d=\frac{|\mu_1-\mu_2|}{s_p}\) as an argument, whereas the power.t.test() function we saw earlier takes arguments delta and sd instead

This function can be used to calculate either the power or the sample size

\hypertarget{example-1-3}{%
\subsubsection{Example 1:}\label{example-1-3}}

Calculate the sample size in group 2 when:\\
effect size=15/20=0.75, Type I error=0.05, Power=0.8, and feasible sample size in group 1 is 25

\begin{quote}
pwr.t2n.test(n1=25,d=0.75,power=0.8)
\end{quote}

\begin{verbatim}
 t test power calculation 

         n1 = 25
         n2 = 34.17153
          d = 0.75
  sig.level = 0.05
      power = 0.8
alternative = two.sided
\end{verbatim}

\hypertarget{example-2-2}{%
\subsubsection{Example 2:}\label{example-2-2}}

Calculate the power available when:\\
effect size=15/20=0.75, Type I error=0.05, Sample size = 25 in both groups

Notice we now have the same result we obtained previously with power.t.test

\begin{quote}
pwr.t2n.test(n1=25,n2=25,d=0.75)
\end{quote}

\begin{verbatim}
 t test power calculation 

        n1 = 25
        n2 = 25
        d = 0.75
        sig.level = 0.05
        power = 0.7383671
        alternative = two.sided
\end{verbatim}

\hypertarget{lecture-6-bayesian-inference-for-means}{%
\chapter{Lecture 6: Bayesian inference for means}\label{lecture-6-bayesian-inference-for-means}}

\hypertarget{example-beach-water-quality}{%
\section{Example: Beach Water Quality}\label{example-beach-water-quality}}

\includegraphics[width=0.5\linewidth]{./6_3}

Image from: \url{http://www.ottawapublichealth.ca/en/public-health-services/beach-water-quality-results.aspx}

\begin{itemize}
\tightlist
\item
  The city health inspector wishes to determine the mean number of E. coli colonies (counts) per 100mL of water at a popular city beach.
\item
  Assume the number of bacteria per liter of water follows a normal distribution with mean µ and standard deviation is known to be σ = 15.
\item
  She collects 10 water samples and finds the bacteria counts to be: 185 200 225 208 194 217 220 203 206 190.
\item
  If the true mean count exceeds 200, the beach is considered unsafe for swimming
\end{itemize}

\hypertarget{descriptive-statistics}{%
\subsection{Descriptive Statistics}\label{descriptive-statistics}}

\includegraphics[width=0.5\linewidth]{./6_4}

\begin{itemize}
\tightlist
\item
  Sample mean = 204.8 counts/100mL
\item
  Sample standard deviation = 13.14 counts/100mL
\end{itemize}

\hypertarget{frequentist-inference}{%
\subsection{Frequentist inference}\label{frequentist-inference}}

\begin{itemize}
\tightlist
\item
  To make inferences about the true population mean \emph{E. coli} count, we could rely on a t-test of the following null hypothesis
\end{itemize}

\[H_0: µ ≤ 200\space  vs. H_A: µ > 200\]

\begin{itemize}
\tightlist
\item
  Or, we could calculate a confidence interval
\end{itemize}

\hypertarget{p-value}{%
\subsection{p-value}\label{p-value}}

\begin{quote}
x=c(185,200,225,208,194,217,220,203,206,190)
\end{quote}

\begin{quote}
t.test(x,mu=200,alternative=``greater'')
\end{quote}

\begin{verbatim}
One Sample t-test
\end{verbatim}

data: x\\
t = 1.1553, df = 9, p-value = {0.1389}\\
alternative hypothesis: true mean is greater than 200\\
95 percent confidence interval:\\
{197.1838} {Inf}\\
sample estimates:\\
mean of x\\
204.8

\includegraphics[width=0.5\linewidth]{./6_6}

\begin{itemize}
\tightlist
\item
  The p-value of the one-sided t-test is 0.1389
\item
  This is typically interpreted as insufficient evidence to reject the null hypothesis \(H_0: µ ≤ 200\)
\item
  The p-value is the probability of observing results more extreme than the data under the null hypothesis
\end{itemize}

\hypertarget{confidence-interval}{%
\subsection{95\% confidence interval}\label{confidence-interval}}

\begin{quote}
x=c(185,200,225,208,194,217,220,203,206,190)
\end{quote}

\begin{quote}
t.test(x,mu=200)
\end{quote}

\begin{verbatim}
One Sample t-test  
\end{verbatim}

data: x\\
t = 1.1553, df = 9, p-value = 0.2777\\
alternative hypothesis: true mean is not equal to 200\\
95 percent confidence interval:\\
{195.4012 214.1988}\\
sample estimates:\\
mean of x\\
204.8

\begin{itemize}
\tightlist
\item
  A one-sided 95\% confidence interval has a lower limit of 197.18 counts/100mL
\item
  A two-sided 95\% confidence interval would be between 195.40 to 214.20 counts/100mL
\item
  In both cases, the value 200 counts /100mL falls within the confidence interval
\end{itemize}

\hypertarget{bayesian-analysis}{%
\subsection{Bayesian analysis}\label{bayesian-analysis}}

\begin{itemize}
\tightlist
\item
  Bayesian statistical inference is based on Bayes Theorem of conditional probability

  \begin{itemize}
  \tightlist
  \item
    It is an alternative to the predominant `frequentist' (or classical) approach to statistical inference
  \end{itemize}
\item
  It can be used for any statistical problem

  \begin{itemize}
  \tightlist
  \item
    Simple problems, e.g.~comparing two means or logistic regression
  \item
    Complex problems, e.g.~network meta-analysis, cost-effectiveness analysis
  \item
    Study design, e.g.~design of adaptive randomized controlled trials
  \end{itemize}
\item
  Bayesian methods involve specific computational skills
\item
  In this class we will make simplifying assumptions to limit ourselves to problems where the calculations can be done easily. None the less, we will be able to see the advantages of the Bayesian approach when it comes to interpreting results
\end{itemize}

\hypertarget{what-is-the-true-mean-e.-coli-count-uxb5}{%
\subsection{What is the true mean E. coli count (µ)?}\label{what-is-the-true-mean-e.-coli-count-uxb5}}

\begin{itemize}
\tightlist
\item
  \textbf{Frequentist approach:}

  \begin{itemize}
  \tightlist
  \item
    Treat the unknown parameter µ as a \textbf{fixed} quantity
  \item
    Assume that the data at hand is one of several datasets that could be observed
  \item
    Make inferential statements of the form P(data \textbar{} Hypothesis about µ), e.g.~P(Observed mean \textgreater{} 204.8 \textbar{} H0: µ ≤ 200)
  \end{itemize}
\item
  Bayesian approach:

  \begin{itemize}
  \tightlist
  \item
    Treat the unknown parameter µ is a \textbf{random} variable
  \item
    Update the subjective prior information about the parameter using the observed data via Bayes Theorem
  \item
    Inferential statements of the form P(Hypothesis about µ \textbar{} data),
    e.g.~P(µ \textgreater{} 200 \textbar{} Observed mean = 204.8 )
  \end{itemize}
\end{itemize}

\hypertarget{bayesian-inductive-vs.-frequentist-deductive-thinking}{%
\subsection{Bayesian (inductive) vs.~Frequentist (deductive) thinking}\label{bayesian-inductive-vs.-frequentist-deductive-thinking}}

\includegraphics[width=0.5\linewidth]{./6_10}

From Goodman S, Ann Intern Med. 1999;130:995-1004

\hypertarget{principal-elements-of-a-bayesian-analysis}{%
\subsection{Principal elements of a Bayesian analysis}\label{principal-elements-of-a-bayesian-analysis}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the \textbf{likelihood function}, i.e.~the probability distribution of the observed data (x) in terms of the unknown parameter (µ).

  \begin{itemize}
  \tightlist
  \item
    This step remains the same whether one uses a Bayesian or a frequentist approach
  \end{itemize}
\item
  Specify a \textbf{prior distribution} f(µ) over the unknown parameter(s)
\item
  Use Bayes Theorem to update the prior distribution with the data and obtain the \textbf{posterior distribution} of f(µ\textbar x)
\end{enumerate}

\hypertarget{the-prior-distribution}{%
\subsection{The prior distribution}\label{the-prior-distribution}}

\begin{itemize}
\tightlist
\item
  A distinguishing feature of a Bayesian analysis is the use of a prior distribution
\item
  A prior probability distribution on the unknown parameters, is a summary of information about them from sources external to (but not necessarily collected before) the observed experiment
\item
  It could be based on:

  \begin{itemize}
  \tightlist
  \item
    a subjective prior belief: e.g.~``it is highly unlikely that the true mean is greater than 400 counts/100mL''
  \item
    a subjective prior based on previously observed data: e.g.~the 95\% confidence interval of the mean count from earlier pilot study
  \item
    It may be `objective' or `non-informative', e.g.~assuming a uniform prior distribution over the possible range (0, 1000) of the number of counts
  \end{itemize}
\end{itemize}

\hypertarget{beach-water-quality-bayesian-analysis}{%
\subsection{Beach Water Quality: Bayesian analysis}\label{beach-water-quality-bayesian-analysis}}

Let X = Number E. coli counts in a sample

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Likelihood function}: X \textasciitilde{} N(µ, σ=15)
\item
  \textbf{Prior distribution} summarizes information we have on the mean number of counts

  \begin{itemize}
  \tightlist
  \item
    e.g.~µ \textasciitilde{} N(mean=θ=200, standard deviation=τ=100)\\
    Applying Bayes Theorem we obtain the\\
  \end{itemize}
\item
  \textbf{Posterior distribution}\\
  e.g.~µ \textbar{} X \textasciitilde{} N(mean=204.79, standard deviation=4.74)
\end{enumerate}

\hypertarget{two-possible-prior-distributions}{%
\subsection{Two possible prior distributions}\label{two-possible-prior-distributions}}

\includegraphics[width=1\linewidth]{./6_14}

\begin{itemize}
\tightlist
\item
  Both priors were selected to be Normal distributions because

  \begin{itemize}
  \tightlist
  \item
    The mean count is continuous and the standard deviation of the normal prior distribution can be adjusted so that it is concentrated over plausible values
  \item
    There is also a mathematical advantage. The normal distribution is a \textbf{conjugate} prior distribution. This means the form of both prior and posterior are the same. Therefore, by choosing a normal prior distribution for a normal likelihood, we ensure that the posterior distribution is also normal\\
  \end{itemize}
\item
  Notice that the non-informative prior has a very high standard deviation. This was chosen so that our normal prior distribution translates into an approximately uniform distribution over the range from 150 to 250
\item
  On the other hand, the informative prior distribution has a much smaller standard deviation, comparable to the standard deviation of the observed data

  \begin{itemize}
  \tightlist
  \item
    Note however that the standard error of the data is smaller still at
    15/sqrt(10)
  \end{itemize}
\end{itemize}

\hypertarget{illustration-of-the-two-prior-distributions}{%
\subsection{Illustration of the two prior distributions}\label{illustration-of-the-two-prior-distributions}}

\includegraphics[width=0.5\linewidth]{./6_16}

\hypertarget{applying-bayes-theorem}{%
\subsection{Applying Bayes Theorem}\label{applying-bayes-theorem}}

\begin{itemize}
\tightlist
\item
  To update the information in the prior with the data we use Bayes Theorem.
\item
  It is based on the same principle that we saw before, but modified by the fact that the distribution is continuous. If you compare with the expressions we had previously, you will notice that the only difference is that the summation is replaced by an integral.
\item
  We have
\end{itemize}

\[f(\mu|X)=\frac{f(X|\mu)f(\mu)}{\int f(X|\mu)f(\mu)dx}\]

\begin{itemize}
\tightlist
\item
  The integral in the denominator can be very challenging to solve. To get around this computationally intensive methods have been developed, e.g.~Monte Carlo Markov Chain (MCMC) methods. These methods are more easily accessible today because of the rapid evolution of computers in the last two decades
\item
  Before the personal computer era, one way to simplify the integral was to use a conjugate prior
\item
  In our example, we have a normal likelihood and a conjugate normal prior distribution with mean θ and standard deviation τ
\item
  This results in the following normal posterior distribution
\end{itemize}

\(\mu\)\textbar X \textasciitilde{} \(N(mean=A\times\theta+B\times\bar x,standard\space deviation=\sqrt{\frac{1}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}}\)

where A and B are constants defined on the next slide

\hypertarget{the-normal-posterior-distribution}{%
\subsection{The normal posterior distribution}\label{the-normal-posterior-distribution}}

\begin{itemize}
\tightlist
\item
  A is the weight given to the prior mean, and B is the weight given to the posterior mean
\end{itemize}

\(A=\frac{\sigma^2/n}{\tau^2+\sigma^2/n}\) and \(B = \frac{\tau^2}{\tau^2+\sigma^2/n}\)

\begin{itemize}
\tightlist
\item
  Notice that A and B add up to 1
\item
  Also notice

  \begin{itemize}
  \tightlist
  \item
    A will be higher than B if the standard deviation of the sample mean is higher than the prior standard deviation, and vice versa
  \item
    If A is higher than B, the posterior mean is weighted more towards the prior mean θ.\\
  \end{itemize}
\item
  Thus the posterior distribution is a compromise between the prior distribution and the observed data, taking into account their relative `informativeness' as defined by their variance
\item
  The posterior distribution's standard deviation is given by
\end{itemize}

\[\sqrt{\frac{1}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}}\]

\begin{itemize}
\tightlist
\item
  If the prior standard deviation is very high, e.g.~τ=100, then the posterior standard deviation is well approximated by the sample standard deviation
\item
  Thus once again we see that the standard deviation of the posterior is a compromise between that of the prior and the likelihood (or data)
\end{itemize}

\hypertarget{beach-water-quality-example-non-informative-prior}{%
\subsection{Beach Water Quality Example: Non-informative prior}\label{beach-water-quality-example-non-informative-prior}}

\begin{itemize}
\tightlist
\item
  Our non-informative N(θ=200, τ=100) prior implies that prior to observing the data, we would say that the true mean (µ) is equally likely to be below or above 200 counts/mL
\item
  In our example, when using the non-informative prior distribution
\end{itemize}

\[A=\frac{\sigma^2/n}{\tau^2+\sigma^2/n}=\frac{15^2/10}{100^2+15^2/10}\approx 0\]
\[B = \frac{\tau^2}{\tau^2+\sigma^2/n}=\frac{100^2}{100^2+15^2/10}\approx 1\]
\[Posterior\space mean=A\times\theta+B\times\bar x\approx0\times 200+1\times204.8=204.79\]

the observed data completely dominated the posterior, and the prior had no influence. This illustrates why non-informative priors are widely used if want to ensure there is no influence of `subjective' opinion

\begin{itemize}
\tightlist
\item
  Posterior standard deviation
\end{itemize}

\[\sqrt{\frac{1}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}}=\sqrt{\frac{1}{\frac{10}{15^2}+\frac{1}{100^2}}}=4.74,\]

which shows that the posterior standard deviation also was essentially the same as that based on the data alone

\hypertarget{bayesian-analysis-using-r}{%
\subsection{Bayesian analysis using R}\label{bayesian-analysis-using-r}}

\begin{verbatim}
> x=c(185,200,225,208,194,217,220,203,206,190)

> mean(x)
[1] 204.8

> normnp(x,m.x=200,s.x=100,sigma.x=15)
Known standard deviation :15
Posterior mean           : 204.7892242
Posterior std. deviation : 4.7380891

Prob.   Quantile 
------  ----------
0.005   192.5847154
0.010   193.7667807
0.025   195.5027402
0.050   196.9957611
0.500   204.7892242
0.950   212.5826873
0.975   214.0757083
0.990   215.8116678
0.995   216.9937331
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Download the R package \href{https://cran.r-project.org/web/packages/Bolstad/index.html}{Bolstad}
\item
  It was developed based on material from an \href{https://mcgill.worldcat.org/title/introduction-to-bayesian-statistics/oclc/957525550\&referer=brief_results}{introductory Bayesian textbook}
\item
  The function that is relevant for our problem is normnp()
\end{itemize}

\hypertarget{posterior-distribution-compared-to-likelihood-and-non-informative-prior}{%
\subsection{Posterior distribution compared to likelihood and non-informative prior}\label{posterior-distribution-compared-to-likelihood-and-non-informative-prior}}

\includegraphics[width=0.5\linewidth]{./6_24}

\hypertarget{prior-and-posterior-distribution-plot-from-r}{%
\subsection{Prior and posterior distribution plot from R}\label{prior-and-posterior-distribution-plot-from-r}}

\includegraphics[width=0.5\linewidth]{./6_25}

\hypertarget{statistics-typically-reported-in-a-bayesian-analysis}{%
\subsection{Statistics typically reported in a Bayesian analysis}\label{statistics-typically-reported-in-a-bayesian-analysis}}

\includegraphics[width=1\linewidth]{./6_26}

\(^*\)The credible interval is the Bayesian equivalent of the frequentist confidence interval

\hypertarget{interpretation-of-the-bayesian-credible-interval-cri}{%
\subsection{Interpretation of the Bayesian credible interval (CrI)}\label{interpretation-of-the-bayesian-credible-interval-cri}}

\includegraphics[width=0.5\linewidth]{./6_27}

\begin{itemize}
\tightlist
\item
  A 95\% credible interval is an interval that has a 0.95 probability of including the true parameter (µ)

  \begin{itemize}
  \tightlist
  \item
    There are several such credible intervals, e.g.

    \begin{itemize}
    \tightlist
    \item
      \((Q_{0.025}, Q_{0.975})\) is called the equal-tailed 95\% CrI and is most commonly reported
    \item
      \((Q_{0.03}, Q_{0.98})\) or \((Q_{0.01}, Q_{0.96})\) are also 95\% credible intervals
    \end{itemize}
  \end{itemize}
\item
  Notice the 95\% refers to the interval we have observed and not to a procedure or to repeated experiments
\end{itemize}

\hypertarget{mean-e.-coli-counts-hypothesis-testing-vs.-confidence-interval-vs.-bayesian-inference}{%
\subsection{Mean E. coli counts: Hypothesis testing vs.~confidence interval vs.~Bayesian inference}\label{mean-e.-coli-counts-hypothesis-testing-vs.-confidence-interval-vs.-bayesian-inference}}

\includegraphics[width=1\linewidth]{./6_28}

\begin{itemize}
\tightlist
\item
  The hypothesis test would lead us to conclude that we do not have enough evidence to reject the null hypothesis. But the Bayesian analysis allows to calculate that the probability of the alternative is in fact highly likely given the observed data
\item
  Notice that numerically the Bayesian interval resulting from a non-informative prior and a frequentist 95\% CI are very similar. However, their interpretation is different
\end{itemize}

\hypertarget{beach-water-quality-informative-prior-distribution}{%
\subsection{Beach Water Quality: Informative prior distribution}\label{beach-water-quality-informative-prior-distribution}}

\includegraphics[width=1\linewidth]{./6_28}

\hypertarget{expressing-the-95-ci-from-earlier-data-as-a-normal-distribution}{%
\subsection{Expressing the 95\% CI from earlier data as a normal distribution}\label{expressing-the-95-ci-from-earlier-data-as-a-normal-distribution}}

\begin{itemize}
\tightlist
\item
  To obtain the parameters of the informative prior distribution we matched the lower and upper limit of the 95\% CI to the 2.5\% and 97.5\% quantiles of the prior distribution
\item
  Since we are assuming the that prior distribution is normal we know that

  \begin{itemize}
  \tightlist
  \item
    The 97.5\% quantile = θ + 2τ = 230
  \item
    The 2.5\% quantile = θ - 2τ = 160
  \end{itemize}
\item
  Solving these two equations give us θ=195 and τ=17.5
\end{itemize}

\hypertarget{beach-water-quality-example-informative-prior}{%
\subsection{Beach Water Quality Example: Informative prior}\label{beach-water-quality-example-informative-prior}}

\begin{itemize}
\tightlist
\item
  In our example, for the informative prior
\end{itemize}

\[A=\frac{\sigma^2/n}{\tau^2+\sigma^2/n}=\frac{15^2/10}{17.5^2+15^2/10}\approx 0.07\]
\[B = \frac{\tau^2}{\tau^2+\sigma^2/n}=\frac{17.5^2}{17.5^2+15^2/10}\approx 0.93\]
\[Posterior\space mean=A\times\theta+B\times\bar x\approx0.07\times 195+0.93\times204.8=204.13\]

Which shows the observed data were more dominant than the prior, though the prior did have some influence

\hypertarget{non-informative-prior}{%
\subsection{Non-informative prior}\label{non-informative-prior}}

\begin{itemize}
\tightlist
\item
  Posterior standard deviation
\end{itemize}

\[\sqrt{\frac{1}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}}=\sqrt{\frac{1}{\frac{10}{15^2}+\frac{1}{17.5^2}}}=4.58,\]

which shows that the posterior standard deviation deviation was lower than the standard deviation based on the observed data alone (4.74).

\begin{itemize}
\tightlist
\item
  So there was some gain in informativeness by using the prior information
\end{itemize}

\hypertarget{results-with-non-informative-vs.-informative-prior}{%
\subsection{Results with non-informative vs.~informative prior}\label{results-with-non-informative-vs.-informative-prior}}

\includegraphics[width=1\linewidth]{./6_33}

\hypertarget{non-informative-prior-1}{%
\subsubsection{Non-informative prior}\label{non-informative-prior-1}}

\includegraphics[width=0.5\linewidth]{./6_34a}

\hypertarget{informative-prior}{%
\subsubsection{Informative prior}\label{informative-prior}}

\includegraphics[width=0.5\linewidth]{./6_34b}

\hypertarget{r-code-for-bayesian-analysis}{%
\subsection{R code for Bayesian analysis}\label{r-code-for-bayesian-analysis}}

\begin{verbatim}
library(Bolstad)

x=c(185,200,225,208,194,217,220,203,206,190)

result=normnp(x,m.x=200,s.x=100,sigma.x=15)
plot(result,xlim=c(0,400))

result=normnp(x,m.x=195,s.x=17.5,sigma.x=15)
plot(result,xlim=c(0,400))
\end{verbatim}

\hypertarget{beach-water-quality-example}{%
\subsection{Beach Water Quality Example}\label{beach-water-quality-example}}

\begin{itemize}
\tightlist
\item
  What is the impact of using other prior distributions?

  \begin{itemize}
  \tightlist
  \item
    Non-informative prior with a different mean, wider or narrower variance
  \item
    Informative prior with a smaller variance
  \item
    Informative prior with a different mean
  \end{itemize}
\end{itemize}

\hypertarget{example-renal-denervation}{%
\subsection{Example: Renal Denervation}\label{example-renal-denervation}}

\begin{itemize}
\tightlist
\item
  A surgical procedure called ``renal denervation'' was developed to help people with hypertension who do not respond to medication.
\item
  In a randomized , blinded, controlled trial (\href{https://www.nejm.org/doi/full/10.1056/NEJMoa1402670}{Bhatt et al., NEJM, 2014}) concluded there was no significant difference between patients who received renal denervation and those who received a sham procedure
\end{itemize}

\includegraphics[width=0.5\linewidth]{./6_38}

``A significant change from baseline to 6 months in office systolic blood pressure was observed in both study groups.

The between-group difference (the primary efficacy end point) did not meet a test of superiority with a margin of 5 mm Hg.

The bars indicate standard deviations.''

\hypertarget{re-analysis-of-renal-denervation-data-using-a-bayesian-approach}{%
\subsection{Re-analysis of Renal Denervation data using a Bayesian approach}\label{re-analysis-of-renal-denervation-data-using-a-bayesian-approach}}

\begin{itemize}
\tightlist
\item
  We need to specify the likelihood function and the prior distribution. Using Bayes Theorem we can derive the posterior distribution
\item
  We will assume that the change in blood pressure in both groups follows a normal distribution with a known standard deviation of 25
\item
  We need to define two prior distributions this time, one for the mean change in the renal denervation group and one for the mean change in the sham group

  \begin{itemize}
  \tightlist
  \item
    we will use a non-informative prior N(0, 1000) prior for both
  \end{itemize}
\end{itemize}

\hypertarget{posterior-distributions}{%
\subsection{Posterior distributions}\label{posterior-distributions}}

\begin{itemize}
\tightlist
\item
  Posterior distribution for the mean change in blood pressure in the renal denervation group (µRD)
\end{itemize}

\(\mu_{RD}|X_{RD}\) \textasciitilde{} \(N\left(A_{RD}\times\theta_{RD}+B_{RD}\times\overline{x_{RD}},standard\space deviation=\sqrt{\frac{1}{\frac{n_{RD}}{\sigma^2}+\frac{1}{\tau^2}}}\right)\)

\begin{itemize}
\tightlist
\item
  Posterior distribution for the mean change in blood pressure in the control group \((\mu_p)\)
\end{itemize}

\(\mu_{s}|X_{s}\) \textasciitilde{} \(N\left(A_{s}\times\theta_{s}+B_{s}\times\overline{x_{s}},standard\space deviation=\sqrt{\frac{1}{\frac{n_{s}}{\sigma^2}+\frac{1}{\tau^2}}}\right)\)

\hypertarget{posterior-distribution-of-the-difference-between-the-two-groups}{%
\subsection{Posterior distribution of the difference between the two groups}\label{posterior-distribution-of-the-difference-between-the-two-groups}}

\begin{itemize}
\tightlist
\item
  Based on the expressions we had seen in Lecture 3 (Slides 33 and 34), we know that the difference between two independent, normally distributed variables is also normal with

  \begin{itemize}
  \tightlist
  \item
    Mean = Difference in the two means
  \item
    Variance = Sum of the two variances
  \end{itemize}
\end{itemize}

\hypertarget{posterior-distribution-of-the-difference-between-the-two-groups-1}{%
\subsection{Posterior distribution of the difference between the two groups}\label{posterior-distribution-of-the-difference-between-the-two-groups-1}}

\includegraphics[width=1\linewidth]{./6_42}

\begin{itemize}
\tightlist
\item
  Therefore:
\end{itemize}

\(\mu_{RD}-\mu_s|data\) \textasciitilde{} N(mean=-2.39, standard deviation=2.33)

\begin{itemize}
\tightlist
\item
  As with the case of the single mean, we can extract various statistics and an equal-tailed 95\% credible interval from this posterior distribution. We can also make probabilistic statements.
\end{itemize}

\hypertarget{summary-of-results-hypothesis-testing-vs.-confidence-interval-vs.-bayesian-inference}{%
\subsection{Summary of results: Hypothesis testing vs.~confidence interval vs.~Bayesian inference}\label{summary-of-results-hypothesis-testing-vs.-confidence-interval-vs.-bayesian-inference}}

\includegraphics[width=1\linewidth]{./6_43}

\begin{itemize}
\tightlist
\item
  The hypothesis test tells us that if there were no difference between the two groups, the probability of observed results more extreme than what we have is 0.26. Since this p-value exceeds the traditional limit of Type I error=0.05 we cannot reject the null hypothesis
\item
  The confidence interval would suggest there is some evidence that the true mean difference between the groups lies outside the zone of clinical equivalence (i.e.~\textless{} -5)
\item
  The Bayesian approach allows to quantify the probability of a clinically meaningful difference between the two groups. It is 0.13. Once again the 95\% credible interval is similar in magnitude to the confidence interval, though its interpretation is different
\end{itemize}

\hypertarget{update-on-the-renal-denervation-story}{%
\subsection{Update on the renal denervation story}\label{update-on-the-renal-denervation-story}}

\begin{itemize}
\tightlist
\item
  More recently ``\ldots{} positive results of second-generation trials helped renal denervation to resurrect as a hopeful therapy for hypertension'' \href{https://vascularnews.com/the-resurgence-of-renal-denervation/}{Lurz, Vascular News, 2018}
\item
  ``Important tasks for the future include refinements in patient selection and technique as well as establishing a measure of procedural success''
\end{itemize}

\hypertarget{why-is-bayesian-inference-not-used-more-widely}{%
\subsection{Why is Bayesian inference not used more widely?}\label{why-is-bayesian-inference-not-used-more-widely}}

\begin{itemize}
\tightlist
\item
  As the ASA statement noted, frequentist statistical methods are still widely taught
\item
  Researchers, including statisticians, are put off by the challenge of learning something new
\item
  Bayesian statistics is perceived as computationally complex
\item
  The prior distribution seems like a subjective choice of the user that could lead to manipulation of results
\end{itemize}

\hypertarget{are-bayesian-methods-worth-the-effort}{%
\subsection{Are Bayesian methods worth the effort?}\label{are-bayesian-methods-worth-the-effort}}

\begin{itemize}
\tightlist
\item
  For simpler problems, like the ones we have seen, frequentist confidence intervals are numerically similar to Bayesian intervals when using a non-informative prior

  \begin{itemize}
  \tightlist
  \item
    None the less, Bayesian methods offer an advantage in terms of interpretation
  \end{itemize}
\item
  For more complex problems, e.g.~those involving hierarchical models or missing data, Bayesian methods are crucial to achieve a sound solution
\end{itemize}

\hypertarget{are-bayesian-methods-complex}{%
\subsection{Are Bayesian methods complex?}\label{are-bayesian-methods-complex}}

\begin{itemize}
\tightlist
\item
  I would argue that they are actually simpler
\item
  No matter how complicated your model, the Bayesian solution always involves the same 3 steps -- specify the likelihood and prior and then obtain the posterior distribution
\item
  The same software package (e.g.~WinBUGS) handles everything -- no need to switch between separates programs for logistic and linear regression, meta-analysis etc.
\end{itemize}

\hypertarget{risk-of-incorrect-conclusions-with-hypothesis-testing}{%
\section{Risk of incorrect conclusions with hypothesis testing}\label{risk-of-incorrect-conclusions-with-hypothesis-testing}}

\includegraphics[width=1\linewidth]{./6_49}

\includegraphics[width=1\linewidth]{./6_50}

Chavalarias et al., JAMA 2016

The ASA Statement:

In light of misuses of and misconceptions concerning p-values, the statement notes that statisticians often supplement or even replace p-values with other approaches. These include methods ``that emphasize estimation over testing such as confidence, credibility, or prediction intervals; Bayesian methods; alternative measures of evidence such as likelihood ratios or Bayes factors; and other approaches such as decision-theoretic modeling and false discovery rates.''

\hypertarget{concerns-with-hypothesis-testing}{%
\subsection{Concerns with hypothesis testing}\label{concerns-with-hypothesis-testing}}

\begin{itemize}
\tightlist
\item
  At the end of a research study there is seldom a need to make a firm decision. Indeed, a single study may not provide enough evidence to make a sound decision
\item
  Yet, the scientific literature has relied heavily on hypothesis testing to identify associations of interest
\item
  Many early reports of associations have proven impossible to replicate suggesting they were spurious
\item
  This is probably a combination of making decisions based on weak evidence, and publication bias
\end{itemize}

\hypertarget{optimizing-decision-making}{%
\subsection{\texorpdfstring{Optimizing decision making\(^*\)}{Optimizing decision making\^{}*}}\label{optimizing-decision-making}}

\begin{itemize}
\tightlist
\item
  When designing a study we focus on the Type I and Type II errors with a view to optimizing the decision process in the long run
\item
  This makes sense in certain settings, e.g.~industrial quality control, where it is possible to define the minimum effect size of interest and also the cost of any errors
\item
  For example, if a factory has to produce screw heads with a diameter of 1 ± 0.01cm, then we know or can estimate

  \begin{itemize}
  \tightlist
  \item
    The minimum effect size of interest is 0.01cm
  \item
    The cost of failing to detect too large or too small screws
  \item
    The cost of throwing away a good batch of screws
  \end{itemize}
\item
  Further, we can control the sample size easily to ensure high power
\item
  In contrast, these elements may be difficult to gauge in a research setting, e.g.

  \begin{itemize}
  \tightlist
  \item
    What is the smallest difference of interest in adipose weight gain in Nck1 Wild Type mice?
  \item
    What is the cost of falsely claiming that a treatment is beneficial?
  \end{itemize}
\item
  Given this ambiguity, researchers often fall back on the default values, like the α=0.05 level
\end{itemize}

\(^*\)Szucs \& Ioannidis, 2017

\hypertarget{concerns-with-p-values}{%
\subsection{Concerns with p-values}\label{concerns-with-p-values}}

\begin{itemize}
\tightlist
\item
  A p-value provides the probability:\\
  P(Data is more extreme than observed\textbar H0)
\item
  But what the investigator desires is P(H0\textbar Data)
\item
  p-values are functions of the data and are affected by sample size
\end{itemize}

\hypertarget{factors-that-influence-the-accuracy-of-hypothesis-testing}{%
\subsection{Factors that influence the accuracy of hypothesis testing}\label{factors-that-influence-the-accuracy-of-hypothesis-testing}}

\begin{itemize}
\tightlist
\item
  The chances of a statistically significant result depend on

  \begin{itemize}
  \tightlist
  \item
    \textbf{Pre-study odds}: The more implausible the null hypothesis, the greater the chance that a significant finding is a false alarm
  \item
    \textbf{Type I error}: This has to be sufficiently low to avoid wrongly rejecting the null hypothesis
  \item
    \textbf{Type II error}: This also has to be low, and defined keeping in mind the magnitude of a practically meaningful effect
  \end{itemize}
\end{itemize}

\hypertarget{probabilities-of-true-and-false-reporting}{%
\subsection{\texorpdfstring{Probabilities of true and false reporting\(^*\)}{Probabilities of true and false reporting\^{}*}}\label{probabilities-of-true-and-false-reporting}}

\begin{itemize}
\tightlist
\item
  Using the 3 factors on the previous slide we can define the following probabilities by applying Bayes Theorem:

  \begin{itemize}
  \tightlist
  \item
    False reporting probability = P(H0 \textbar{} significant result)
  \item
    True reporting probability = P(H1 \textbar{} significant result)
  \end{itemize}
\item
  It is based on a modeling of these probabilities that Ioannidis (PLOS One 2005) claimed most published research findings are false
\end{itemize}

\(^*\)Szucs \& Ioannidis, 2017

\hypertarget{illustration-from-nuzzo-et-al.}{%
\subsection{Illustration from Nuzzo et al.}\label{illustration-from-nuzzo-et-al.}}

\includegraphics[width=1\linewidth]{./6_50}

\hypertarget{illustration-of-true-and-false-reporting-probabilities}{%
\subsection{Illustration of true and false reporting probabilities}\label{illustration-of-true-and-false-reporting-probabilities}}

\begin{itemize}
\tightlist
\item
  Let us assume

  \begin{itemize}
  \tightlist
  \item
    P(H0)=P(H1)=0.5, Type I error = 0.05, Type II error = 0.4
  \end{itemize}
\end{itemize}

False reporting probability P(H0\textbar significant result)\\
\(\frac{P(significant\space result|H0)P(H0)}{P(significant\space result)}=\frac{0.05\times0.5}{0.05\times0.5+0.6\times0.5}=0.0769\)

which is higher than the value of 0.05, which we often confuse to be the false-reporting probability

\begin{itemize}
\tightlist
\item
  Let us assume

  \begin{itemize}
  \tightlist
  \item
    P(H0)=5/6 P(H1)=1/6, Type I error = 0.05, Type II error = 0.4
  \end{itemize}
\end{itemize}

False reporting probability P(H0\textbar significant result)
\(\frac{P(significant\space result|H0)P(H0)}{P(significant\space result)}=\frac{0.05\times(\frac{5}{6})}{0.05\times(\frac{5}{6})+0.6\times(\frac{1}{6})}=0.2941\)

which is considerably larger than 0.05

\begin{itemize}
\tightlist
\item
  We may not be able to control the pre-study odds but by designing a study with higher power, we can reduce the FRP somewhat
\end{itemize}

\hypertarget{pre-study-odds-of-h0h1}{%
\subsection{Pre-study odds of H0:H1}\label{pre-study-odds-of-h0h1}}

\includegraphics[width=1\linewidth]{./6_60}

\hypertarget{lecture-7-statistical-inference-for-proportions}{%
\chapter{Lecture 7: Statistical inference for proportions}\label{lecture-7-statistical-inference-for-proportions}}

\hypertarget{when-is-a-single-proportion-used}{%
\section{When is a single proportion used?}\label{when-is-a-single-proportion-used}}

\begin{itemize}
\tightlist
\item
  Proportions are very commonly used in medical research to summarize dichotomous variables
\item
  Some examples include estimation of:

  \begin{itemize}
  \tightlist
  \item
    response rate in a survey
  \item
    prevalence of a disease
  \item
    sensitivity, specificity, positive and negative predictive values for a diagnostic test
  \end{itemize}
\end{itemize}

\hypertarget{central-limit-theorem-2}{%
\subsection{Central Limit Theorem}\label{central-limit-theorem-2}}

\begin{itemize}
\tightlist
\item
  From the text by Moore and McCabe:
\end{itemize}

\emph{``\ldots{} as the sample size increases, the distribution of \(\bar Y\) becomes closer to a normal distribution}\ldots{} no matter what the population distribution may be, as long as the population has a finite standard deviation σ.

This famous fact of probability theory is called the \emph{central limit theorem}.

For large sample size n, we can regard \(\bar Y\) as having the \(N(\mu,\sigma/\sqrt n)\) distribution''

\hypertarget{application-of-the-central-limit-theorem-normal-approximation-to-the-binomial-distribution}{%
\subsection{Application of the Central Limit Theorem: Normal Approximation to the Binomial Distribution}\label{application-of-the-central-limit-theorem-normal-approximation-to-the-binomial-distribution}}

\begin{itemize}
\tightlist
\item
  In Lecture 3, we also studied expressions that showed that the sum of a series of random variables follows a Normal distribution (like the mean does)
\item
  Recall that the \textbf{sum of the number of successes} in each of n Bernoulli trials follows a Binomial distribution with mean nπ and variance nπ(1-π)
\item
  Let us look at the sampling distribution when n=5, 20 and 50 and π=0.1, 0.5 and 0.9
\end{itemize}

\hypertarget{sampling-distributions-when-ux3c00.1}{%
\subsection{Sampling distributions when π=0.1}\label{sampling-distributions-when-ux3c00.1}}

\includegraphics[width=0.5\linewidth]{./7_6}

\hypertarget{sampling-distributions-when-ux3c00.5}{%
\subsection{Sampling distributions when π=0.5}\label{sampling-distributions-when-ux3c00.5}}

\includegraphics[width=0.5\linewidth]{./7_7}

\hypertarget{sampling-distributions-when-ux3c00.9}{%
\subsection{Sampling distributions when π=0.9}\label{sampling-distributions-when-ux3c00.9}}

\includegraphics[width=0.5\linewidth]{./7_8}

\hypertarget{sampling-distribution-of-a-binomial-variable}{%
\subsection{Sampling distribution of a Binomial variable}\label{sampling-distribution-of-a-binomial-variable}}

\begin{itemize}
\tightlist
\item
  We can see that as the sample size increases, the mean of the sampling distribution tends towards a normal distribution with mean nπ and variance nπ(1-π)
\item
  This is true even for extreme values of π close to 0 or 1, though values closer to π=0.5 will approach normality at a smaller sample size
\end{itemize}

\hypertarget{normal-approximation-to-the-binomial-distribution}{%
\subsection{Normal approximation to the Binomial distribution}\label{normal-approximation-to-the-binomial-distribution}}

*Recall that the probability density function for the Binomial distribution is given by

\[Pr\{X=k|n,p\}=\frac{n!}{k!(n-k)!}\pi^k(1-\pi)^{n-k}\]

\begin{itemize}
\tightlist
\item
  This expression can get very cumbersome to evaluate (especially with no computer) as n increases. Also tables of the Binomial distribution will not list all values of n
\item
  Therefore, as n increases, we can use the normal approximation to the Binomial distribution instead to determine the Binomial density of cumulative probability
\item
  How large should n be? The normal approximation to the binomial distribution is fairly good if both np and n(1−p) are at least equal to 5, where p is the sample proportion.
\end{itemize}

\hypertarget{example-4-estimating-the-number-of-true-positives}{%
\subsection{Example 4: Estimating the number of true positives}\label{example-4-estimating-the-number-of-true-positives}}

\begin{itemize}
\tightlist
\item
  In low resource settings, hospitals often rely on smear microscopy for TB diagnosis despite its poor sensitivity
\item
  The GeneXpert test is a point-of-care nucleic acid amplification test that has revolutionized testing for pulmonary TB .

  \begin{itemize}
  \tightlist
  \item
    It has a sensitivity of 70\% in TB cases missed by smear
  \end{itemize}
\item
  In a particular clinic setting, it is estimated that 150 TB patients are missed annually by smear microscopy.

  \begin{itemize}
  \tightlist
  \item
    What is the probability that at least 100 of them are correctly identified by GeneXpert?
  \end{itemize}
\end{itemize}

\hypertarget{example-4-applying-a-continuity-correction}{%
\subsection{Example 4: Applying a continuity correction}\label{example-4-applying-a-continuity-correction}}

\begin{itemize}
\tightlist
\item
  X=Number of positive results on GeneXpert out of 150 patients
\item
  P(X ≥ 100 \textbar{} Binomial(n=150, π=0.7))\\
  = P(X ≥ 99.5 \textbar Binomial(n=150, π=0.7))\\
  ≈ P(X ≥ 99.5 \textbar Normal(mean=105,variance=32))\\
  = 1 -- pnorm(99.5,mean=105,sd=sqrt(32))\\
  = 0.83
\end{itemize}

\hypertarget{continuity-correction}{%
\subsection{Continuity correction}\label{continuity-correction}}

\begin{itemize}
\tightlist
\item
  The change from 100 to 99.5 is called the continuity correction. It is used to make the normal approximation slightly more accurate.
\item
  If n is large the continuity correction may not have a huge impact when estimating the probability of being greater than or the probability of being less than a certain value.
\item
  However, we absolutely need the continuity correction to estimate the probability of being exactly equal to a certain value. This is because this probability is always zero under a normal distribution
\end{itemize}

\hypertarget{example-4}{%
\subsection{Example}\label{example-4}}

\begin{itemize}
\tightlist
\item
  What is the probability that exactly 100 patients will be detected by GeneXpert?
\item
  P(X = 100 \textbar{} Binomial(n=150, π=0.7))\\
  = P(99.5 ≤ X ≤ 100.5 \textbar Binomial(n=150, π=0.7))\\
  ≈ P(99.5 ≤ X ≤ 100.5 \textbar{} Normal(mean=105,variance=32))\\
  = pnorm(100.5,mean=105,sd=sqrt(32)) -- pnorm(99.5,mean=105,sd=sqrt(32))\\
  = 0.05
\end{itemize}

\hypertarget{comparison-with-the-exact-results-based-on-the-binomial-distribution}{%
\subsection{Comparison with the exact results based on the Binomial distribution}\label{comparison-with-the-exact-results-based-on-the-binomial-distribution}}

\begin{itemize}
\tightlist
\item
  What is the probability that at least 100 or exactly 100 patients will be detected by GeneXpert?
\item
  The exact results based on the Binomial distribution are given by
\end{itemize}

P(X ≥ 100 \textbar{} Binomial(n=150, π=0.7))\\
= 1- pbinom(99,150,0.7) = 0.84

P(X = 100 \textbar{} Binomial(n=150, π=0.7))\\
= dbinom(100,150,0.7) = 0.05

We can see they are very close to the approximate values based on the Normal distribution

\hypertarget{example-asymptomatic-colonization-with-clostridium-difficile}{%
\subsection{Example: Asymptomatic colonization with Clostridium difficile}\label{example-asymptomatic-colonization-with-clostridium-difficile}}

\begin{itemize}
\tightlist
\item
  \emph{Clostridium difficile} diarrhea (CDAD) is the most common nosocomial diarrhea, prolonging hospitalization and for some patients leading to colectomy or death.
\item
  Patients are typically exposed to CD in a hospital setting, but may also become exposed in the community
\item
  After exposure, an estimated 2/3 patients become asymptomatic carriers. There is a concern that asymptomatic colonization may play a greater role in spreading CD infection than previously thought
\item
  A large, multi-centre Canadian study examined the risk of asymptomatic colonization at admission to hospital. They also examined various risk factors for asymptomatic colonization, including patient and pathogen characteristics (Kong et al, Am J Inf Control, 2015)
\end{itemize}

\hypertarget{asymptomatic-colonization-with-clostridium-difficile-selected-results}{%
\subsection{Asymptomatic colonization with Clostridium difficile: Selected results}\label{asymptomatic-colonization-with-clostridium-difficile-selected-results}}

\begin{itemize}
\tightlist
\item
  Out of 5232 subjects, 212 were asymptomatically colonized at admission
\item
  Among the 212 asymptomatically colonized patients:

  \begin{itemize}
  \tightlist
  \item
    136 were hospitalized during the previous year
  \item
    98 used antibiotics in the previous 8 weeks
  \item
    1 used a probiotic in the previous 8 weeks
  \end{itemize}
\item
  Among 209 of the 212 patients, information was available on the strain of \emph{Clostridium difficile}

  \begin{itemize}
  \tightlist
  \item
    33 patients carried the hypervirulent NAP1 strain
  \end{itemize}
\end{itemize}

\hypertarget{example-asymptomatic-colonization-with-clostridium-difficile-1}{%
\subsection{Example: Asymptomatic colonization with Clostridium difficile}\label{example-asymptomatic-colonization-with-clostridium-difficile-1}}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Estimate the proportion of asymptomatically colonized patients together with a 95\% confidence interval. What is your interpretation of the 95\% confidence interval?
\item
  Estimate the proportion of patients with each risk factor and calculate a 95\% confidence interval in each case.
\item
  It has previously been reported that 50\% of patients who develop Clostridium difficile infection carry the NAP1 strain. Carry out a hypothesis test to determine whether patients who are asymptomatically colonized at admission have a comparable risk of carrying the NAP1 strain.
\end{enumerate}

\hypertarget{methods-for-means-vs.-proportions}{%
\subsection{Methods for means vs.~proportions}\label{methods-for-means-vs.-proportions}}

\begin{itemize}
\tightlist
\item
  You will notice that all of the methods we studied earlier for means, have a counterpart for proportions
\item
  As in the case of inference for means, we can distinguish between the following themes

  \begin{itemize}
  \tightlist
  \item
    Methods for a single proportion and for the difference between proportions
  \item
    Frequentist inference and Bayesian inference
  \item
    Hypothesis tests and confidence intervals
  \end{itemize}
\end{itemize}

\hypertarget{analogy-between-calculation-of-means-and-proportions}{%
\subsection{Analogy between calculation of means and proportions}\label{analogy-between-calculation-of-means-and-proportions}}

\begin{tabular}{l|l|l}
\hline
  & Means & Proportions\\
\hline
Sample data & \$(x\_1,x\_2,...,x\_n)\$ & \$(x\_1,x\_2,...,x\_n)=(1,0,...,1)\$\\
\hline
Estimator of the population mean & \$\textbackslash{}bar x=\textbackslash{}frac\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}nx\_1\}\{n\}\$ & \$p=\textbackslash{}frac\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}nx\_1\}\{n\}=\textbackslash{}frac\{\textbackslash{}\#\textbackslash{}space of\textbackslash{}space 1's\}\{n\}\$\\
\hline
Estimator of the population standard deviation & \$sd=\textbackslash{}sqrt\{\textbackslash{}frac\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}n(x\_i-\textbackslash{}bar x)\textasciicircum{}2\}\{n-1\}\}\$ & \$sd=\textbackslash{}sqrt\{p(1-p)\}\$\\
\hline
95\% confidence interval for the population mean & \$\textbackslash{}bar x\textbackslash{}pm 1.96\textbackslash{}frac\{sd\}\{\textbackslash{}sqrt n\}\$ & \$p\textbackslash{}pm 1.96\textbackslash{}frac\{sd\}\{\textbackslash{}sqrt n\}\$\\
\hline
\end{tabular}

\hypertarget{frequentist-confidence-interval-for-a-single-proportion}{%
\subsection{Frequentist confidence interval for a single proportion}\label{frequentist-confidence-interval-for-a-single-proportion}}

\begin{itemize}
\tightlist
\item
  Confidence interval for a large sample using the normal approximation
\item
  Confidence interval for a small sample using the exact Binomial distribution
\end{itemize}

\hypertarget{large-sample-confidence-interval-for-a-proportion}{%
\subsection{Large sample confidence interval for a proportion}\label{large-sample-confidence-interval-for-a-proportion}}

\begin{itemize}
\tightlist
\item
  Assume a large random sample of a dichotomous variable X of size n is drawn from a population with unknown proportion of successes (π)
\item
  Let p be the observed proportion of successes
\item
  Then an approximate level \((1-\alpha)\)\% confidence interval for \(\pi\) is given by
\end{itemize}

\[p\pm Z_{1-\frac{\alpha}{2}}\times\sqrt{\frac{p(1-p)}{n}}\]

where \(Z_{1-\alpha/2}\) is the upper \(1-\alpha/2\) standard Normal quantile

\begin{itemize}
\tightlist
\item
  This confidence interval has the usual frequentist interpretation that it was obtained using a \textbf{procedure} about which we have (1-α)\% confidence. Across (1-α)\% of repeated studies the confidence interval will include π. However, we do not know if the particular interval at hand includes the true value of π
\item
  Notice that we used the Normal distribution quantile to construct the confidence interval. We were able to do this because:

  \begin{itemize}
  \tightlist
  \item
    We are relying on the central limit theorem that tells us that even though the variable of interest (X) follows a Bernoulli distribution, the sample mean is normally distributed provided the sample size is large enough.
  \item
    The sample proportion automatically provides an estimate for the sample standard deviation. Therefore we do not need to provide a separate estimate for the standard deviation and introduce another source of uncertainty. As a result we do not need to rely on the t-distribution
  \end{itemize}
\end{itemize}

\hypertarget{exact-confidence-interval-for-a-proportion}{%
\subsection{Exact confidence interval for a proportion}\label{exact-confidence-interval-for-a-proportion}}

\begin{itemize}
\tightlist
\item
  When the sample size is small or when p is close to 0 or 1, the normal approximation based on the central limit theorem can be poor
\item
  In such cases, it may be preferable to use the exact method for obtaining a confidence interval which is based directly on the binomial distribution.
\item
  This method is more difficult to calculate, though the availability of fast computers today makes this irrelevant
\end{itemize}

\hypertarget{clopper-pearson-exact-confidence-interval-for-a-proportion-biometrika-1934}{%
\subsection{Clopper-Pearson exact confidence interval for a proportion (Biometrika, 1934)}\label{clopper-pearson-exact-confidence-interval-for-a-proportion-biometrika-1934}}

\begin{itemize}
\tightlist
\item
  Let x be the observed number of successes and n be the number of trials. Let X denote a Binomial(π,n) random variable
\item
  The two-sided (1-α)\% exact confidence interval is given by the interval \((p_L, p_H)\) such that
\end{itemize}

\[\sum_{k=0}^x{n \choose k}p_H^k(1-p_H)^{n-k}=\frac{\alpha}{2}\]
\[\sum_{k=x}^n{n \choose k}p_L^k(1-p_L)^{n-k}=\frac{\alpha}{2}\]

\hypertarget{exact-confidence-interval-for-a-proportion-1}{%
\subsection{\texorpdfstring{Exact confidence interval for a proportion\(^*\)}{Exact confidence interval for a proportion\^{}*}}\label{exact-confidence-interval-for-a-proportion-1}}

\includegraphics[width=1\linewidth]{./7_26}

\(^*\)From \url{http://www.biyee.net/data-solution/resources/binomial-confidence-interval.aspx}

\hypertarget{r-function-to-obtain-a-confidence-interval-for-a-small-proportion}{%
\subsection{R function to obtain a confidence interval for a small proportion}\label{r-function-to-obtain-a-confidence-interval-for-a-small-proportion}}

\begin{itemize}
\tightlist
\item
  The R function binom.test carries out an exact binomial test and also provides an exact confidence interval
\end{itemize}

\begin{verbatim}
binom.test(x, n, p = 0.5,
           alternative = c("two.sided", "less", "greater"),
           conf.level = 0.95)
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The user has to provide the number of successes and the sample size among other arguments
\end{itemize}

\hypertarget{exact-confidence-interval-for-a-proportion-2}{%
\subsection{Exact confidence interval for a proportion}\label{exact-confidence-interval-for-a-proportion-2}}

\begin{itemize}
\tightlist
\item
  Following is a comparison of exact and approximate confidence intervals. The gain in accuracy drops off with increasing sample size.
\end{itemize}

\includegraphics[width=0.5\linewidth]{./7_28}

\hypertarget{how-do-you-determine-if-your-sample-is-sufficiently-large}{%
\subsection{How do you determine if your sample is sufficiently large?}\label{how-do-you-determine-if-your-sample-is-sufficiently-large}}

\begin{itemize}
\tightlist
\item
  Some common rules of thumb:

  \begin{itemize}
  \tightlist
  \item
    Both \emph{n(1-p)} and \emph{np} should be greater than 5 (greater than 10 according to some text books)
  \item
    \(np\pm3\sqrt{np(1-p)}\) should lie between 0 and n
  \item
    In general if the distribution is shifted towards 0 or 1 and if confidence intervals calculated by the usual method give negative lower or upper bounds, it is better to use the exact method.
  \end{itemize}
\end{itemize}

\hypertarget{example-zero-proportion}{%
\subsection{Example: Zero proportion}\label{example-zero-proportion}}

\begin{itemize}
\tightlist
\item
  The standard contrast agent used by radiologists over a long period has been shown to cause a serious reaction in about 15 of every 10,000 patients exposed to it
\item
  A new contrast agent is introduced. Based on the first report of its use in 167 patients, no patient had a serious reaction. Do we have enough evidence to say that the new contrast agent is at least as safe to use as the old one?
\item
  The thumb rules on the previous slide would lead us to conclude an exact confidence interval is required
\item
  The risk of an adverse reaction with the old reagent is 15/10000 = 0.0015
\item
  The point estimate of the risk with the new contrast agent is 0
\item
  The 95\% one-sided exact confidence interval is (0, 0.0178)
\item
  This tells us that the observed data could possibly arise from situations where the true value of π ranges from 0 to 0.0178. This includes situations where the true risk of 0.0015
\item
  It also means we cannot eliminate the possibility that the risk slightly greater than 0.0015
\end{itemize}

\hypertarget{example-binom.test}{%
\subsection{Example: binom.test()}\label{example-binom.test}}

\begin{verbatim}
> binom.test(0,167,alternative="less",p=0.0015)

    Exact binomial test

data:  0 and 167
number of successes = 0, number of trials = 167, p-value = 0.7783
alternative hypothesis: true probability of success is less than 0.0015
95 percent confidence interval:
 0.00000000 0.01777858
sample estimates:
probability of success 
                     0 
\end{verbatim}

The R function above provides the result of a one-sided hypothesis test of \(H_0: p≥0.0015\) vs.~\(H_A: p<0.0015\). The confidence interval is the range of possible values of p that cannot be rejected under the null hypothesis.

\hypertarget{example-asymptomatic-colonization-with-c.-difficile}{%
\subsection{Example: Asymptomatic colonization with C. difficile}\label{example-asymptomatic-colonization-with-c.-difficile}}

\begin{itemize}
\tightlist
\item
  Proportion of patients asymptomatically colonized at admission = 212/5232 = 0.041
\item
  Proportion of patients with risk factor

  \begin{itemize}
  \tightlist
  \item
    Recent hospitalization = 136/212 = 0.642
  \item
    Recent antibiotic use = 98/212 = 0.462
  \item
    Recent probiotic use = 1/212 = 0.005
  \end{itemize}
\item
  Proportion of asymptomatically colonized carrying NAP1 strain = 33/209 = 0.158
\end{itemize}

\hypertarget{exact-or-approximate-ci}{%
\subsection{Exact or approximate CI?}\label{exact-or-approximate-ci}}

\begin{itemize}
\tightlist
\item
  For most of the proportions on the previous slide we can see that \emph{np} and \emph{n(1-p)} are both greater than 5
\item
  A quick way to check this is to look at the number of subjects with and without the outcome and see if they are both greater than 5
\end{itemize}

\hypertarget{ci-for-proportion}{%
\subsection{95\% CI for proportion}\label{ci-for-proportion}}

\begin{itemize}
\tightlist
\item
  The 95\% confidence interval for the risk of asymptomatic colonization at admission is given by
\end{itemize}

\[p\pm z_{1-\alpha/2}\times\sqrt{\frac{p(1-p)}{n}}=0.041\pm1.96\sqrt{\frac{0.041(1-0.041)}{5232}}\]
\[=0.041\pm0.005\]
\[=(0.036,0.046),\space approximately\]

\begin{itemize}
\tightlist
\item
  This confidence interval tells us that the information in the data is consistent with values of 𝜋 ranging from 3.6\% to 4.6\%. The observed data is unlikely to arise from a setting where 𝜋 \textgreater0.05.
\item
  This implies, based on the observed data, we would reject H0: 𝜋 \textgreater0.05 at the Type I error level of 0.05
\item
  The confidence intervals for the proportion of patients with different risk factors can be obtained similarly
\end{itemize}

\hypertarget{exact-or-approximate-ci-1}{%
\subsection{Exact or approximate CI?}\label{exact-or-approximate-ci-1}}

\begin{itemize}
\tightlist
\item
  However, only 1 subject reported using probiotics and 211 subjects did not
\item
  Therefore, based on our rule of thumb we cannot use the normal approximation to obtain a 95\% confidence interval for this proportion and must use an exact method instead
\item
  Using the approximate method would result in the following confidence interval (-0.005, 0.014), which is clearly not appropriate as a proportion cannot be negative!
\end{itemize}

\hypertarget{exact-95-equal-tailed-confidence-interval-for-proportion-of-probiotic-use}{%
\subsection{Exact 95\% equal-tailed confidence interval for proportion of probiotic use}\label{exact-95-equal-tailed-confidence-interval-for-proportion-of-probiotic-use}}

\begin{verbatim}
> binom.test(1,212)

    Exact binomial test

data:  1 and 212
number of successes = 1, number of trials = 212, p-value < 2.2e-16
alternative hypothesis: true probability of success is not equal to 0.5
95 percent confidence interval:
 0.0001194165 0.0259997103
sample estimates:
probability of success 
           0.004716981
\end{verbatim}

The R function above provides the result of a two-sided hypothesis test of \(H_0: p=0.5\). The confidence interval is the range of possible values of p that cannot be rejected under the null hypothesis.

\hypertarget{confidence-intervals-for-proportion-of-patients-with-risk-factor-among-patients-asymptomatically-colonized-with-c.-difficile}{%
\subsection{95\% confidence intervals for proportion of patients with risk factor among patients asymptomatically colonized with C. difficile}\label{confidence-intervals-for-proportion-of-patients-with-risk-factor-among-patients-asymptomatically-colonized-with-c.-difficile}}

\includegraphics[width=1\linewidth]{./7_38}

\hypertarget{statistics-used-to-compare-two-proportions-p_1-and-p_2}{%
\subsection{\texorpdfstring{Statistics used to compare two proportions \(p_1\) and \(p_2\)}{Statistics used to compare two proportions p\_1 and p\_2}}\label{statistics-used-to-compare-two-proportions-p_1-and-p_2}}

\begin{itemize}
\tightlist
\item
  Several different statistics may be used to compare proportions. These include relative comparisons and absolute comparisons

  \begin{itemize}
  \tightlist
  \item
    Difference: \(p_1-p_2\)
  \item
    Odds ratios: \(\frac{p_1(1-p_2)}{p_2(1-p_1)}\)
  \item
    Risk ratios: \(\frac{p_1}{p_2}\)
  \item
    Number needed to treat: \(\frac{1}{p_1-p_2}\)
  \end{itemize}
\end{itemize}

\hypertarget{example-probiotics-for-prevention-of-cdad}{%
\subsection{Example: Probiotics for prevention of CDAD}\label{example-probiotics-for-prevention-of-cdad}}

\begin{itemize}
\tightlist
\item
  \emph{Clostridium difficile} diarrhea (CDAD), which was introduced earlier in the lecture, is strongly associated with antibiotic use
\item
  An outbreak of a virulent form of CDAD in Quebec in 2004 has heightened the interest in tackling this infectious disease
\item
  A probiotic is a live microorganism or a mixture of various bacteria administered to improve the microbial balance in the host GI system
\end{itemize}

\hypertarget{results-from-two-randomized-controlled-trials}{%
\subsection{Results from two randomized controlled trials}\label{results-from-two-randomized-controlled-trials}}

\begin{itemize}
\tightlist
\item
  A recent systematic review identified 10 studies on the efficacy of lactobacillus probiotics for prevention of CDAD
\item
  We will take a closer look at two of them
\item
  For each of these studies we will calculate a 95\% confidence interval for the difference in proportions and also a p-value for a two-sided test
\end{itemize}

\hypertarget{forest-plot-of-relative-risk-from-10-studies}{%
\subsection{\texorpdfstring{Forest plot of relative risk from 10 studies\(^*\)}{Forest plot of relative risk from 10 studies\^{}*}}\label{forest-plot-of-relative-risk-from-10-studies}}

\includegraphics[width=1\linewidth]{./7_42}

\(^*\)From \url{http://cmajopen.ca/content/4/4/E706.abstract}

\hypertarget{results-from-two-randomized-controlled-trials-1}{%
\subsection{Results from two randomized controlled trials}\label{results-from-two-randomized-controlled-trials-1}}

\includegraphics[width=1\linewidth]{./7_43}

\begin{itemize}
\tightlist
\item
  Compare the two proportions reported in each study using hypothesis tests, confidence intervals
\end{itemize}

\hypertarget{confidence-interval-for-the-difference-between-two-proportions}{%
\subsection{Confidence Interval for the difference between two proportions}\label{confidence-interval-for-the-difference-between-two-proportions}}

\begin{itemize}
\tightlist
\item
  Assume we have data from two independent, random samples of size \(n_1\) and \(n_2\) and that the observed proportions of the outcome of interest in the two samples are \(p_1\) and \(p_2\), respectively
\item
  Assume these two samples are drawn from two independent, large populations with the true proportions of the outcomes of interest being \(\pi_1\) and \(\pi_2\), respectively
\item
  When \(n_1\) and \(n_2\) are large, an approximate level \((1-\alpha)\)\% confidence interval for \(\pi_1-\pi_2\) is
\end{itemize}

\[p_1-p_2\pm Z_{1-\alpha/2}\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}\]

where \(Z_{1-\alpha/2}\) is the upper \(1-\alpha/2\) standard Normal critical value

\hypertarget{hypothesis-test-for-comparing-two-proportions}{%
\subsection{Hypothesis test for comparing two proportions}\label{hypothesis-test-for-comparing-two-proportions}}

\begin{itemize}
\tightlist
\item
  Carrying out a hypothesis test involves the same 4 steps as we had seen before:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  State the null and alternative hypotheses. The alternative could be one or two-sided
\item
  Calculate the test statistic measuring the evidence in favour of the null hypothesis
\item
  Determine the rejection region specified by the desired Type I error (α)
\item
  Determine if the test statistic falls in the rejection region. If yes, you conclude that you have enough evidence to reject the null hypothesis. If no, you conclude that you do not have enough evidence to reject the null hypothesis
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The null and alternative hypotheses may be stated in one of 3 ways as in the table below:
\end{itemize}

\includegraphics[width=0.5\linewidth]{./7_46}

\begin{itemize}
\item
  Notice that \(H_0:\pi_1=\pi_2\) is equivalent to \(H_0:risk\space ratio=1\) or \(H_0:odds\space ratio=1\). In other words, these hypotheses can also be tested using the methods for the difference between two proportions = 0
\item
  The test statistic in all 3 cases is given by \(z=\frac{p_1-p_2}{s_p}\)\\
  where \(s_p=\sqrt{p(1-p)\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}\) and \(p=\frac{x_1+x_2}{n_1+n_2},\)\\
  where \(x_1\) and \(x_2\) are the number of subjects who had the outcome of interest in group 1 and group 2, respectively
\item
  The rejection region is specified by the desired Type I error (α).
\item
  Quantiles of the normal distribution are used to define the rejection region as summarized in the table below
\end{itemize}

\includegraphics[width=0.7\linewidth]{./7_48}

\begin{itemize}
\tightlist
\item
  Note that \(P(Z>Z_{1-\alpha})=P(Z<Z_\alpha)=\alpha\). Similarly, \(P(Z>Z_{1-\alpha/2})=P(Z<Z_\alpha/2)=\alpha/2\), where Z is a standard normal variable
\item
  The p-value is the probability of being more extreme than the observed z statistic under the standard normal distribution. Note that the measurement of `more extreme' is determined by whether the alternative hypothesis is one or two-sided
\end{itemize}

\includegraphics[width=0.7\linewidth]{./7_49}

\hypertarget{probiotics-example-confidence-interval-for-difference-in-proportions}{%
\subsection{Probiotics example: Confidence interval for difference in proportions}\label{probiotics-example-confidence-interval-for-difference-in-proportions}}

\begin{itemize}
\tightlist
\item
  Using the results from Allen et al., we will denote the probiotics group as Group 1 and the placebo group as Group 2
\item
  First calculate the difference in proportion of CDAD cases in each group\\
  \(p_1 - p_2 = 21/1493 - 30/1488 = 0.014 - 0.020 = -0.006\)
\item
  Then calculate the standard deviation of the difference in proportions
\end{itemize}

\[\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}=\sqrt{\frac{\left(\frac{21}{1493}\right)\left(1-\frac{21}{1493}\right)}{1493}+\frac{\left(\frac{30}{1488}\right)\left(1-\frac{30}{1488}\right)}{1488}}=0.005\]

\begin{itemize}
\tightlist
\item
  The 95\% confidence interval is then given by\\
  \(-0.006 ± Z_{1-0.025} × 0.005 = -0.006 ± 1.96 × 0.005 = (-0.016, 0.004),\)\\
  implying the observed difference between the two groups is not statistically significant compared to the Type I error level of 0.05
\end{itemize}

\hypertarget{probiotics-example-hypothesis-test}{%
\subsection{Probiotics example: Hypothesis test}\label{probiotics-example-hypothesis-test}}

\begin{itemize}
\tightlist
\item
  The authors of the study carried out a two-sided hypothesis test, allowing for the possibility that probiotics decreased or increased the risk of CDAD
\item
  This may be expressed as
\end{itemize}

\(H_0:\pi_1=\pi_2\) vs.~\(H_a:\pi_1\neq\pi_2\)

\begin{itemize}
\tightlist
\item
  To calculate the test statistic, we need to first calculate a pooled proportion given by
\end{itemize}

\[p=\frac{x_1+x_2}{n_1+n_2}=\frac{21+30}{1493+1488}=0.017\]

\begin{itemize}
\tightlist
\item
  The test statistic is given by
\end{itemize}

\[z=\frac{p_1-p_2}{\sqrt{p(1-p)\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}=\frac{-0.006}{\sqrt{0.017(1-0.017)\left(\frac{1}{1493}+\frac{1}{1488}\right)}}\]

\begin{itemize}
\tightlist
\item
  Since this is a two-sided test, we calculate the p-value using the following expression:
\end{itemize}

\(2P(Z\geq|z|)=2P(Z\geq|-1.267|)\)\\
\(=2P(Z\geq1.267)\)\\
\(=2(1-P(Z<1.267))\)

\begin{verbatim}
= 2*(1 – pnorm(1.267))   # R code 
\end{verbatim}

\(= 0.19,\space approximately\)

\begin{itemize}
\tightlist
\item
  Implying that even if there were no difference between the two groups, there is a 0.19 probability of observing the difference reported in this study or something more extreme
\item
  Compared to the traditional level of significance of 0.05, we would consider this result is not statistically significant
\end{itemize}

\hypertarget{probiotics-example-results-for-both-studies}{%
\subsection{Probiotics example: Results for both studies}\label{probiotics-example-results-for-both-studies}}

\includegraphics[width=1\linewidth]{./7_54}

\begin{itemize}
\tightlist
\item
  Notice that the Gao study resulted in the conclusion that the observed difference in proportions was statistically significant
\item
  Looking at just these two studies, it is clear that no study should be evaluated on its own
\item
  Ideally, we should analyze results from all available studies (i.e.~carry out a meta-analysis) before we can draw a conclusion. Even then, it would be important to consider the possibility of publication bias
\end{itemize}

\hypertarget{comparing-two-proportions-in-r}{%
\subsection{Comparing two proportions in R}\label{comparing-two-proportions-in-r}}

\hypertarget{description}{%
\subsubsection{Description}\label{description}}

prop.test can be used for testing the null that the proportions (probabilities of success) in several groups are the same, or that they equal certain given values.

\hypertarget{usage}{%
\subsubsection{Usage}\label{usage}}

\begin{verbatim}
prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)
\end{verbatim}

\hypertarget{probiotics-example-obtaining-the-results-in-r}{%
\subsection{Probiotics example: Obtaining the results in R}\label{probiotics-example-obtaining-the-results-in-r}}

\hypertarget{gao-study}{%
\subsubsection{Gao study}\label{gao-study}}

\begin{verbatim}
> cdad=c(9,20)
> n=c(171,84)
> prop.test(cdad,n,correct=F)

    2-sample test for equality of proportions without continuity correction

data:  c(9, 20) out of c(171, 84)
X-squared = 19.223, df = 1, p-value = 1.163e-05
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.2825003 -0.0884270
sample estimates:
    prop 1     prop 2 
0.05263158 0.23809524
\end{verbatim}

\hypertarget{allen-study}{%
\subsubsection{Allen study}\label{allen-study}}

\begin{verbatim}
> cdad=c(21,30)
> n=c(1493,1488)
> prop.test(cdad,n,correct=F)

    2-sample test for equality of proportions without continuity correction

data:  cdad out of n
X-squared = 1.6467, df = 1, p-value = 0.1994
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.015405919  0.003214617
sample estimates:
    prop 1     prop 2 
0.01406564 0.02016129 
\end{verbatim}

\hypertarget{number-needed-to-treat}{%
\subsection{Number needed to treat}\label{number-needed-to-treat}}

\begin{itemize}
\tightlist
\item
  The number needed to treat (NNT) is often reported in randomized controlled trials with a binary outcome
\item
  It is the estimated number of patients who need to be treated with the new treatment rather than the standard treatment for one additional patient to benefit
\item
  It is calculated as the \textbf{inverse of the absolute risk difference}, i.e.~\(NNT=\frac{1}{|p_1-p_2|}\)
\item
  Notice that the NNT can range from a minimum of 1 to a maximum of ∞
\end{itemize}

\hypertarget{probiotics-example-number-needed-to-treat}{%
\subsection{Probiotics example: Number needed to treat}\label{probiotics-example-number-needed-to-treat}}

\includegraphics[width=1\linewidth]{./7_58}

\begin{itemize}
\tightlist
\item
  Therefore, the Gao study leads to the interpretation that roughly 4 patients need to be given probiotics to prevent 1 case of CDAD
\item
  The Allen study on the other hand suggests 167 patients would need to be given probiotics to prevent 1 case of CDAD
\end{itemize}

\hypertarget{confidence-interval-for-number-needed-to-treat}{%
\subsection{Confidence interval for number needed to treat}\label{confidence-interval-for-number-needed-to-treat}}

\begin{itemize}
\tightlist
\item
  Let the interval \((d_L, d_H)\) denote the 95\% confidence interval for the absolute risk difference
\item
  Calculating the confidence interval for the NNT is obtained by inverting these limits

  \begin{itemize}
  \tightlist
  \item
    When the risk difference is statistically significant it is straightforward to obtain the interval as \((1/d_H, 1/d_L)\), e.g.~see the Gao study
  \item
    When the risk difference is not statistically significant the interval encompasses both the NNH (or number needed to harm) and the NNB (number needed to benefit) and is more complicated. One way to write the interval is as (\(1/d_H\) to ∞ to \(1/d_L\)), e.g.~see the paper by Altman (BMJ 1998) on course website
  \end{itemize}
\end{itemize}

\hypertarget{sample-size-determination-for-studies-of-proportions}{%
\section{Sample size determination for studies of proportions}\label{sample-size-determination-for-studies-of-proportions}}

\hypertarget{sample-size-for-desired-margin-of-error}{%
\subsection{Sample Size For Desired Margin Of Error}\label{sample-size-for-desired-margin-of-error}}

\begin{itemize}
\tightlist
\item
  The sample size (n) required to obtain a \((1-\alpha)\)\% confidence interval for a proportion with margin of error approximately equal to a specified value \emph{m} is
\end{itemize}

\[n=\left(\frac{z_{1-\alpha/2}}{m}\right)^2\pi^*(1-\pi^*)\]

where \(\pi^*\) is a guessed value for the true proportion and \(z_{1-\alpha/2}\) is the upper \(1-\alpha/2\) standard Normal critical value.

\begin{itemize}
\tightlist
\item
  If \(\pi^*\) is not known it can be set to 0.5 as this will maximize the sample size required. This gives
\end{itemize}

\[n=\left(\frac{z_{1-\alpha/2}}{2m}\right)^2\]

\hypertarget{sample-size-for-desired-power-and-type-i-error}{%
\subsection{Sample Size For Desired Power And Type I Error}\label{sample-size-for-desired-power-and-type-i-error}}

\begin{itemize}
\tightlist
\item
  The sample size (n) required to test the null hypothesis \(H_0:\pi=\pi_0\) vs.\\
  \(H_a:\pi\neq\pi_0\) with Type I error = \(\alpha\) and Type II error = \(\beta\) = 1- Power, such that the minimum detectable difference between the two groups is \(\pi_1-\pi_0\) is given by
\end{itemize}

\[n=\frac{[z_{1-\alpha/2}\sqrt{\pi_0(1-\pi_0)}+z_{1-\beta}\sqrt{\pi_1(1-\pi_1)}]^2}{(\pi_1-\pi_0)^2}\]

\begin{itemize}
\tightlist
\item
  Similar expressions can be defined for a one-sided test replacing \(Z_{1-α/2}\) by \(Z_{1-α}\)
\item
  Note once again that whereas the Type II error did not come into play when carrying out the hypothesis test, it is used here when planning the study
\end{itemize}

\hypertarget{example-5}{%
\subsection{Example}\label{example-5}}

Q. A study will be conducted to determine whether refugees entering Canada from Cambodia need to be routinely tested for \emph{Strongyloides} infection. Nothing is known about the prevalence of the disease in this population. Assuming a perfect test was available for the detection of \emph{Strongyloides}, what sample size would be required to detect the prevalence within a margin of error of m = 5\% with 90\% confidence?

A. Since nothing is known about the prevalence of \emph{Strongyloides} in this population, we use a guess value of \(\pi^*=0.5\). The required sample size is

\[n=\left(\frac{z_{1-\alpha/2}}{m}\right)^2\pi^*(1-\pi^*)=\left(\frac{1.645}{0.05}\right)^20.5(1-0.5)=271\space (approximately)\]

\hypertarget{planning-a-study-to-compare-proportions}{%
\subsection{Planning a study to compare proportions}\label{planning-a-study-to-compare-proportions}}

\begin{itemize}
\tightlist
\item
  Once again, we distinguish between the sample size required to estimate a confidence interval or to carry out a hypothesis test
\item
  Inputs required for the two approaches are summarized below
\end{itemize}

\begin{tabular}{l|l}
\hline
Confidence interval & Hypothesis test\\
\hline
Confidence level 1-α & Type I error α\\
\hline
 & Type II error β\\
\hline
Guess value for the two proportions \$π\_1\$ and \$π\_2\$ & Guess value for the two proportions \$π\_1\$ and \$π\_2\$\\
\hline
Desired precision (or half-width of interval or margin or error) (m) & The minimum important difference to detect \$(π\_1 - π\_2 = m)\$\\
\hline
\$k = n\_1/n\_2\$, the ratio of the two sample sizes & \$k = n\_1/n\_2\$, the ratio of the two sample sizes\\
\hline
\end{tabular}

\hypertarget{sample-size-formulae}{%
\subsection{Sample size formulae}\label{sample-size-formulae}}

\includegraphics[width=1\linewidth]{./7_65}

Notes

\begin{itemize}
\tightlist
\item
  In both cases, the Group 1 sample size is given by kn
\item
  In the expression for the hypothesis test, replace \(Z_{1-α/2}\) by \(Z_{1-α}\) for a one-sided test
\item
  You may find other variations of the expression for the hypothesis test, e.g.~those based on a pooled proportion
\end{itemize}

\hypertarget{example-6}{%
\subsection{Example}\label{example-6}}

\begin{itemize}
\tightlist
\item
  A cardiologist wishes to design a clinical trial comparing a new drug with a control in the treatment of the acute phase of myocardial infarction.
\item
  It is of interest to test the null hypothesis that there is no difference in 1-month survival following treatment with either drug. This will be tested with a two-sided test at the 5\% level.
\item
  If, in fact, the new drug increases the survival rate by 20\%, the investigator wishes to risk a 10 percent chance of failing to conclude that the new drug is significantly better than the control.
\item
  Based on previous experience the survival rate among controls was 60\%. How large a sample size is needed?
\end{itemize}

\hypertarget{inputs-for-the-sample-size-calculation}{%
\subsection{Inputs for the sample size calculation}\label{inputs-for-the-sample-size-calculation}}

\begin{tabular}{l|l}
\hline
Hypothesis test & Inputs\\
\hline
Type I error α & 5\%\\
\hline
Type II error β & 10\%\\
\hline
Guess value for the two proportions \$π\_1\$ and \$π\_2\$ & \$\textbackslash{}pi\_1=0.6\$ and \$\textbackslash{}pi\_2=0.8\$\\
\hline
The minimum important difference to detect \$(π\_1 - π\_2 = m)\$ & \$\textbackslash{}pi\_1-\textbackslash{}pi\_2=0.2\$\\
\hline
\$k = n\_1/n\_2\$, the ratio of the two sample sizes & k=1\\
\hline
\end{tabular}

\hypertarget{example-7}{%
\subsection{Example}\label{example-7}}

\begin{itemize}
\tightlist
\item
  The sample size required is given by
\end{itemize}

\[n=\left(\frac{Z_{1-\alpha/2}+Z_{1-\beta}}{\pi_1-\pi_2}\right)^2\left(\frac{\pi_1(1-\pi_1)}{k}+\pi_2(1-\pi_2)\right)\]
\[=\left(\frac{Z_{1-0.025}+Z_{1-0.1}}{0.6-0.8}\right)^2\left(\frac{0.6(1-0.6)}{1}+0.8(1-0.8)\right)\]
\[=105\space in\space each\space group\]

\hypertarget{lecture-8-other-statistics-for-comparing-proportions-and-methods-for-contingency-tables}{%
\chapter{Lecture 8: Other statistics for comparing proportions and methods for contingency tables}\label{lecture-8-other-statistics-for-comparing-proportions-and-methods-for-contingency-tables}}

\hypertarget{odds}{%
\section{Odds}\label{odds}}

\begin{itemize}
\tightlist
\item
  Another way of reporting a proportion (p) is in terms of the odds, which is the ratio of the probability of a `success' (p) to the probability of a `loss' (1-p)
\end{itemize}

\[Odds=\frac{p}{1-p}\]

\begin{itemize}
\tightlist
\item
  The odds can vary from 0 (when p=0) to ∞ (when p=1)
\item
  \textbf{Example:} A proportion of 0.2 corresponds to an odds of 0.2/0.8 = 0.25, while a proportion of 0.8 corresponds to an odds of 0.8/0.2 = 4
\end{itemize}

\hypertarget{odds-ratio-and-relative-risk}{%
\subsection{Odds Ratio and Relative Risk}\label{odds-ratio-and-relative-risk}}

\begin{itemize}
\tightlist
\item
  So far we have looked at how proportions may be compared in terms of the difference between them
\item
  The odds ratio and relative risk (or risk ratio) are also comparisons of proportions, but relying on their ratio
\item
  Both these statistics range from 0 to ∞, with a value of 1 meaning no difference in the proportions
\item
  Though they are mathematically more complicated, they are often reported, in part because they are obtained naturally from regression models
\end{itemize}

\includegraphics[width=1\linewidth]{./8_5}

\begin{itemize}
\tightlist
\item
  Observed odds ratio = \(\frac{ad}{bc}\)
\item
  Observed risk ratio = \(\frac{\frac{a}{a+c}}{\frac{b}{b+d}}\)
\item
  Note that if the risks \(\frac{a}{a+c}\) and \(\frac{b}{b+d}\) are small, then the odds ratio is approximately equal to the relative risk because a \textless\textless{} c and b \textless\textless{} d
\end{itemize}

\hypertarget{confidence-interval-for-an-odds-ratio}{%
\subsection{Confidence interval for an odds ratio}\label{confidence-interval-for-an-odds-ratio}}

\begin{itemize}
\tightlist
\item
  The distribution of the odds ratio is somewhat skew, so that the confidence interval is usually based on a normal distribution approximation to ln(odds ratio), where ln is the notation for the natural logarithm to the base e
\item
  Using Taylor's series expansion, we can show:
\end{itemize}

\[Variance(ln(odds\space ratio)) \cong \frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}\]

\begin{itemize}
\tightlist
\item
  So that a 95\% confidence interval for the ln(odds ratio) is given by
\end{itemize}

\[\left(ln(odds\space ratio)-1.96\sqrt{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}},ln(odds\space ratio)+1.96\sqrt{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}}\right)\]

\begin{itemize}
\tightlist
\item
  To convert this to an interval on the odds ratio scale, we have to take the exponent of both the lower and the upper limit
\end{itemize}

\[\left(e^{ln(odds\space ratio)-1.96\sqrt{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}}},e^{ln(odds\space ratio)+1.96\sqrt{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}}}\right)\]

\hypertarget{confidence-interval-for-the-relative-risk}{%
\subsection{Confidence interval for the relative risk}\label{confidence-interval-for-the-relative-risk}}

\begin{itemize}
\tightlist
\item
  Similarly, for a relative risk or risk ratio,
\end{itemize}

\[Variance(ln(relative\space risk))\cong\frac{c}{a(a+c)}+\frac{d}{b(b+d)}\]

\begin{itemize}
\tightlist
\item
  So that the approximate 95\% confidence interval for the relative risk is
\end{itemize}

\[\left(e^{ln(relative\space risk)-1.96\sqrt{\frac{c}{a(a+c)}+\frac{d}{b(b+d)}}},e^{ln(relative\space risk)+1.96\sqrt{\frac{c}{a(a+c)}+\frac{d}{b(b+d)}}}\right)\]

\hypertarget{probiotics-example-results-for-both-studies-1}{%
\subsection{Probiotics example: Results for both studies}\label{probiotics-example-results-for-both-studies-1}}

\includegraphics[width=1\linewidth]{./8_9}

\begin{itemize}
\tightlist
\item
  Notice that the Gao study resulted in the conclusion that the observed difference in proportions was statistically significant
\item
  Looking at just these two studies, it is clear that no study should be evaluated on its own
\item
  Ideally, we should analyze results from all available studies (i.e.~carry out a meta-analysis) before we can draw a conclusion. Even then, it would be important to consider the possibility of publication bias
\end{itemize}

\hypertarget{probiotics-and-cdad-rearranging-the-data}{%
\subsection{Probiotics and CDAD: Rearranging the data}\label{probiotics-and-cdad-rearranging-the-data}}

\includegraphics[width=1\linewidth]{./8_10}

\hypertarget{r-function-for-odds-ratio-and-relative-risk}{%
\subsection{R function for odds ratio and relative risk}\label{r-function-for-odds-ratio-and-relative-risk}}

\begin{verbatim}
rror=function(a,b,c,d) {
  
  rr = (a/(a+c))/(b/(b+d))
  or = (a*d)/(b*c)
  
  sd.rr = sqrt(c/(a*(a+c))+d/(b*(b+d)))
  sd.or = sqrt(1/a+1/b+1/c+1/d)
  
  or.ll=or*exp(-1.96*sd.or)
  or.ul=or*exp(1.96*sd.or)
  rr.ll=rr*exp(-1.96*sd.rr)
  rr.ul=rr*exp(1.96*sd.rr)
  
  
  return(list(or=or,rr=rr,sd.or=sd.or,sd.rr=sd.rr,or.ll=or.ll,or.ul=or.ul,rr.ll=rr.ll,rr.ul=rr.ul))
  
}
\end{verbatim}

\hypertarget{example-probiotics-and-cdad}{%
\subsection{Example: Probiotics and CDAD}\label{example-probiotics-and-cdad}}

\begin{itemize}
\tightlist
\item
  \textbf{Gao et al.}
\end{itemize}

\[odds\space ratio = (9×64)/(20×162)=0.18\]
\[risk\space ratio = (9/171)/(21/84)=0.22\]

the odds ratio is not a good approximation of the risk ratio

\begin{itemize}
\tightlist
\item
  \textbf{Allen et al.}
\end{itemize}

\[odds\space ratio = (21×1458)/(30×1473)=0.69\]
\[risk\space ratio =(21/1473)/(30/1458)=0.70\]

the odds ratio is an accurate approximation of the risk ratio

\hypertarget{example-95-confidence-intervals-for-gao-et-al.}{%
\subsection{Example: 95\% confidence intervals for Gao et al.}\label{example-95-confidence-intervals-for-gao-et-al.}}

\begin{itemize}
\tightlist
\item
  Odds Ratio

  \begin{itemize}
  \tightlist
  \item
    The standard deviation of the odds ratio can be calculated as \(\sqrt{\frac{1}{9}+\frac{1}{20}+\frac{1}{162}+\frac{1}{64}}=0.43\)
  \item
    A 95\% CI for the odds ratio is given by\\
    (0.16 × exp(-1.96×0.43) , 0.16 × exp(1.96×0.43))\\
    = (0.07, 0.38)
  \end{itemize}
\end{itemize}

\hypertarget{example-95-confidence-intervals-for-otitis-data}{%
\subsection{Example: 95\% confidence intervals for otitis data}\label{example-95-confidence-intervals-for-otitis-data}}

\begin{itemize}
\tightlist
\item
  Risk Ratio

  \begin{itemize}
  \tightlist
  \item
    The standard deviation of the risk ratio can be calculated as \(\sqrt{\frac{162}{9\times 171}+\frac{64}{20\times 84}}=0.38\)
  \item
    A 95\% CI for the risk ratio is given by\\
    (0.21 × exp(-1.96×0.38) , 0.21 × exp (1.96×0.38))\\
    = (0.08, 0.35)
  \end{itemize}
\end{itemize}

\hypertarget{example-probiotics-and-cdad-1}{%
\subsection{Example: Probiotics and CDAD}\label{example-probiotics-and-cdad-1}}

\begin{itemize}
\tightlist
\item
  The 95\% confidence intervals for the Allen et al.~study can be calculated similarly as
\end{itemize}

\[95\%\space CI\space for\space odds\space ratio = (0.40, 1.22)\]
\[95\%\space CI\space for\space risk\space ratio = (0.40, 1.20)\]

\begin{itemize}
\tightlist
\item
  Therefore, our conclusions remain similar to what we had before, namely that there the Gao study suggests that there is a statistically significant benefit of probiotics but the Allen study concludes the opposite
\end{itemize}

\hypertarget{risk-ratio-vs.-risk-difference}{%
\subsection{Risk ratio vs.~risk difference}\label{risk-ratio-vs.-risk-difference}}

\includegraphics[width=1\linewidth]{./8_16}

\begin{figure}
\includegraphics[width=1\linewidth]{./8_17} \caption{Hypothetical example of a study including 120 subjects: 60 in the group exposed to an environmental factor and 60 in the unexposed group. After 2 years of follow-up it was measured whether subjects had the outcome of interest (black) or did not have the outcome of interest (white).}\label{fig:unnamed-chunk-78}
\end{figure}

From: Relative risk versus absolute risk: one cannot be interpreted without the other\\
Nephrol Dial Transplant. 2017;32(suppl\_2):ii13-ii18. \url{doi:10.1093/ndt/gfw465}\\
Nephrol Dial Transplant \textbar{} © The Author 2017. Published by Oxford University Press on behalf of ERA-EDTA. All rights reserved.

\begin{itemize}
\tightlist
\item
  When interpreting a risk ratio it is important to keep the risk in the control group in mind
\item
  The risk ratio may exaggerate the importance of a result. Research studies are often reported in the form ``New treatment reduces risk of disease X by 75\%''
\item
  Whilst this sounds good, the benefit really depends on how common or rare the disease is. A large reduction of relative risk for a rare disease might not mean much reduction in the absolute risk
\item
  For example, a 75\% reduction in relative risk for something that has a 4 in a million absolute risk of happening brings the absolute risk down to 1 in a million
\item
  Therefore, the risk difference is often preferred to the relative risk when estimating the public health impact of an intervention
\end{itemize}

\hypertarget{methods-for-contingency-tables}{%
\section{Methods for contingency tables}\label{methods-for-contingency-tables}}

\hypertarget{comparing-two-or-more-proportions-the-generic-setup}{%
\subsection{Comparing Two or More Proportions: The generic setup}\label{comparing-two-or-more-proportions-the-generic-setup}}

\includegraphics[width=1\linewidth]{./8_20}

\hypertarget{examples-from-two-randomized-controlled-trials}{%
\subsection{Examples from two randomized controlled trials}\label{examples-from-two-randomized-controlled-trials}}

\includegraphics[width=1\linewidth]{./8_21}

\hypertarget{r-code-to-examine-an-r-c-table}{%
\subsection{R code to examine an r × c table}\label{r-code-to-examine-an-r-c-table}}

\begin{verbatim}
# recreating the acute otitis media data for each child
tmt=rep(c(“AMX”,”SUL”,”Placebo”),c(40,36,41))
aom=rep(c(“AOM”,”No AOM”,“AOM”,”No AOM” “AOM”,”No AOM”),c(12,28,19,17,28,13))

# frequency in each cell
result=table(aom,tmt)
print(result)

# percentage in each cell
result/117

# calculate row and column totals
margin.table(result,1)
margin.table(result,2)

# calculate row and column percentage
prop.table(result,1)
prop.table(result,2)
\end{verbatim}

\hypertarget{output-from-r}{%
\subsection{Output from R}\label{output-from-r}}

\begin{verbatim}
> # frequency in each cell
> result=table(aom,tmt)
> print(result)
   tmt
aom  1  2  3
  0 28 17 13
  1 12 19 28
 
> # percentage in each cell
> result/117
   tmt
aom         1         2         3
  0 0.2393162 0.1452991 0.1111111
  1 0.1025641 0.1623932 0.2393162

> # calculate row and column percentage

> prop.table(result,1)
   tmt
aom         1         2         3
  0 0.4827586 0.2931034 0.2241379
  1 0.2033898 0.3220339 0.4745763

> prop.table(result,2)
   tmt
aom         1         2         3
  0 0.7000000 0.4722222 0.3170732
  1 0.3000000 0.5277778 0.6829268
\end{verbatim}

\hypertarget{hypothesis-tests-to-compare-two-or-more-proportions}{%
\subsection{Hypothesis tests to compare two or more proportions}\label{hypothesis-tests-to-compare-two-or-more-proportions}}

\begin{itemize}
\tightlist
\item
  Suppose we wish to test the null hypothesis that
\end{itemize}

\[H_0:\pi_1=\pi_2=...=\pi_P,\]

that is, we have measured the frequency of occurrence of a dichotomous outcome in P populations, and wish to check if the frequencies are all equal.

\begin{itemize}
\tightlist
\item
  There are several candidate tests
\end{itemize}

\hypertarget{methods-to-compare-two-or-more-proportions}{%
\subsection{Methods to Compare Two or More Proportions}\label{methods-to-compare-two-or-more-proportions}}

\textbf{1. Normal approximation (Z) Test:} We have seen this test when P = 2. The test does not apply when P \textgreater{} 2. Alternative hypothesis can be one or two-sided.

\begin{itemize}
\tightlist
\item
  Requires large sample sizes to be accurate. ``Large'' is often stated as a criterion like: Sample size × min\{\(\pi,(1-\pi)\)\} \textgreater{} 5
\end{itemize}

\hypertarget{examples-from-two-randomized-controlled-trials-1}{%
\subsection{Examples from two randomized controlled trials}\label{examples-from-two-randomized-controlled-trials-1}}

\begin{itemize}
\tightlist
\item
  The C. difficile-associated diarrhea example can be handled using the Z-test as we have already seen.
\item
  However, it is not possible to directly extend these methods to the case when there are three (or more) outcome categories and/or three (or more) populations as in the acute otitis media example.
\item
  Furthermore, the Z-test is not suitable if the sample size is ``small''
\end{itemize}

\hypertarget{methods-to-compare-two-or-more-proportions-1}{%
\subsection{Methods to Compare Two or More Proportions}\label{methods-to-compare-two-or-more-proportions-1}}

\textbf{2. Chi-square (\(\chi^2\)) Test:} The \(\chi^2\) test does apply when P \textgreater{} 2, but the alternative hypothesis is always two-sided.

\begin{itemize}
\tightlist
\item
  It also requires large sample sizes to be accurate. ``Large'' is often operationalized as ``the \textbf{expected} number of subjects in each cell in the r × c table must be at least 5''.
\item
  We will see soon how to calculate these expected cell sizes.
\end{itemize}

\textbf{3. Fisher's Exact Test:} is an ``exact'' test suitable for small sample sizes.

\begin{itemize}
\tightlist
\item
  It also applies when P \textgreater{} 2, but unlike the \(\chi^2\) test, the alternative hypothesis can be one- or two-sided.
\item
  In contrast, both the \(\chi^2\) and Z-test require ``large'' sample sizes in order to be accurate since they are based on the normal approximation to the binomial distribution.
\end{itemize}

\hypertarget{which-method-do-we-use}{%
\subsection{Which method do we use?}\label{which-method-do-we-use}}

\begin{itemize}
\tightlist
\item
  While it is common to use a \(\chi^2\) test for large sample sizes and Fisher's Exact Test for smaller sample sizes, a natural question is ``Why not just use Fisher's Exact Test all the time, since it is always applicable?''
\item
  There are two possible answers. The first is that, as we will see, it is computationally expensive compared to the \(\chi^2\) test.
\item
  Second, there are different assumptions behind each. As will become clear from the examples on the next few pages, in the Fisher's Exact Test, all ``margins'' are held fixed (``conditioned upon''), while this is not the case for the Z and \(\chi^2\) tests. Thus there is a slightly different inferential philosophy behind each.
\end{itemize}

\hypertarget{hypothesis-test-for-comparing-two-or-more-proportions}{%
\subsection{Hypothesis test for comparing two or more proportions}\label{hypothesis-test-for-comparing-two-or-more-proportions}}

\begin{itemize}
\tightlist
\item
  Carrying out a hypothesis test involves the same steps as we had seen before:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses. The alternative is always two-sided if more than two proportions are involved
\item
  Calculate the test statistic measuring the evidence in favour of the null hypothesis
\item
  Determine the rejection region specified by the desired Type I error (α)
\item
  Determine if the test statistic falls in the rejection region. If yes, you conclude that you have enough evidence to reject the null hypothesis. If no, you conclude that you do not have enough evidence to reject the null hypothesis
\item
  Calculate a p-value
\end{enumerate}

\hypertarget{example-one-sample-chi2-test}{%
\subsection{\texorpdfstring{Example: One Sample \(\chi^2\) test}{Example: One Sample \textbackslash chi\^{}2 test}}\label{example-one-sample-chi2-test}}

\begin{itemize}
\tightlist
\item
  Let us first consider a chi-square test for a single proportion. Let us assume the null and alternative hypothesis of interest are
\end{itemize}

\[H_0:\pi=0.8\space vs\space H_a:\pi\neq0.8\]

\begin{itemize}
\tightlist
\item
  The observed data is summarized in the table below:
\end{itemize}

\begin{tabular}{l|r|r}
\hline
  &  Success &  Failure\\
\hline
Population & 60 & 40\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Procedure: If the null hypothesis were true and then the split of successes and failures in this same sample of 100 subjects would be expected to be as follows:
\end{itemize}

\begin{tabular}{l|r|r}
\hline
  &  Success &  Failure\\
\hline
Population & 80 & 20\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Observed discrepancies from these expected values are evidence against the null hypothesis. We calculate:
\end{itemize}

\[\chi^2=\sum_{all\space cells}\frac{(Observed-Expected)^2}{Expected}\]
\[=\frac{(60-80)^2}{80}+\frac{(40-20)^2}{20}=\frac{400}{80}+\frac{400}{20}=25\]

\begin{itemize}
\tightlist
\item
  The rejection region is the region beyond the \(Q_{1-α}\) quantile of the \(\chi^2\) distribution with 1 degree of freedom (1 df). This can be obtained using the qchisq() function in R
\end{itemize}

\begin{verbatim}
> qchisq(0.95,1)
[1] 3.841459
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Because 25 exceeds 3.841459 we can reject the null hypothesis
\item
  The reason we the number of degrees of freedom is 1 is because we once we calculate the expected value in 1 cell, the expected value in the remaining (1) cell is known
\item
  Note that if the same hypothesis had been tested using a Z-test, the z-statistic would have been 5 or the square-root of the chi-squared statistic.
\end{itemize}

\hypertarget{calculating-the-p-value-under-a-chi-square-distribution}{%
\subsection{Calculating the p-value under a chi-square distribution}\label{calculating-the-p-value-under-a-chi-square-distribution}}

\includegraphics[width=0.5\linewidth]{./8_36}

\begin{itemize}
\tightlist
\item
  As the graph illustrates, the p-value is the probability of being greater than the observed test statistic
\item
  For our example the p-value may be obtained as follows:
\end{itemize}

\begin{verbatim}
> 1-pchisq(25,1)
[1] 5.733031e-07
\end{verbatim}

\hypertarget{example-two-sample-chi2-test}{%
\subsection{\texorpdfstring{Example: Two sample \(\chi^2\) Test}{Example: Two sample \textbackslash chi\^{}2 Test}}\label{example-two-sample-chi2-test}}

\includegraphics[width=1\linewidth]{./8_37}

\begin{itemize}
\tightlist
\item
  We would like to test the hypothesis \(H_0:\pi_1=\pi_2\); that is, the proportion of CDAD cases is the same in the probiotics and placebo groups.
\item
  Procedure: Since we hypothesize \(\pi_1=\pi_2\), we expect to observe the following data, on average. The expected number of observations at the intersection of the \(r^{th}\) row and \(c^{th}\) column is given by \(\frac{N_{.r}\times N_{.c}}{N}\)
\end{itemize}

\includegraphics[width=1\linewidth]{./8_38}

\begin{itemize}
\tightlist
\item
  Once again, the observed discrepancies from the expected values are evidence against the null hypothesis. We calculate:
\end{itemize}

\[\chi^2=\sum_{all\space cells}\frac{(Observed-Expected)^2}{Expected}\]
\[=\frac{(21-25.55)^2}{25.55}+\frac{(30-25.45)^2}{25.45}+\frac{(1473-1468.45)^2}{1468.45}+\frac{(1458-1462.55)^2}{1462.55}\cong1.65\]

\begin{itemize}
\tightlist
\item
  Notice that once again, fixing the expected value in 1 cell determined the expected value in the remaining cells. Therefore the number of degrees of freedom in the problem is 1 and the relevant rejection region is the region beyond the \(Q_{1-α}\) quantile of the \(\chi^2\) distribution with 1 degree of freedom (1 df).\\
\item
  Because 1.65 is less than qchisq(0.95,1) = 3.841459 we cannot reject the null hypothesis
\end{itemize}

\hypertarget{example-chi2-test-for-the-2-x-3-table}{%
\subsection{\texorpdfstring{Example: \(\chi^2\) test for the 2 x 3 Table}{Example: \textbackslash chi\^{}2 test for the 2 x 3 Table}}\label{example-chi2-test-for-the-2-x-3-table}}

\begin{itemize}
\tightlist
\item
  Infants judged to be at high risk of acute otitis media (AOM) were randomized to one of three treatment groups a) amoxicillin (AMX), b) sulfisoxazole (SUL), or c) placebo, for a 6-month period (even in the absence of symptoms)
\item
  The results from this study were summarized earlier in the lecture
\item
  We would like to test the hypothesis \(H_0:\pi_1=\pi_2=\pi_3\); that is, the proportions of infants who were AOM free at 6 months is the same irrespective of the treatment received. The alternative hypothesis is that the three proportions are not equal
\end{itemize}

\begin{table}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
\multicolumn{5}{c}{Observed Data} \\
\cline{1-5}
  & AMX & SUL & Placebo & Total\\
\hline
AOM during 6 mths of treatment & 12 & 19 & 28 & 59\\
\hline
AOM free & 28 & 17 & 13 & 58\\
\hline
Total & 40 & 36 & 41 & 117\\
\hline
\end{tabular}
\end{table}

\begin{table}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
\multicolumn{5}{c}{Expected Data} \\
\cline{1-5}
  & AMX & SUL & Placebo & Total\\
\hline
AOM during 6 mths of treatment & 20.17 & 18.15 & 20.681 & 59\\
\hline
AOM free & 19.83 & 17.85 & 20.320 & 58\\
\hline
Total & 40.00 & 36.00 & 41.000 & 117\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  Once again, the observed discrepancies from the expected values are evidence against the null hypothesis. We calculate:
\end{itemize}

\[\chi^2=\sum_{all\space cells}\frac{(Observed-Expected)^2}{Expected}\]
\[=\frac{(12-20.17)^2}{20.17}+\frac{(28-19.83)^2}{19.83}+\frac{(19-18.15)^2}{18.15}\]
\[+\frac{(17-17.85)^2}{17.85}+\frac{(19-18.15)^2}{18.15}+\frac{(17-17.85)^2}{17.85}\]
\[\cong11.991\]

\begin{itemize}
\tightlist
\item
  For tables where r\textgreater1 and c\textgreater1, we determine the number of degrees of freedom (df) using df = (r-1) × (c-1). For our example, df = (2-1) × (3-1) = 2
\item
  Assuming the desired Type I error is 0.05, the rejection region is the region above
\end{itemize}

\begin{verbatim}
> qchisq(0.95,2)
[1] 5.991465
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the test statistic lies above this value, we have enough evidence to reject the null hypothesis of no difference in the probability of being AOM free across the 3 treatment groups
\item
  Alternatively, we could report the p-value to support the same conclusion
\end{itemize}

\begin{verbatim}
> 1-pchisq(11.991,2)
[1] 0.002489932 
\end{verbatim}

\hypertarget{some-more-notes-on-the-chi-square-test}{%
\subsection{Some more notes on the chi-square test}\label{some-more-notes-on-the-chi-square-test}}

\begin{itemize}
\tightlist
\item
  We saw examples involving 2 × 1, 2 × 2 and 2 × 3 tables. But the method applies more generally to r × c tables.
\item
  Generally speaking, the null and alternative hypotheses may be stated as\\
  \(H_0:\) There is no association between X and Y\\
  \(H_A:\) There is an association between X and Y
\end{itemize}

where X and Y are the two categorical variables whose cross-tabulation defines the r × c table

\hypertarget{chi-square-test-in-r}{%
\subsection{Chi-square test in R}\label{chi-square-test-in-r}}

\begin{itemize}
\tightlist
\item
  Tests involving one or two proportions can be carried out using the function prop.test()
\item
  In order to carry out a chi-square test for ≥2 proportions we can use the chisq.test() function. The observed data must be supplied in the form of a matrix
\item
  On the following slides you will find the R code for carrying out each of the three examples on the chi-square test
\end{itemize}

\hypertarget{chi-square-test-for-a-single-proportion}{%
\subsection{Chi-square test for a single proportion}\label{chi-square-test-for-a-single-proportion}}

\begin{verbatim}
> prop.test(60,100,p=0.8,correct=F)

    1-sample proportions test without continuity correction

data:  60 out of 100, null probability 0.8
X-squared = 25, df = 1, p-value = 5.733e-07
alternative hypothesis: true p is not equal to 0.8
95 percent confidence interval:
 0.5020026 0.6905987
sample estimates:
  p 
0.6 
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Notice that the arguments to prop.test() include the value under the null hypothesis and also correct=F to avoid the continuity correction
\item
  As seen in the previous lecture and also in the R output on the right, the chi-squared statistic is the square of the Z-statistic. The p-value is identical
\end{itemize}

\hypertarget{two-sample-chi2-test-in-r}{%
\subsection{\texorpdfstring{Two sample \(\chi^2\) Test in R}{Two sample \textbackslash chi\^{}2 Test in R}}\label{two-sample-chi2-test-in-r}}

\begin{verbatim}
a = matrix(c(21,1473,30,1458),
+            byrow=T,nrow=2)
> chisq.test(a,correct=F)

    Pearson's Chi-squared test

data:  a
X-squared = 1.6529, df = 1, p-value = 0.1986
 
> sqrt(1.6529)
[1] 1.285652
\end{verbatim}

Notice that the data is supplied to the chisq.test() function in the form of a matrix a

I provided the argument correct=F so the continuity correction is not applied, and we could compare the results with those obtained by hand

\hypertarget{three-sample-chi2-test-in-r}{%
\subsection{\texorpdfstring{Three sample \(\chi^2\) Test in R}{Three sample \textbackslash chi\^{}2 Test in R}}\label{three-sample-chi2-test-in-r}}

\begin{verbatim}
> a=matrix(c(12,19,28,28,17,13),2,3,
                         byrow=T)
> result=chisq.test(a,correct=F)
> print(result)

    Pearson's Chi-squared test

data:  a
X-squared = 11.991, df = 2, p-value = 0.00249

> names(result)
[1] "statistic" "parameter" "p.value"   "method"    "data.name" "observed"  "expected"  "residuals" "stdres" 
\end{verbatim}

For this example, I defined an object called result which has the output of the chisq.test() function

By applying the names() function to this object, we can see that the chisq.test() function provides more than the output we have been looking at so far

Among other details, it provides the expected values in each cell of the contingency table

\hypertarget{example-ecmo}{%
\subsection{\texorpdfstring{Example: ECMO\(^*\)}{Example: ECMO\^{}*}}\label{example-ecmo}}

\begin{itemize}
\tightlist
\item
  Extracorporeal membrane oxygenation (ECMO) is a potentially life-saving procedure that is used to treat newborn babies who suffer from severe respiratory failure
\item
  Data from a randomized controlled trial evaluating ECMO are shown in the following table
\end{itemize}

\begin{table}
\centering
\begin{tabular}{l|r|r|r}
\hline
\multicolumn{4}{c}{Observed Data} \\
\cline{1-4}
  & Died & Lived & Total\\
\hline
CMT & 4 & 6 & 10\\
\hline
ECMO & 1 & 28 & 29\\
\hline
Total & 5 & 34 & 39\\
\hline
\end{tabular}
\end{table}

\(^*\)Text by Samuels et al., Chapter 10

\hypertarget{example-fishers-exact-test}{%
\subsection{Example: Fisher's Exact Test}\label{example-fishers-exact-test}}

\begin{itemize}
\tightlist
\item
  As with the Z and chi-square tests, we would like to test the null hypothesis \(H_0:\pi_{EMO}=\pi_{CMT}\), where \(\pi\) denotes the risk of mortality
\item
  However, since the sample size is so small, there is doubt about the applicability of these tests to this data set. In fact as the following table reveals the expected number of subjects in some cells is less than 5
\end{itemize}

\begin{table}
\centering
\begin{tabular}{l|r|r|r}
\hline
\multicolumn{4}{c}{Expected Data} \\
\cline{1-4}
  & Died & Lived & Total\\
\hline
CMT & 1.28 & 8.72 & 10\\
\hline
ECMO & 3.72 & 25.28 & 29\\
\hline
Total & 5.00 & 34.00 & 39\\
\hline
\end{tabular}
\end{table}

\hypertarget{fishers-exact-test}{%
\subsection{Fisher's exact test}\label{fishers-exact-test}}

\begin{itemize}
\tightlist
\item
  An ``exact'' test can be constructed via the following reasoning: We have observed a total of 5 failures. If groups CMT and ECMO receive equally effective treatments, then the five failures should be equally distributed between the two groups.
\item
  If the sample sizes were equal, we would expect 2.5 failures in each group, but since the CMT group is much smaller, we expect the failures to be divided in a roughly 1:3 ratio.
\item
  As in the previous tests, discrepancies from this ``fair split'' indicate departures from the null hypothesis.
\end{itemize}

\hypertarget{example-fishers-exact-test-1}{%
\subsection{Example: Fisher's exact test}\label{example-fishers-exact-test-1}}

\begin{itemize}
\tightlist
\item
  We will first enumerate all possible tables that could have been observed and calculate the probability of each split
\end{itemize}

\includegraphics[width=1\linewidth]{./8_52}

\begin{itemize}
\tightlist
\item
  The probability of each split is given by an expression that we will see shortly
\item
  The sum of these probabilities is 1
\item
  The calculation of the p-value depends on whether the alternative is one- or two-sided.
\item
  If the alternative is one-sided, then the p-value is obtained by considering the observed table and the tables that are more extreme in the direction supporting the alternative.
\item
  If the alternative is two-sided, then the p-value can be obtained by considering the observed table and all tables that are associated with a smaller probability.
\item
  See the following slide for an illustration for our particular example.
\end{itemize}

\hypertarget{calculation-of-p-value-for-fishers-exact-test}{%
\subsection{Calculation of p-value for Fisher's exact test}\label{calculation-of-p-value-for-fishers-exact-test}}

\includegraphics[width=1\linewidth]{./8_54}

\hypertarget{fishers-exact-test-1}{%
\subsection{Fisher's exact test}\label{fishers-exact-test-1}}

\begin{itemize}
\tightlist
\item
  The probability of each possible table that can be observed while keeping the margins fixed, is calculated using the hypergeometric distribution
\item
  In general, if we observe the following table then its probability is given by the expression on the next slide
\end{itemize}

\begin{tabular}{l|l|l|l}
\hline
  & Column 1 & Column 2 & Total\\
\hline
Row 1 & a & b & a+b\\
\hline
Row 2 & c & d & c+d\\
\hline
Total & a+c & b+d & N=a+b+c+d\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Probability of each table
\end{itemize}

\[=\frac{(a+b)!(b+d)!(a+c)!(c+d)!}{N!a!b!c!d!}\]

\begin{itemize}
\tightlist
\item
  Notice that unlike the chi-square test, the value in each cell and the fixed margins enter the calculations, illustrating the requirement of the Fisher's exact test that the marginal totals should be fixed
\item
  Another, equivalent but more complex expression, gives a clue as to how the probability is obtained:
\end{itemize}

\[\frac{\frac{(a+c)!}{a!c!}\frac{(b+d)!}{b!d!}}{\frac{N!}{(a+c)!(b+d)!}}\]

\begin{itemize}
\tightlist
\item
  It is the probability of selecting a failures among a+c possible failures for Group 1 and b successes among b+d possible successes for Group 1 , while selecting (a+c) failures among a sample size of N
\end{itemize}

\hypertarget{carrying-out-fishers-exact-test-in-r---two-sided-alternative}{%
\subsection{Carrying out Fisher's exact test in R - two-sided alternative}\label{carrying-out-fishers-exact-test-in-r---two-sided-alternative}}

\begin{verbatim}
> a=matrix(c(4,1,6,28),2,2,byrow=T)
> fisher.test(a)

    Fisher's Exact Test for Count Data

data:  a
p-value = 0.01102
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
   1.366318 944.080411
sample estimates:
odds ratio 
  16.78571 
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Once again we provide the data as a matrix
\item
  The default is the two-sided alternative
\item
  Also, note there is no need to provide an argument for a continuity correction as this is an exact test
\end{itemize}

\hypertarget{carrying-out-fishers-exact-test-in-r---one--sided-alternative}{%
\subsection{Carrying out Fisher's exact test in R - one -sided alternative}\label{carrying-out-fishers-exact-test-in-r---one--sided-alternative}}

\begin{itemize}
\tightlist
\item
  Unlike for the chi-square test, here we can specify as an argument whether the test is one or two-sided
\item
  The results on the following slide provide the results from R in the two cases where the alternative favors i) CMT or ii) ECMO for our example
\end{itemize}

\hypertarget{alternative-cmt-has-higher-risk-of-dying}{%
\subsubsection{Alternative: CMT has higher risk of dying}\label{alternative-cmt-has-higher-risk-of-dying}}

\begin{verbatim}
> fisher.test(a,alternative="greater")

    Fisher's Exact Test for Count Data

data:  a
p-value = 0.01102
alternative hypothesis: true odds ratio is greater than 1
95 percent confidence interval:
 1.833681      Inf
sample estimates:
odds ratio 
  16.78571 
\end{verbatim}

\hypertarget{alternative-ecmo-has-higher-risk-of-dying}{%
\subsubsection{Alternative: ECMO has higher risk of dying}\label{alternative-ecmo-has-higher-risk-of-dying}}

\begin{verbatim}
> fisher.test(a,alternative="less")

    Fisher's Exact Test for Count Data

data:  a
p-value = 0.9996
alternative hypothesis: true odds ratio is less than 1
95 percent confidence interval:
   0.0000 467.3486
sample estimates:
odds ratio 
  16.78571 
\end{verbatim}

\hypertarget{lecture-9-non-parametric-methods}{%
\chapter{Lecture 9: Non-parametric methods}\label{lecture-9-non-parametric-methods}}

\hypertarget{parametric-inference}{%
\section{Parametric Inference}\label{parametric-inference}}

\begin{itemize}
\tightlist
\item
  Thus far, we studied methods for statistical inference that relied on assuming a distributional form that links observed data to the unknown parameters. We then estimated parameters of that distribution (e.g.~mean or standard deviation) based on a sample
\item
  For example, we assumed that continuous data may be normally distributed or that the mean of the variable may be normally distributed based on the central limit theorem. For dichotomous data, we assume a Bernoulli or binomial distribution instead.
\item
  Once the parameters have been estimated (for example, the mean and/or variance for a Normal distribution), the distribution is fully specified. This is known as parametric inference.
\end{itemize}

\hypertarget{examples}{%
\subsection{\texorpdfstring{Examples\(^*\)}{Examples\^{}*}}\label{examples}}

\begin{itemize}
\tightlist
\item
  \textbf{Visual acuity} can be measured on an ordinal scale. We know 20-20 vision is better than 20-30, which is better than 20-40 vision and so on. However, a numeric value cannot easily be assigned to each level of visual acuity that all ophthalmologists would agree on
\item
  Computing means and standard deviations for such data is therefore not meaningful, and parametric methods for hypothesis testing cannot be used
\item
  A \textbf{patient's condition after treatment} may be measured on a 5-point scale:

  \begin{itemize}
  \tightlist
  \item
    1=much improved
  \item
    2=slightly improved
  \item
    3=stays the same
  \item
    4=slightly worse
  \item
    5=much worse
  \end{itemize}
\item
  We cannot say whether the difference between 1 and 2 on this scale is the same as the difference between 2 and 3
\end{itemize}

\hypertarget{non-parametric-inference}{%
\subsection{Non-Parametric Inference}\label{non-parametric-inference}}

\begin{itemize}
\tightlist
\item
  Sometimes we may be unwilling to specify in advance the general shape of the distribution, and prefer to base the inference on the data, without a parametric model. In this case, we use \emph{distribution free}, or \emph{nonparametric} methods.
\item
  These methods are used typically when we have a very small sample size and the central limit cannot be relied on
\item
  We will study 3 hypothesis testing methods in this lecture

  \begin{itemize}
  \tightlist
  \item
    The \textbf{sign test} for comparing paired samples
  \item
    The \textbf{Wilcoxon signed rank} test for comparing paired samples
  \item
    The \textbf{Wilcoxon rank sum} test for comparing samples that are not paired
  \end{itemize}
\item
  The same data will be used to illustrate all 3 tests
\end{itemize}

\hypertarget{hypothesis-testing-procedure}{%
\subsection{Hypothesis testing procedure}\label{hypothesis-testing-procedure}}

\begin{itemize}
\tightlist
\item
  The procedure for non-parametric hypothesis testing is similar to that for parametric tests
\end{itemize}

\includegraphics[width=1\linewidth]{./9_7}

\hypertarget{example-comparison-of-two-matched-samples}{%
\subsection{Example: Comparison of Two Matched Samples}\label{example-comparison-of-two-matched-samples}}

\begin{itemize}
\tightlist
\item
  \textbf{Example:} Suppose that a new post-surgical treatment is being compared with a standard treatment by observing the recovery times of n treatment subjects and m controls. Suppose the data are matched m=n=9 and that the observed recovery times (in days) are:
\end{itemize}

\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
Pair &  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9\\
\hline
Control & 20 & 21 & 24 & 30 & 32 & 36 & 40 & 48 & 54\\
\hline
Treatment & 19 & 22 & 25 & 26 & 28 & 29 & 34 & 37 & 38\\
\hline
Sign (Treatment < Control) & + & - & - & + & + & + & + & + & +\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Thus 7 out of 9, or 78\%, of recovery times were better (i.e.~lower) in the treatment group. Is this likely to be due to chance, or is it ``statistically significant''?
\end{itemize}

\hypertarget{example-8}{%
\subsection{Example}\label{example-8}}

\begin{verbatim}
>x=c(20,21,24,30,32,36,40,48,54)
>y=c(19,22,25,26,28,29,34,37,38)
>t.test(x,y,paired=T)

    Paired t-test

data:  x and y
t = 2.7939, df = 8, p-value = 0.02342
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 0.9118896 9.5325549
sample estimates:
mean of the differences 
               5.222222 
\end{verbatim}

\includegraphics[width=0.5\linewidth]{./9_9}

\begin{itemize}
\tightlist
\item
  The distribution of the recovery time is not known and the sample is small
\item
  A two-sided paired t-test gives us a p-value of 0.023, leading to the conclusion that there is sufficient evidence to reject the null hypothesis of no difference in the mean recovery times in the two groups
\item
  Can we be confident of these results given the assumptions of the t-test are not really met?
\end{itemize}

\hypertarget{comparison-of-two-matched-samples-the-sign-test}{%
\subsection{Comparison of Two Matched Samples: The Sign Test}\label{comparison-of-two-matched-samples-the-sign-test}}

\begin{itemize}
\tightlist
\item
  Let m1 and m2 denote the median of the continuous variable being compared in the control group and in the treatment group, respectively. Let P(+) and P(-) denote the number of positive and negative signs
\item
  \textbf{Null and alternative hypotheses} can be expressed either in terms of the median or in terms of the number of positive signs:
\end{itemize}

\includegraphics[width=1\linewidth]{./9_11}

\hypertarget{the-sign-test-the-test-statistic}{%
\subsection{The Sign Test: The test statistic}\label{the-sign-test-the-test-statistic}}

\begin{itemize}
\tightlist
\item
  It is simply the number of positive signs
\item
  Reasoning: If the two procedures are truly equivalent then we would expect roughly equal numbers of +'s and --'s in the sample
\end{itemize}

\hypertarget{the-sign-test-calculating-the-p-value}{%
\subsection{The Sign Test: Calculating the p-value}\label{the-sign-test-calculating-the-p-value}}

\begin{itemize}
\tightlist
\item
  Define the desired Type I error of α
\item
  Notice that the `number of positive signs' out of a fixed number of N pairs can be considered like the `number of successes' out of N trials in a Binomial experiment.
\item
  The p-value is simply the probability of being equal to or more extreme than the test-statistic. Once again, its value will depend on whether the alternative hypothesis is one- or two-sided, as we will see in the example
\end{itemize}

\hypertarget{example-9}{%
\subsection{Example}\label{example-9}}

\begin{itemize}
\tightlist
\item
  The value of the test statistic is \(n_+ = 7\)
\item
  The p-value corresponding to each alternative hypothesis appears in the table below. It is calculated using the pbinom function for the cumulative probability under the Binomial distribution
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_14}

\begin{itemize}
\tightlist
\item
  In this particular example, using the two-sided sign test would lead us to conclude that there isn't enough evidence to reject the null hypothesis that the median recovery time is the same in both groups
\item
  In this case the sign test was more conservative than the t-test
\end{itemize}

\hypertarget{carrying-out-the-sign-test-in-r}{%
\subsection{Carrying out the Sign Test in R}\label{carrying-out-the-sign-test-in-r}}

\begin{verbatim}
> binom.test(7,9,alternative="greater")

    Exact binomial test

data:  7 and 9
number of successes = 7, number of trials = 9, p-value = 0.08984
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
 0.4503584 1.0000000
sample estimates:
probability of success 
             0.7777778 
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the sign test is nothing but the test for a single proportion based on the Binomial distribution, we can use the binom.test() function in R
\item
  The example above corresponds to a one-sided test
\end{itemize}

\hypertarget{are-non-parameteric-methods-really-distribution-free}{%
\subsection{Are non-parameteric methods really ``distribution free''?}\label{are-non-parameteric-methods-really-distribution-free}}

\begin{itemize}
\tightlist
\item
  They are in the sense that we make no assumptions concerning the \textbf{underlying distribution from which the data arise}.
\item
  However, note that we still used a probability distribution in carrying out the sign test. We used the Binomial distribution from which we calculated the p-value
\item
  The t-test requires the assumption that the data arise from a Normal distribution or that the sample size is large, while the sign test does not require these assumptions. * The sign test made no assumptions about an underlying population, or the shape of any distribution. In using the sign test, we also did not need to consider degrees of freedom, or whether we had equal variances or not
\end{itemize}

\hypertarget{comparison-of-two-matched-samples-the-wilcoxon-signed-rank-test}{%
\subsection{Comparison of Two Matched Samples: The Wilcoxon Signed Rank Test}\label{comparison-of-two-matched-samples-the-wilcoxon-signed-rank-test}}

\begin{itemize}
\tightlist
\item
  The sign test is very wasteful of information since it assigns each pair only a ``+'' or ``-'', regardless of the magnitude of the difference.
\item
  We can take the differences into account by using a \textbf{Wilcoxon Signed Rank Test}. The statement of the null and alternative hypotheses remains the same as for the Sign test.
\item
  However, the calculation of the test statistic is different (see following slide)

  \begin{itemize}
  \tightlist
  \item
    Place the \textbf{absolute} differences in recovery times between the two groups in ascending order of magnitude
  \item
    Rank the differences. Calculate the average rank in case of ties
  \item
    Calculate the total of the ranks for the pairs with a + sign
  \end{itemize}
\end{itemize}

\hypertarget{example-wilcoxon-signed-rank-test}{%
\subsection{Example: Wilcoxon Signed Rank Test}\label{example-wilcoxon-signed-rank-test}}

\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
Pair &  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9\\
\hline
Control & 20 & 21 & 24 & 30 & 32 & 36 & 40 & 48 & 54\\
\hline
Treatment & 19 & 22 & 25 & 26 & 28 & 29 & 34 & 37 & 38\\
\hline
Absolute difference in Recovery time & 1 & 1 & 1 & 4 & 4 & 7 & 6 & 11 & 16\\
\hline
Rank of the absolute difference & 1 & 2 & 3 & 4 & 5 & 7 & 6 & 8 & 9\\
\hline
Adjusted rank & 2 & 2 & 2 & 4.5 & 4.5 & 7 & 6 & 8 & 9\\
\hline
Sign (Treatment < Control) & + & - & - & + & + & + & + & + & +\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  The test statistic is given by \(T_+ = 2+4.5+4.5+7+6+8+9 = 41\)
\item
  Summing up the ranks corresponding to the ``+'' signs we have \(T_+ = 41\).\\
  Summing up the ranks corresponding to the ``-'' signs we have \(T_- = 4\).
\item
  If there is no difference in the two groups, then we would expect \(T_+\) and \(T_-\) to be approximately equal to each other, so that unequal values indicate departures from the null hypothesis.
\item
  Is the observed difference ``significant''?
\item
  To answer this question we compare the test statistic to a rejection region defined by the distribution of the signed ranks
\item
  As with the other probability distributions we have studied so far, we can use R to obtain the quantiles of the signed ranks. The relevant function is qsignrank()
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_21}

\begin{itemize}
\tightlist
\item
  Recall that the value for the test statistic is \(T_+ = 41\)
\item
  The conclusion when using each of the three alternative hypotheses in the context of our example appears in the table below
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_22}

\begin{itemize}
\tightlist
\item
  The p-value corresponding to each alternative hypothesis appears in the table below. It is calculated using the psignrank() function for the cumulative probability under the Wilcoxon signed rank distribution.
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_23}

\begin{itemize}
\tightlist
\item
  Note that the expressions for calculations of the p-value on the previous slide were suitable for the situation where the test-statistic is above the mean value of the test statistic of n(n+1)/4
\item
  In our example, \(T_+ = 41\) was above the expected value of n(n+1)/4 = 90/4 = 22.5
\item
  Alternatively, if \(T_+\) turns out to be less than the mean, we can work with \(T_- = n(n+1)/2 - T_+\) and apply the expressions on the previous slide.
\item
  For example, if \(T_+ = 15\) in a given problem with n=9, then we can calculate \(T_- = n(n+1)/2 - T_+ = 45 - 15 = 30\) and calculate the p-value using the expressions on the previous slide
\end{itemize}

\hypertarget{wilcoxon-signed-rank-test-in-r}{%
\subsection{Wilcoxon Signed Rank Test in R}\label{wilcoxon-signed-rank-test-in-r}}

\begin{verbatim}
x=c(20,21,24,30,32,36,40,48,54)
y=c(19,22,25,26,28,29,34,37,38)
wilcox.test(x,y,paired=T,correct=F, alternative="greater")

    Wilcoxon signed rank test

data:  x and y
V = 41, p-value = 0.01386
alternative hypothesis: true location shift is greater than 0

Warning message:
In wilcox.test.default(x, y, paired = T, correct = F, alternative = "greater") :
  cannot compute exact p-value with ties
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The Wilcoxon Signed Rank test can be carried out using the wilcox.test() function
\item
  Notice, the arguments mentioning it is a paired test, that there is no continuity correction and that it is one-sided
\item
  The warning tells us that this function cannot compute exact p-values in the presence of ties. The p-value that we calculated on the previous slide is exact, whereas the one from R is based on a normal approximation (which we will cover next)
\item
  The continuity correction leads to a higher p-value of 0.0161, which is more conservative but not necessarily more precise
\end{itemize}

\hypertarget{normal-approximation-to-the-sign-and-wilcoxon-signed-rank-tests}{%
\subsection{Normal approximation to the Sign and Wilcoxon Signed Rank Tests}\label{normal-approximation-to-the-sign-and-wilcoxon-signed-rank-tests}}

\begin{itemize}
\tightlist
\item
  If the sample size is large enough, we can use a normal approximation for both the sign and the signed rank tests. A continuity correction can be used to improve the approximation
\item
  The approximate sign test is the same as the test for a single proportion based on the normal approximation to the Binomial distribution
\item
  For the Wilcoxon signed rank test we can show that under the null hypothesis of ``no difference'',\\
  The expectation \(E(T_+)=\frac{n(n+1)}{4}=\frac{9\times 10}{4}=22.5\)\\
  and \(var(T_+)=\frac{n(n+1)(2n+1)}{24}=\frac{9\times 10\times 19}{24}=71.25\)
\end{itemize}

\hypertarget{normal-approximation-to-the-wilcoxon-signed-rank-test}{%
\subsection{Normal approximation to the Wilcoxon Signed Rank Test}\label{normal-approximation-to-the-wilcoxon-signed-rank-test}}

\begin{itemize}
\tightlist
\item
  Using the information about the expected value and the variance we can calculate a Z-statistic that follows a standard normal distribution
\item
  Applying a continuity correction, calculate
\end{itemize}

\[z=\frac{T_+-E(T_+)}{sd(T_+)}=\frac{40.5-22.5}{\sqrt{71.25}}=2.13\]

\begin{itemize}
\tightlist
\item
  From the Normal tables, the one-sided p-value is, \emph{p}=P(Z\textgreater2.13)=0.0164, which would lead to a similar conclusion as the exact test
\item
  Notice that this result is similar enough but not identical to the calculation by R owing perhaps to differences in the way tied ranks are calculated
\end{itemize}

\hypertarget{comparison-of-two-unmatched-samples}{%
\subsection{Comparison of two unmatched samples}\label{comparison-of-two-unmatched-samples}}

\begin{itemize}
\tightlist
\item
  Suppose that the data in the previous example were not paired, but instead came from two independent samples
\item
  We would then use the \textbf{Wilcoxon Rank Sum} test
\item
  The hypotheses of interest would be stated in the same as for the sign test and signed rank test, but the construction of the test statistic is slightly different
\item
  This test is also known as the Mann-Whitney test
\end{itemize}

\hypertarget{wilcoxon-rank-sum-test}{%
\subsection{Wilcoxon Rank Sum Test}\label{wilcoxon-rank-sum-test}}

\begin{itemize}
\tightlist
\item
  As shown on the following slide calculation of the test statistic involves the following steps

  \begin{itemize}
  \tightlist
  \item
    First, order the recovery times in the two groups without concern for the group to which an observation belongs
  \item
    In the case of ties, you may need to calculate an adjusted rank by taking the average of the rank for the tied observations
  \item
    Then, sum up the ranks in one of the groups
  \end{itemize}
\end{itemize}

\hypertarget{wilcoxon-rank-sum-test-calculation-of-the-test-statistic}{%
\subsection{Wilcoxon Rank Sum Test: Calculation of the test statistic}\label{wilcoxon-rank-sum-test-calculation-of-the-test-statistic}}

\includegraphics[width=1\linewidth]{./9_30}
\(^*\)Recovery Time

\begin{itemize}
\tightlist
\item
  T denotes Treatment group and C denotes Control group
\item
  There were no ties in this example
\item
  Now summing the ranks in the control group gives:\\
  \(T_C = 2 + 3 + 5 + 10 + 11 + 13 + 16 + 17 + 18 = 95\)
\item
  The exact test statistic is given by \(T_C-\frac{m(m+1)}{2}\), where m is the size of the control group
\item
  For our example, \(T_C-\frac{m(m+1)}{2}=95-\frac{9(9+1)}{2}=95-45=50\)
\end{itemize}

\hypertarget{exact-wilcoxon-rank-sum-test}{%
\subsection{Exact Wilcoxon Rank Sum Test}\label{exact-wilcoxon-rank-sum-test}}

\begin{itemize}
\tightlist
\item
  As with the signed rank test, there is an exact and an approximate version of the Wilcoxon Rank Sum test
\item
  The exact version will rely on the quantiles and cumulative probabilities of Wilcoxon Rank Sum distribution for calculation of the rejection region and the p-value, respectively
\end{itemize}

\hypertarget{example-wilcoxon-rank-sum-test}{%
\subsection{Example: Wilcoxon Rank Sum Test}\label{example-wilcoxon-rank-sum-test}}

\begin{itemize}
\tightlist
\item
  We can use the R function qwilcox() to obtain the quantiles that defined the rejection region for our example. Note that m is the number of controls and n is the number in the treatment group.
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_32}

\begin{itemize}
\tightlist
\item
  Recall that the value for the test statistic is 50
\item
  The conclusion when using each of the three alternative hypotheses in the context of our example appears in the table
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_33}

\begin{itemize}
\tightlist
\item
  The p-value corresponding to each alternative hypothesis appears in the table below. It is calculated using the pwilcox() function for the cumulative probability under the Wilcoxon rank sum distribution
\end{itemize}

\includegraphics[width=0.7\linewidth]{./9_34}

\begin{itemize}
\tightlist
\item
  Therefore, our data would lead us to conclude we \textbf{cannot} reject the null hypothesis of equality of the median recovery time between the two treatments
\end{itemize}

\hypertarget{approximate-wilcoxon-rank-sum-test}{%
\subsection{Approximate Wilcoxon Rank Sum Test}\label{approximate-wilcoxon-rank-sum-test}}

\begin{itemize}
\tightlist
\item
  On an average, \emph{if the null hypothesis of ``no difference'' is true}, we would expect \(T_c\) to be
\end{itemize}

\[E(T_C)=\frac{m(m+n+1)}{2}=\frac{9(9+9+1)}{2}=85.5\]

where m is the number of patients in the control group.

We would expect its variance to beand its variance to be

\[var(T_C)=\frac{mn(m+n+1)}{12}=\frac{9\times 9(9+9+1)}{12}=128.25\]

\begin{itemize}
\tightlist
\item
  and thus \(sd(T_C)=\sqrt{128.25}=11.32\)
\item
  Using the information on the expectation and variance of \(T_C\) we can calculate a Z-statistic
\item
  If the sample is large enough, \(z=\frac{T_C-E(T_C)}{sd(T_C)}\)
\end{itemize}

z follows a normal distribution, so that the null hypothesis can be tested using the usual normal tables

\begin{itemize}
\tightlist
\item
  Because \(T_C\) takes only whole number values, the continuity correction improves the accuracy of the approximation. Here, the continuity correction acts as if the whole number 95 occupies the entire interval from 94.5 to 95.5.
\item
  Thus we calculate
\end{itemize}

\[z=\frac{T_C-E(T_C)}{sd(T_C)}=\frac{94.5-85.5}{11.32}=0.795\]

so that p=P(Z\textgreater0.795) = \textbf{1-}pnorm(0.795) = 0.2133

\hypertarget{wilcoxon-rank-sum-test-in-r}{%
\subsection{Wilcoxon Rank Sum Test in R}\label{wilcoxon-rank-sum-test-in-r}}

\begin{verbatim}
> wilcox.test(x,y,correct=T,alternative= "greater")

    Wilcoxon rank sum test

data:  x and y
W = 50, p-value = 0.2181
alternative hypothesis: true location shift is greater than 0
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The function is the same as the one used for the signed rank test, but with the paired= argument set to its default value of F
\item
  The lack of ties in this example means that this result corresponds to the exact test by default
\end{itemize}

\begin{verbatim}
wilcox.test(x,y,correct=T,alternative="greater",exact=F)

    Wilcoxon rank sum test with continuity correction

data:  x and y
W = 50, p-value = 0.2134
alternative hypothesis: true location shift is greater than 0
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We could use the exact=F argument to obtain a p-value closer to the one we got using the approximate method
\end{itemize}

\hypertarget{summary-of-hypothesis-tests-covered-in-this-course}{%
\subsection{Summary of hypothesis tests covered in this course}\label{summary-of-hypothesis-tests-covered-in-this-course}}

\includegraphics[width=1\linewidth]{./9_40}
\includegraphics[width=1\linewidth]{./9_41}
\includegraphics[width=1\linewidth]{./9_42}

\hypertarget{bootstrap-confidence-interval}{%
\section{Bootstrap confidence interval}\label{bootstrap-confidence-interval}}

\hypertarget{confidence-intervals-vs.-hypothesis-testing}{%
\subsection{Confidence intervals vs.~Hypothesis Testing}\label{confidence-intervals-vs.-hypothesis-testing}}

\begin{itemize}
\tightlist
\item
  As mentioned in earlier lectures

  \begin{itemize}
  \tightlist
  \item
    Hypothesis testing is preferable when the goal of the statistical inference is to make a decision
  \item
    In exploratory research studies, where we are not yet interested in making a decision, a confidence interval reflecting the magnitude of the uncertainty in our inference is more useful
  \end{itemize}
\end{itemize}

\hypertarget{bootstrap-confidence-intervals}{%
\subsection{Bootstrap confidence intervals}\label{bootstrap-confidence-intervals}}

\begin{itemize}
\tightlist
\item
  A popular non-parametric method for the calculation of confidence intervals is the Bootstrap
\item
  It was described in a 1979 paper by Bradley Efron
\item
  The key idea is to perform the computations on the data itself and not to rely on any assumption about its distribution. The data is thus `pulling itself up by its bootstraps' so to speak\(^*\)
\item
  The Bootstrap method relies on resampling, i.e.~sampling from the observed data with replacement
\item
  The central limit theorem relied on the sampling distribution of the data (i.e.~distribution of the statistic in repeated samples drawn from the population)
\item
  The Bootstrap relies instead on the empirical (or resampling) distribution of the data (i.e.~the distribution of the statistic in repeated samples drawn from the observed dataset)
\end{itemize}

\(^*\)\href{https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf}{MIT Opencourseware handout}

\hypertarget{bootstrap-example}{%
\subsection{\texorpdfstring{Bootstrap: Example\(^*\)}{Bootstrap: Example\^{}*}}\label{bootstrap-example}}

\begin{itemize}
\tightlist
\item
  Suppose we roll an 8-sided die 10 times and we get the following (after sorting)

  \begin{itemize}
  \tightlist
  \item
    1, 1, 2, 3, 3, 3, 3, 4, 7, 7
  \end{itemize}
\item
  Imagine writing these values on 10 slips of paper, putting them in a hat and drawing one at random
\item
  The true vs.~resampling distributions would be as follows
\end{itemize}

\begin{tabular}{l|l|l|l|l|l|l|l|l}
\hline
    &    1 &    2 &    3 &    4 &    5 &    6 &    7 &    8\\
\hline
True distribution & 1/8 & 1/8 & 1/8 & 1/8 & 1/8 & 1/8 & 1/8 & 1/8\\
\hline
Resampling distribution & 2/10 & 1/10 & 4/10 & 1/10 & 0 & 0 & 2/10 & 0\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  We can see from this example that the resampling distribution is not the true distribution
\item
  It does not therefore provide a good estimate for the true distribution and should not be used to estimate the mean or quantiles of the true distribution
\item
  However, it can be shown that it provides a pretty good estimate of the variability in the distribution. That is why the resampling distribution can be used to estimate confidence intervals
\end{itemize}

\(^*\)\href{https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf}{MIT Opencourseware handout}

\hypertarget{the-empirical-bootstrap}{%
\subsection{The empirical bootstrap}\label{the-empirical-bootstrap}}

\begin{itemize}
\tightlist
\item
  Suppose we observe n data points \(x_1, x_2, …, x_n\) from the distribution F and that u is a statistic calculated from this sample, e.g.~the sample mean
\item
  An empirical bootstrap sample is a resample of the same size n from the observed sample
\item
  We denote the resampled values by \(x_1^*, x_2^*, …, x_n^*\). These values follow the empirical distribution \(F^*\) while \(u^*\) is the statistic calculated from the bootstrap sample
\item
  The bootstrap principle says that:

  \begin{itemize}
  \tightlist
  \item
    \(F^* ≈ F\)
  \item
    The variation of u is well-approximated by the variation of \(u^*\)
  \end{itemize}
\item
  The variation in u depends on the sample size. Therefore, the size of the resample is the same as the original sample to be able to estimate this variation
\end{itemize}

\hypertarget{example-10}{%
\subsection{Example}\label{example-10}}

\begin{itemize}
\tightlist
\item
  Let us use the same dataset we have been using so far. Say the goal is to estimate the surgical recovery time in the Treatment group
\end{itemize}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r}
\hline
Subject &  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9\\
\hline
Recovery time (days) & 19 & 22 & 25 & 26 & 28 & 29 & 34 & 37 & 38\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Because the sample size is small and we do not know whether the true distribution is normal, it is preferable not to use the confidence interval based on the central limit theorem, which we covered in Lecture 3
\item
  We wish to estimate the median with an 80\% confidence interval
\item
  The sample median is 28 days. We would like to know how much the sample median (u) varies around the true median (m), i.e.~we would like to know the distribution of δ=u-m
\item
  The bootstrap principle says that we can approximate the distribution of δ by the distribution of \(δ^*= u^*-u\), where \(u^*\) is the median in an empirical bootstrap sample
\item
  Note that the distribution of \(δ^*= u^*-u\) depends on quantities we already know. Therefore, it can be easily recreated by using a computer
\item
  We can draw as many resamples as we wish and calculate \(δ^*\) each time. We can estimate \(δ^*\) to a high degree of precision by drawing a large number of resamples
\item
  For example, if we used R to generate 10 bootstrap samples from our observed sample. They may look like this:
\end{itemize}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r}
\hline
S1 & S2 & S3 & S4 & S5 & S6 & S7 & S8 & S9 & S10\\
\hline
26 & 37 & 26 & 26 & 26 & 26 & 38 & 37 & 29 & 38\\
\hline
22 & 26 & 28 & 22 & 37 & 19 & 38 & 28 & 34 & 37\\
\hline
29 & 29 & 19 & 22 & 37 & 29 & 34 & 38 & 22 & 38\\
\hline
38 & 38 & 25 & 37 & 22 & 26 & 38 & 38 & 37 & 38\\
\hline
29 & 34 & 25 & 22 & 38 & 29 & 19 & 29 & 29 & 25\\
\hline
29 & 29 & 38 & 28 & 29 & 26 & 25 & 26 & 29 & 19\\
\hline
22 & 22 & 29 & 26 & 25 & 26 & 38 & 29 & 34 & 38\\
\hline
25 & 26 & 29 & 34 & 37 & 28 & 25 & 28 & 37 & 29\\
\hline
22 & 19 & 29 & 26 & 22 & 29 & 34 & 22 & 22 & 25\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  Notice that the same value may be sampled multiple times within each sample because it is a resample
\item
  The median of the 10 samples is
\end{itemize}

\[26, 29, 28, 26, 29, 26, 34, 29, 29, 37\]

\begin{itemize}
\tightlist
\item
  We can calculate \(δ^*\) in each case and sort in ascending order to obtain
\end{itemize}

\[-2, -2, -2, 0, 1, 1, 1, 1, 6, 9\]

\begin{itemize}
\tightlist
\item
  The 10\% and 90\% quantiles of \(δ^*\) may be approximated by -2 and 9
\item
  Therefore the bootstrap 80\% confidence interval for the true median (m) is
\end{itemize}

\[(u-\delta^*_{0.9},u-\delta^*_{0.1})=(28-(6),28-(-2))=(22,32)\]

\begin{itemize}
\tightlist
\item
  Note that the bootstrap cannot improve our point estimate of u. It is possible that u differs considerably from m, but u-m may still be well approximated by \(u^*-u\)
\end{itemize}

\hypertarget{r-code-for-bootstrap}{%
\subsection{R code for bootstrap}\label{r-code-for-bootstrap}}

\begin{verbatim}
x=c(19,22,25,26,28,29,34,37,38)
n=9 # sample size
nboot = 10000 # number of bootstrap samples
tmp=sample(x,n*nboot,replace=T)
resamples=matrix(tmp,ncol=nboot)

ustar=apply(resamples,2,median)
u=median(x)
deltastar=ustar-u

# quantiles of deltastar
d=quantile(deltastar,c(0.1,0.9))

# confidence interval
u-c(d[2],d[1])
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We can increase the number of resamples to a very large value, e.g.~10000
\item
  The function sample() allows us to sample with replacement
\item
  The function apply() is very useful for calculating a statistic using each row or column of a matrix
\end{itemize}

\hypertarget{lecture-10-analysis-of-variance-anova}{%
\chapter{Lecture 10: Analysis of Variance (ANOVA)}\label{lecture-10-analysis-of-variance-anova}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\begin{itemize}
\tightlist
\item
  So far we have studied methods for comparing means between two groups
\item
  Analysis of Variance (or ANOVA for short) allows us to compare means across more than two groups\\
\item
  You can think of it as the equivalent of the chi-squared test that allowed us to compare multiple proportions with a single hypothesis test
\end{itemize}

\hypertarget{example-mao-and-schizophrenia}{%
\subsection{Example: MAO and Schizophrenia}\label{example-mao-and-schizophrenia}}

\begin{itemize}
\tightlist
\item
  Monoamine oxidase (MAO) is an enzyme that is thought to play a role in the regulation of behavior.
\item
  To see whether different categories of patients with schizophrenia have different levels of MAO activity, researchers collected blood specimens from 42 patients and measured the MAO activity in the platelets.
\item
  The data gathered are displayed on the following two slides. Values are expressed as nmol benzylaldehyde product per \(10^8\) platelets per hour
\end{itemize}

\begin{tabular}{l|l|r|r}
\hline
Diagnosis & MAO activity & Mean & Standard deviation\\
\hline
I: Chronic undifferentiated schizophrenia (18 patients) & 6.8 4.1 7.3 14.2 18.8 9.9 7.4 11.9 5.2 7.8 7.8 8.7 12.7 14.5 10.7 8.4 9.7 10.6 & 9.8 & 3.6\\
\hline
II: Undifferentiated with paranoid features (16 patients) & 7.8 4.4 11.4 3.1 4.3 10.1 1.5 7.4 5.2 10.0 3.7 5.5 8.5 7.7 6.8 3.1 & 6.3 & 2.9\\
\hline
III: Paranoid schizophrenia (8 patients) & 6.4 10.8 1.1 2.9 4.5 5.8 9.4 6.8 & 6.0 & 3.2\\
\hline
\end{tabular}

\includegraphics[width=1\linewidth]{./10_6}

\begin{itemize}
\tightlist
\item
  From the descriptive statistics and the boxplot, we can see that the mean MAO value in diagnostic category I is higher than in the other two groups
\item
  But we can also see that there is a lot of variability within each group leading to overlapping boxplots
\item
  Is the observed difference between the means statistically significant? In other words, if there was in fact no difference in mean MAO scores between the 3 groups, would it be unusual to obtain the data at hand?
\end{itemize}

\hypertarget{why-not-answer-these-questions-with-repeated-t-tests}{%
\subsection{Why not answer these questions with repeated t-tests?}\label{why-not-answer-these-questions-with-repeated-t-tests}}

\begin{itemize}
\tightlist
\item
  We could have carried out 3 separate t-tests to compare the groups. The null hypotheses would be
\end{itemize}

\includegraphics[width=1\linewidth]{./10_8}

\begin{itemize}
\tightlist
\item
  There are several drawbacks:

  \begin{itemize}
  \tightlist
  \item
    Large number of pairwise comparisons are difficult to comprehend
  \item
    As the number of tests increases, the Type I error increases
  \item
    Does not permit us to recognize the structure of the data, e.g.~a natural ordering within the groups
  \item
    Does not permit us to improve our estimate of the variance in each group by using information from all groups
  \end{itemize}
\end{itemize}

\hypertarget{graphical-perspective-on-anova}{%
\subsection{Graphical perspective on ANOVA}\label{graphical-perspective-on-anova}}

\begin{itemize}
\tightlist
\item
  When carrying out an ANOVA, the first step is to carry out the global null hypothesis test of
\end{itemize}

\[H_0:\mu_1=\mu_2=...=\mu_J\]

where J is the number of groups being tested. The alternative is always two-sided and states that at least two of the means are not equal

\begin{itemize}
\tightlist
\item
  We will look at a series of hypothetical dotplots to understand how this hypothesis is tested
\item
  A dotplot is essentially a scatterplot, where the variable on the x-axis is a categorical variable and the variable on y-axis is a continuous variable
\end{itemize}

\includegraphics[width=1\linewidth]{./10_10}

\begin{itemize}
\tightlist
\item
  Graph (a) illustrates a setting where \(H_0\) is true. Therefore any differences observed between the groups in the sample means are due to chance
\item
  Graph (b) illustrates a setting where \(H_0\) is false. The means for groups 1 and 2 are quite different from those for groups 3 and 4. The small standard deviation in each group makes it possible to visualize the differences easily
\end{itemize}

\includegraphics[width=0.5\linewidth]{./10_12}

\includegraphics[width=1\linewidth]{./10_13}

\begin{itemize}
\tightlist
\item
  In the third graph, \(H_0\) is in fact false. The means in the four groups are identical to those in graph (b) seen earlier
\item
  However, the individual group standard deviations are very large making it hard to tell that the population means differ
\item
  In summary, the different graphs show us that in order to judge whether there is a difference between the group means, we need to consider the inherent variability in the data
\item
  In order to be able to detect a difference there must not only be a variation \textbf{between} the group means, but this variation must be large relative to the variability \textbf{within} the groups
\item
  Hence in order to compare means, we need an analysis of the variance (or ANOVA)
\end{itemize}

\hypertarget{one-way-anova}{%
\subsection{One-way ANOVA}\label{one-way-anova}}

\begin{itemize}
\tightlist
\item
  An ANOVA for comparing 3 or more groups defined by one variable is called a one-way ANOVA
\item
  For example, the 3 groups defined by the variable ``schizophrenia diagnosis'' can be compared using a one-way ANOVA
\item
  More complex models would allow comparisons across groups defined by more than one variable, e.g.~a two-way ANOVA could be used to compare groups defined by ``schizophrenia diagnosis'' and ``family history''
\end{itemize}

\hypertarget{one-way-anova-assumptions}{%
\subsection{One-way ANOVA: Assumptions}\label{one-way-anova-assumptions}}

\begin{itemize}
\tightlist
\item
  The observations in the different groups are randomly selected
\item
  The continuous variable of interest follows a normal distribution
\item
  The variance of this variable is the same in each group
\end{itemize}

\hypertarget{one-way-anova-notation}{%
\subsection{One-way ANOVA: Notation}\label{one-way-anova-notation}}

\begin{itemize}
\tightlist
\item
  We assume that

  \begin{itemize}
  \tightlist
  \item
    There are J groups to compare
  \item
    The observations are denoted as \(y_{ij}\) for the \(i^{th}\) observation in the \(j^{th}\) group
  \item
    The sample size in the \(j^{th}\) group is \(n_j\)
  \item
    The mean in the \(j^{th}\) group is \(\bar y_j\)
  \item
    The standard deviation in the \(j^{th}\) group is \(s_j\)
  \end{itemize}
\item
  The total number of observations is \(n_.=\sum_{j=1}^Jn_j\)
\item
  The grand mean, i.e.~the mean across all groups is
\end{itemize}

\[\bar{\bar y}=\frac{\sum_{j=1}^J\sum_{i=1}^{n_j}y_{ij}}{n_.}\]

\begin{itemize}
\tightlist
\item
  The grand mean can also be expressed as the weighted average of the group means
\end{itemize}

\[\bar{\bar y}=\frac{\sum_{j=1}^Jn_j\bar y_{j}}{\sum_{j=1}^Jn_j}\]

\hypertarget{one-way-anova-mao-and-schizophrenia}{%
\subsection{One-way ANOVA: MAO and Schizophrenia}\label{one-way-anova-mao-and-schizophrenia}}

\begin{itemize}
\tightlist
\item
  There are J=3 groups in this problem
\item
  The sample size in each group is \(n_1 = 18\), \(n_2 = 16\) and \(n_3 = 8\)
\item
  Following our notation

  \begin{itemize}
  \tightlist
  \item
    \(y_{11} = 6.8, y_{13} = 6.4, y_{31} = 7.3\)
  \end{itemize}
\item
  The grand mean is given by
\end{itemize}

\[\bar{\bar y}=\frac{\sum_{j=1}^Jn_j\bar y_{i}}{\sum_{j=1}^Jn_j}=\frac{=18\times9.8+16\times6.3+8\times6.0}{18+16+8}=7.7\]

\begin{itemize}
\tightlist
\item
  In keeping with our earlier discussion, in order to implement an ANOVA for our example, we need to calculate the between-group and within-group variance in MAO
\end{itemize}

\hypertarget{anova-within-group-variation}{%
\subsection{ANOVA: Within-group variation}\label{anova-within-group-variation}}

\begin{itemize}
\tightlist
\item
  Since the variance in each group is assumed to be the same, it is best estimated by the pooled variance \((s_p^2)\) across groups.

  \begin{itemize}
  \tightlist
  \item
    You will recognize this expression as being similar to the one you saw when we studied t-tests
  \end{itemize}
\end{itemize}

\[s_p^2=\frac{\sum_{j=1}^J(n_j-1)s_j^2}{\sum_{j=1}^J(n_j-1)}\]

\begin{itemize}
\tightlist
\item
  In addition to the familiar pooled variance we define some other related measures that are typically reported in an ANOVA
\end{itemize}

\includegraphics[width=1\linewidth]{./10_23}

\begin{itemize}
\tightlist
\item
  Notice that the mean square within groups is the same as the pooled variance
\end{itemize}

\hypertarget{mao-within-group-variation}{%
\subsection{MAO: Within-group variation}\label{mao-within-group-variation}}

\includegraphics[width=1\linewidth]{./10_24}

\hypertarget{anova-between-group-variation}{%
\subsection{ANOVA: Between-group variation}\label{anova-between-group-variation}}

\begin{itemize}
\tightlist
\item
  The variation between groups is expressed by comparing each group mean to the grand mean
\end{itemize}

\includegraphics[width=1\linewidth]{./10_25}

\hypertarget{mao-between-group-variation}{%
\subsection{MAO: Between-group variation}\label{mao-between-group-variation}}

\begin{itemize}
\tightlist
\item
  The variation between groups is expressed by comparing each group mean to the grand mean
\end{itemize}

\includegraphics[width=1\linewidth]{./10_25}

\hypertarget{a-fundamental-relationship-of-anova}{%
\subsection{A fundamental relationship of ANOVA}\label{a-fundamental-relationship-of-anova}}

\begin{itemize}
\tightlist
\item
  Notice that the deviation of an individual observation from the grand mean can be expressed as follows:
\end{itemize}

\[y_{ij}-\bar{\bar y}=(y_{ij}-\bar y_j)+(\bar y_j-\bar{\bar y})\]

\begin{itemize}
\tightlist
\item
  In other words, this deviation is made up of two deviations:\\
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  a within-group deviation \((y_{ij}-\bar y_j)\), and\\
\item
  a between-group deviation \((\bar y_j-\bar{\bar y})\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  A less obvious relation is that this split also holds at the level of the sums of squares, i.e.
\end{itemize}

\[\sum\sum(y_{ij}-\bar{\bar y})^2=\sum\sum(y_{ij}-\bar y_j)^2+\sum\sum(\bar y_j-\bar{\bar y})^2\]

where the summation is over both i and j

\begin{itemize}
\tightlist
\item
  The quantity on the left of the equation, is called the total sum of squares
\item
  The preceding equation shows that\\
  Total sum of squares\\
  = Within-group sum of squares + Between group sum of squares
\item
  As we will see, answering our original question will depend on whether the between group sum of squares makes up the larger or the smaller part of the total sum of squares
\item
  The following slide depicts the ANOVA table that is used to summarize the information on the total, the between-group and the within-group variation
\end{itemize}

\hypertarget{anova-table}{%
\subsection{ANOVA table}\label{anova-table}}

\includegraphics[width=1\linewidth]{./10_30}

\begin{itemize}
\tightlist
\item
  Please note that the summation is across i and j
\item
  SS/df is calculated separately for each row
\end{itemize}

\hypertarget{mao-example-anova-table}{%
\subsection{MAO Example: ANOVA table}\label{mao-example-anova-table}}

\includegraphics[width=1\linewidth]{./10_31}

\begin{itemize}
\tightlist
\item
  Please note that the sum of squares is slightly different from the earlier slide due to differences in rounding
\item
  Our goal now is to use the data in this table to test the null hypothesis specified earlier
\end{itemize}

\hypertarget{the-f-test}{%
\subsection{The F-test}\label{the-f-test}}

\begin{itemize}
\tightlist
\item
  Recall the null and alternative hypotheses are\\
  \(H_0:\mu_1=\mu_2=...=\mu_J\), vs.\\
  Ha: At least two of the means are not equal
\item
  As with the other hypothesis tests we studied earlier, we need to calculate the test statistic and determine the rejection region
\item
  The test statistic is given by:
\end{itemize}

\[F=\frac{MS(between)}{MS(within)},\]

where MS denotes the mean squared error

\begin{itemize}
\tightlist
\item
  The rejection region is determined using an F distribution with degrees of freedom equal to the degrees of freedom between groups and the degrees of freedom within groups
\end{itemize}

\hypertarget{mao-example-the-f-test}{%
\subsection{MAO example: The F-test}\label{mao-example-the-f-test}}

\begin{itemize}
\tightlist
\item
  The test statistic is given by:
\end{itemize}

\[F=\frac{MS(between)}{MS(within)}=\frac{68.1}{10.7}=6.36,\]

where MS denotes the mean squared error

\begin{itemize}
\tightlist
\item
  The rejection region is determined using an F distribution with degrees of freedom equal to the degrees of freedom between groups and the degrees of freedom within groups
\end{itemize}

\hypertarget{the-f-distribution}{%
\subsection{The F-distribution}\label{the-f-distribution}}

\includegraphics[width=0.5\linewidth]{./10_35}

\begin{itemize}
\tightlist
\item
  This probability distribution is defined over the positive real line
\item
  It is parametrized by the numerator degrees of freedom and the denominator degrees of freedom
\item
  The figure on the right corresponds to a \(F_{4,20}\) distribution whose 95\% quantile is 2.87
\end{itemize}

\hypertarget{rejection-region}{%
\subsection{Rejection region}\label{rejection-region}}

\begin{itemize}
\tightlist
\item
  If the Type I error is set to the traditional value of 0.05, then the rejection region for our problem is the region above the 95\% quantile of the \(F_{2,39}\) distribution
\end{itemize}

This quantile can be obtained from R as follows:

\begin{verbatim}
> qf(0.95,2,39)
[1] 3.238096
\end{verbatim}

\hypertarget{mao-example-conclusion}{%
\subsection{MAO example: Conclusion}\label{mao-example-conclusion}}

\begin{itemize}
\tightlist
\item
  Since the F test statistic of value of 6.36 lies in the region above 3.23, we have enough evidence to reject the null hypothesis of equality of the mean MAO level in the three groups
\item
  One can calculate an accompanying p-value by estimating the probability of being higher than the observed test statistic as follows:
\end{itemize}

\begin{verbatim}
> 1-pf(6.36,2,39)
[1] 0.004068354
\end{verbatim}

\hypertarget{anova-in-r}{%
\subsection{ANOVA in R}\label{anova-in-r}}

\begin{itemize}
\tightlist
\item
  The variable defining the groups must be identified as a factor (or a categorical variable). Otherwise it will be treated as a continuous variable
\item
  The relevant functions are the aov and summary functions as illustrated below (Note some numbers may differ slightly from those on the preceding slides due to rounding)
\end{itemize}

\begin{verbatim}
> m1=aov(mao~as.factor(diagnosis),data=a)
> summary(m1)
                     Df Sum Sq Mean Sq F value  Pr(>F)   
as.factor(diagnosis)  2  136.1   68.06   6.346 0.00411 **
Residuals            39  418.3   10.72                   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end{verbatim}

\hypertarget{checking-the-assumptions-of-anova}{%
\subsection{Checking the assumptions of ANOVA}\label{checking-the-assumptions-of-anova}}

\begin{itemize}
\tightlist
\item
  Once the ANOVA model is fit, it is important to check whether the assumptions under which it was fit hold

  \begin{itemize}
  \tightlist
  \item
    Normality of the observations within groups
  \item
    Constant variance across groups
  \end{itemize}
\end{itemize}

Both these assumptions can be verified using the residuals, i.e.~the differences \((y_{ij}-\bar y_j)\)

\hypertarget{residuals-for-mao-example}{%
\subsection{Residuals for MAO example}\label{residuals-for-mao-example}}

\begin{tabular}{r|r}
\hline
Observation & Residual\\
\hline
6.8 & -3.005560\\
\hline
4.1 & -5.705560\\
\hline
7.3 & -2.505560\\
\hline
14.2 & 4.394444\\
\hline
18.8 & 8.994444\\
\hline
9.9 & 0.094444\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  The first six residuals for the MAO example are listed in the adjoining table. Note that residuals can be positive or negative. Observations closest to the mean are associated with a near 0 value of the residual
\end{itemize}

\includegraphics[width=0.5\linewidth]{./10_41}

\begin{itemize}
\tightlist
\item
  The dot plot on the right illustrates the distribution of the residuals for each diagnostic category
\item
  We can see more clearly now that the residuals are scattered around zero
\end{itemize}

\hypertarget{quantile-quantile-plot-to-check-for-normality}{%
\subsection{Quantile-quantile plot to check for normality}\label{quantile-quantile-plot-to-check-for-normality}}

\begin{itemize}
\tightlist
\item
  A quantile-quantile plot compares the quantiles of the standardized residuals to the quantiles of the standard normal distribution
\item
  If in fact the \textbf{standardized} residuals were normally distributed we know that

  \begin{itemize}
  \tightlist
  \item
    50\% of the standardized residuals will lie below zero
  \item
    2.5\% of them will lie below a value of -1.96
  \item
    More generally, p\% of them will lie below \(Q_p\) where \(Q_p\) denotes the \textbf{quantile} of the standard normal distribution
  \end{itemize}
\item
  If the residuals are normally distributed, we would expect a scatter plot of the standardized residuals vs.~the standard normal quantiles to lie along the diagonal line
\end{itemize}

\hypertarget{quantile-quantile-qq-plot-for-mao-data}{%
\subsection{Quantile-quantile (QQ) plot for MAO data}\label{quantile-quantile-qq-plot-for-mao-data}}

\includegraphics[width=0.5\linewidth]{./10_43}

\begin{itemize}
\tightlist
\item
  The QQ plot for the MAO data shows that the scatter of points is close to the diagonal, meaning that our assumption that the observations are normally distributed is reasonable
\end{itemize}

\hypertarget{residual-plot-to-evaluate-constant-variance-across-groups}{%
\subsection{Residual plot to evaluate constant variance across groups}\label{residual-plot-to-evaluate-constant-variance-across-groups}}

\begin{itemize}
\tightlist
\item
  A scatter plot of the standardized residuals vs.~the groups being compared will allow us to check

  \begin{itemize}
  \tightlist
  \item
    if the variance is comparable across groups
  \item
    if there are any outlying or unusual values
  \end{itemize}
\item
  The following plot of fictitious data illustrates a case where the variance is not constant across groups

  \begin{itemize}
  \tightlist
  \item
    Note that i denotes the group in this illustration
  \end{itemize}
\end{itemize}

\includegraphics[width=0.5\linewidth]{./10_45}

\hypertarget{residuals-for-mao-example-1}{%
\subsection{Residuals for MAO example}\label{residuals-for-mao-example-1}}

\includegraphics[width=0.5\linewidth]{./10_46}

\begin{itemize}
\tightlist
\item
  This plot is similar to the dot plot we saw earlier, except it is based on the standardized residuals
\item
  The two green lines demarcate the values -2 and 2 between which we would expect 95\% of the standardized residuals to lie
\item
  There appears to be one unusual value lying beyond 2 in the diagnostic category 1
\end{itemize}

\hypertarget{multiple-comparisons}{%
\subsection{Multiple comparisons}\label{multiple-comparisons}}

\begin{itemize}
\tightlist
\item
  In the event the F-test is statistically significant, it would suggest that there is evidence of a difference between at least one pair of means
\item
  We therefore proceed to compare different pairs of means to identify which of them is statistically and clinically significant
\item
  More generally, some statistical problems, e.g.~in exploratory genetic studies, may involve a very large number of hypothesis tests. The ideas we discuss here under multiple comparisons, would also apply to such studies
\item
  As mentioned at the beginning of the lecture, one of the concerns with carrying out numerous hypothesis tests is that the Type I error can increase rapidly
\item
  The relevance of these concerns is an area of debate in statistics. We will touch upon

  \begin{itemize}
  \tightlist
  \item
    Why the problem arises
  \item
    Commonly used methods for controlling the Type I error
  \item
    Why there is a debate
  \end{itemize}
\item
  Multiple comparisons: The problem
\item
  Let us consider a situation where the global null hypothesis of equality of the means across the J groups is in fact true

  \begin{itemize}
  \tightlist
  \item
    This would mean the null hypothesis of equality of means is true for all pairwise comparisons as well
  \end{itemize}
\item
  Assume we carry out a series of pairwise hypothesis tests comparing the various groups, and that \textbf{the} Type I error of each of these tests is set at 0.05.

  \begin{itemize}
  \tightlist
  \item
    In other words, for each test, though the null hypothesis is true, there is a 5\% chance that we may falsely reject the null hypothesis
  \end{itemize}
\item
  If any of the individual tests was significant, we would reject our global null hypothesis
\item
  In other words, a Type I error would occur if any of the individual pairwise comparisons were statistically significant
\item
  For illustration, let J=3. This would mean there are 3 pairwise comparisons.

  \begin{itemize}
  \tightlist
  \item
    The probability that at least one of the pairwise tests is significant is \(1-(1-0.05)^3 = 0.14\), increasing our Type I error considerably beyond the desired value of 0.05
  \end{itemize}
\end{itemize}

\begin{tabular}{r|r}
\hline
J &  Overall risk\\
\hline
2 & 0.05\\
\hline
3 & 0.14\\
\hline
4 & 0.19\\
\hline
6 & 0.26\\
\hline
8 & 0.30\\
\hline
10 & 0.40\\
\hline
\end{tabular}

\begin{itemize}
\tightlist
\item
  More generally, the table above shows how the Type I error for the global test increases with J despite the Type I error for individual tests (α) being fixed at 0.05
\item
  If we were comparing 10 independent groups, there is a very high chance of 0.40 that at least one of the pairwise comparisons is statistically significant
\end{itemize}

\hypertarget{multiple-comparisons-solutions}{%
\subsection{Multiple comparisons: Solutions}\label{multiple-comparisons-solutions}}

\begin{itemize}
\tightlist
\item
  To control the Type I error for the global hypothesis test, we make a distinction between two types of Type I error:

  \begin{itemize}
  \tightlist
  \item
    Comparison-wise error \((α_C)\) or the Type I error for each of the individual comparisons,
  \item
    Experiment-wise error \((α_E)\) or the Type I error for the family of hypotheses comprising the different individual comparisons
  \end{itemize}
\item
  It can be shown that \(α_E ≤ k × α_C\), where k is the number of individual hypothesis tests
\item
  We will cover two (of many proposed) solutions for the multiple comparisons problem

  \begin{itemize}
  \tightlist
  \item
    Bonferroni correction
  \item
    Tukey's Honest Significant Method (HSD)
  \end{itemize}
\end{itemize}

\hypertarget{bonferroni-correction}{%
\subsection{Bonferroni correction}\label{bonferroni-correction}}

\begin{itemize}
\tightlist
\item
  This simple approach is perhaps the most commonly encountered correction
\item
  It relies on the relation between the experiment-wise and the comparison-wise error rates
\item
  The idea is simple.

  \begin{itemize}
  \tightlist
  \item
    If the desired experiment-wise error rate is α, define the comparison-wise error rate to be α/k, where k is the number of comparisons.
  \item
    This ensures that the experiment-wise error rate is\\
    \(α_E ≤ k × α_C = k × (α/k) = α\)
  \end{itemize}
\item
  To implement this approach you can either

  \begin{itemize}
  \tightlist
  \item
    multiply the observed p-value for the t-test with Type1 error α by k and compare to α
  \item
    Or you can set the significance level to α/k when carrying out the t-test
  \end{itemize}
\item
  The drawback of the Bonferroni correction is that when the number of planned hypothesis tests is very large, α/k approaches 0 making it practically impossible to reject the null hypothesis
\end{itemize}

\hypertarget{bonferroni-correction-for-the-mao-example}{%
\subsection{Bonferroni correction for the MAO example}\label{bonferroni-correction-for-the-mao-example}}

\begin{verbatim}
> pairwise.t.test(a$mao,as.factor(a$diagnosis),p.adj="none")

    Pairwise comparisons using t tests with pooled SD 

data:  a$mao and as.factor(a$diagnosis) 

  1      2     
2 0.0033 -     
3 0.0087 0.8233

P value adjustment method: none 


> pairwise.t.test(a$mao,as.factor(a$diagnosis),p.adj="bonf")

    Pairwise comparisons using t tests with pooled SD 

data:  a$mao and as.factor(a$diagnosis) 

  1      2     
2 0.0099 -     
3 0.0262 1.0000

P value adjustment method: bonferroni
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The panel above includes the p-values based on the Bonferroni correction and without the correction
\item
  Notice that the corrected p-values are obtained by multiplying the uncorrected p-values by 3
\item
  If the pre-determined significance level were α=0.01, then the Bonferroni correction would lead to concluding that only the difference between groups 1 and 2 is significant. The difference between groups 1 and 3 would no longer be significant
\item
  If the significance level were α=0.05, both approaches would lead to the same conclusion that the pairs (1,2) and (1,3) are statistically significantly different from each other
\end{itemize}

\hypertarget{bonferroni-correction-1}{%
\subsection{Bonferroni correction}\label{bonferroni-correction-1}}

\begin{itemize}
\tightlist
\item
  The Bonferroni correction can also be applied in the form of confidence intervals rather than hypothesis tests
\item
  You can calculate a series of confidence intervals for pairwise differences using the same methods we studied previously, except you would replace the \(t_{α/k,df}\) quantile instead of the \(t_{α,df}\) quantile
\end{itemize}

\hypertarget{tukeys-confidence-interval}{%
\subsection{Tukey's confidence interval}\label{tukeys-confidence-interval}}

\begin{itemize}
\tightlist
\item
  This approach also involves replacing the standard t-distribution quantile by a value that adjusts for the multiple comparisons
\item
  This method, due to Tukey, relies on a probability distribution called the studentized range distribution rather than the t-distribution
\item
  The end result is a wider confidence interval than would be obtained based on the t-distribution quantiles, making it more difficult to conclude that a difference is statistically significant
\end{itemize}

\hypertarget{results-for-the-mao-data}{%
\subsection{Results for the MAO data}\label{results-for-the-mao-data}}

\begin{verbatim}
> TukeyHSD(m1)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = mao ~ as.factor(diagnosis), data = a)

$`as.factor(diagnosis)`
         diff       lwr        upr     p adj
2-1 -3.524306 -6.265642 -0.7829689 0.0090142
3-1 -3.843056 -7.233250 -0.4528613 0.0231957
3-2 -0.318750 -3.773525  3.1360251 0.9725497
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Above are the adjusted 95\% Tukey HSD confidence intervals for the MAO data
\item
  The output also includes an adjusted p-value that is higher than the p-value obtained with no correction
\end{itemize}

\hypertarget{multiple-comparisons-the-debate}{%
\subsection{Multiple comparisons: The debate}\label{multiple-comparisons-the-debate}}

\begin{itemize}
\tightlist
\item
  The article by Rothman in the course notes argues that correction for multiple comparisons is in fact never necessary because:

  \begin{itemize}
  \tightlist
  \item
    All arguments rely on the assumption that the null hypothesis is true. In many instances the null hypothesis of equality between groups is unlikely
  \item
    If we correct for the overall Type I error to remain small, we automatically increase the risk of the Type II error. This means we run the risk of missing an important difference between groups
  \end{itemize}
\item
  Calculations arguing in favour of controlling for multiple comparisons assume that the various hypothesis tests are completely independent
\item
  But in fact they are not as they involve the same underlying data
\item
  A more appealing solution is to model the data appropriately, using a model that recognizes the dependence between the various comparisons and naturally results in wider confidence intervals
\end{itemize}

\hypertarget{steps-involved-in-anova}{%
\subsection{Steps involved in ANOVA}\label{steps-involved-in-anova}}

\begin{itemize}
\tightlist
\item
  Summarize your data using relevant descriptive statistics and plots
\item
  State the global hypothesis and carry out the corresponding F-test
\item
  Check whether assumptions regarding normality and constant variance were reasonable
\item
  If the F-test is significant (but even if not), you may wish to report pairwise comparisons using confidence intervals and apply a correction as relevant
\end{itemize}

\hypertarget{lecture-11-analysis-of-variance-anova-2}{%
\chapter{Lecture 11: Analysis of Variance (ANOVA) 2}\label{lecture-11-analysis-of-variance-anova-2}}

\hypertarget{way-anova-model}{%
\section{1-way ANOVA model}\label{way-anova-model}}

\begin{itemize}
\tightlist
\item
  Previously we studied methods for 1-way ANOVA focusing on the division of the total variance into between-group and within-group variances
\item
  It can also be helpful to think of 1-way ANOVA in terms of the following model
\end{itemize}

\[y_{ij}=\mu+\tau_j+\epsilon_{ij}\]

where µ is the true mean in the population and \(\tau_j\) is the deviation of the mean in the \(j^{th}\) group from µ

\begin{itemize}
\tightlist
\item
  The \(\tau_j\)'s are referred to as the group effects. Notice that \(H_0: μ_1 = μ_2 = … = μ_J\) is equivalent to \(H_0: τ_1 = \tau_2 = … = τ_J = 0\)
\item
  The unknown parameters in the model on the previous slide are estimated as follows\\
  \(y_{ij}=\hat{\mu}+\hat{\tau_j}+\hat{\epsilon_{ij}}\), or\\
  \(y_{ij}=\bar{\bar y}+(\bar y_j-\bar{\bar y})+(y_{ij}-\bar y_j)\)
\item
  We can see now why the within-group sum of squares may be referred to as the residual sum of squares.
\end{itemize}

\hypertarget{extending-the-1-way-anova-model}{%
\subsection{Extending the 1-way ANOVA model}\label{extending-the-1-way-anova-model}}

\begin{itemize}
\tightlist
\item
  We would like to reduce the residual term \((y_{ij}-\bar y_j)\) to be as small as possible
\item
  This may be achieved by including other sources of variation in the model besides the J groups
\item
  In the simplest case we consider one more source of variation resulting in a 2-way ANOVA model

  \begin{itemize}
  \tightlist
  \item
    the \(2^{nd}\) source of variation could be a variable of interest, e.g.~an experimental intervention, or an extraneous variable, e.g.~a subject or design characteristic
  \end{itemize}
\end{itemize}

\hypertarget{study-designs-handled-with-2-way-anova-models}{%
\subsection{Study designs handled with 2-way ANOVA models}\label{study-designs-handled-with-2-way-anova-models}}

\begin{itemize}
\tightlist
\item
  \textbf{Randomized block design}: Experimental subjects are first divided into homogeneous blocks before they are randomly assigned to a treatment group.
\item
  \textbf{Factorial design}: Two variables (factors) of interest are measured on each unit of observation
\item
  \textbf{Repeated measures design}: Multiple observations are made on the same subject, e.g.~over time or by different observers
\end{itemize}

\hypertarget{example-repeated-measures-design}{%
\subsection{Example: Repeated Measures Design}\label{example-repeated-measures-design}}

\begin{itemize}
\tightlist
\item
  A study was designed to examine if the plasma concentration of a hormone was affected by the activity level of subjects
\item
  Measurements were obtained on each of 5 subjects while they were resting, exercising or sleeping
\end{itemize}

\begin{tabular}{l|r|r|r|l}
\hline
  &  Sleeping &  Resting &  Exercising  &    Subject Mean\\
\hline
Subject 1 & 1.30 & 1.78 & 2.670 & 1.917\\
\hline
Subject 2 & 1.15 & 1.25 & 2.250 & 1.55\\
\hline
Subject 3 & 0.50 & 1.27 & 1.460 & 1.077\\
\hline
Subject 4 & 0.30 & 0.55 & 1.660 & 0.837\\
\hline
Subject 5 & 1.30 & 0.80 & 0.800 & 0.967\\
\hline
Mean & 0.91 & 1.13 & 1.768 & \\
\hline
\end{tabular}

\hypertarget{number-of-study-units-from-3rs-website-of-michael-festing}{%
\subsection{\texorpdfstring{Number of study units (from 3rs website of Michael Festing\(^*\))}{Number of study units (from 3rs website of Michael Festing\^{}*)}}\label{number-of-study-units-from-3rs-website-of-michael-festing}}

\includegraphics[width=0.5\linewidth]{./11_08a}

\begin{itemize}
\tightlist
\item
  In this study the animals are all housed in one cage and the treatment is given by injection.
\item
  Any two animals can receive different treatments, so the animal is the experimental unit and N (the number of experimental units) is 8
\end{itemize}

\includegraphics[width=0.5\linewidth]{./11_08b}

\begin{itemize}
\tightlist
\item
  In this study the animals are housed two per cage and the treatment is given in the food or water.
\item
  Both animals in a cage receive the same treatment. So N (the number of experimental units) is 4
\end{itemize}

\(^*\)\href{http://www.3rs-reduction.co.uk/}{website}

\hypertarget{example-repeated-measures-design-1}{%
\subsection{Example: Repeated Measures Design}\label{example-repeated-measures-design-1}}

\begin{itemize}
\tightlist
\item
  Though the study has 15 unique observations, measurements on the same patient are more similar than measurements on different patients
\item
  Like in the paired t-test, the statistical analysis must remove the effect of variability across patients in order to compare the three activity levels
\item
  The number of units is in fact 5. This can be recognized using a two-way ANOVA
\end{itemize}

\hypertarget{example-2-way-anova-model}{%
\subsection{Example: 2-way ANOVA model}\label{example-2-way-anova-model}}

\begin{itemize}
\tightlist
\item
  Our goal is to test the null hypothesis\\
  \(H_0:\mu_{sleeping}=\mu_{resting}=\mu_{exercising}\), vs.\\
  Ha: At least two of the means are not equal
\item
  The 2-way ANOVA model may be expressed as\\
  \(y_{ijk}=\mu+\tau_j+\beta_i+\epsilon_{ijk}\), where\\
  \(\mu\) is the population mean\\
  \(\tau_j\) is the group effect\\
  \(\beta_i\) is the patient effect, and\\
  \(\epsilon_{ijk}\) is the error term
\end{itemize}

\hypertarget{example-visualizing-the-patient-effects}{%
\subsection{Example: Visualizing the patient-effects}\label{example-visualizing-the-patient-effects}}

\begin{itemize}
\tightlist
\item
  To visualize the impact of the patient effects, we can reorganize our model as follows
\end{itemize}

\[y_{ijk}-\tau_j=\mu+\beta_i+\epsilon_{ijk}\]

The left-hand-side describes the data after the group effects have been removed. It can be estimated as follows:

\[y_{ijk}-\hat{\tau_j}=y_{ijk}-(\bar y_{.j}-\bar{\bar y})=residual+grand\space mean\]

\includegraphics[width=0.6\linewidth]{./11_12}

\begin{itemize}
\tightlist
\item
  The above shows that even after removing the group (or activity) effects, there is variability in the data due to the patient-effects
\item
  In order to measure the relative importance of this variability we will need to calculate the means squares for patient-effects
\end{itemize}

\includegraphics[width=0.6\linewidth]{./11_14}

\begin{itemize}
\tightlist
\item
  On the above we see that

  \begin{itemize}
  \tightlist
  \item
    The variability among the group means is unchanged between the plots
  \item
    The within-group variability reduced after adjusting for the patient-effects
  \item
    As a result, the difference between the three group means is more pronounced
  \end{itemize}
\item
  Recall that the F-test statistic is the ratio of the between-group to the within-group variance. As the within-group variance decreases, the F-test is more likely to be statistically significant
\end{itemize}

\hypertarget{example-1-way-anova-model}{%
\subsection{Example: 1-way ANOVA model}\label{example-1-way-anova-model}}

\includegraphics[width=1\linewidth]{./11_16}

\begin{itemize}
\tightlist
\item
  When we ignore the patient effects, the F-statistic is 0.993/0.3244 = 3.061 and the corresponding p-value is 0.0843 when comparing to the \(F_{2,12}\) distribution
\item
  Therefore, at the Type I error level of 0.05 we would conclude that there is insufficient evidence to reject the null hypothesis that the mean plasma concentration of the hormone is the same at all three activity levels
\end{itemize}

\hypertarget{example-2-way-anova-model-1}{%
\subsection{Example: 2-way ANOVA model}\label{example-2-way-anova-model-1}}

\includegraphics[width=1\linewidth]{./11_17}

\begin{itemize}
\tightlist
\item
  By comparing the 1-way and the 2-way ANOVA models, we can see that the previously estimated within-groups variance is now split further into the between-patient variance and the within-group variance (or the residual)
\end{itemize}

\includegraphics[width=1\linewidth]{./11_18}

\begin{itemize}
\tightlist
\item
  After adjusting for the patient effects (i.e.~the between patient variability), we find that there is now a statistically significant difference between the three activity levels. F-statistic = 0.993/0.182 = 5.471. The corresponding p-value is 0.0318
\item
  This example illustrates how ignoring the dependence between multiple observations on the same patient could lead us to an incorrect conclusion
\end{itemize}

\hypertarget{way-anova-in-r}{%
\subsection{2-way ANOVA in R}\label{way-anova-in-r}}

\begin{verbatim}
> m1=aov(y~as.factor(group),data=repmd)
> summary(m1)
                 Df Sum Sq Mean Sq F value Pr(>F)  
as.factor(group)  2  1.986  0.9930   3.061 0.0843 .
Residuals        12  3.893  0.3244                 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end{verbatim}

\begin{verbatim}
> m2=aov(y~as.factor(group)+as.factor(id),data=repmd)
> summary(m2)
                 Df Sum Sq Mean Sq F value Pr(>F)
as.factor(group)  2  1.986  0.9930   5.471 0.0318
as.factor(id)     4  2.441  0.6103   3.362 0.0679
Residuals         8  1.452  0.1815               
                  
as.factor(group) *
as.factor(id)    .
Residuals         
---
Signif. codes:  
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end{verbatim}

\hypertarget{example-factorial-design}{%
\subsection{\texorpdfstring{Example: Factorial Design\(^*\)}{Example: Factorial Design\^{}*}}\label{example-factorial-design}}

\begin{itemize}
\tightlist
\item
  Researchers wished to study the neuroprotective effect of insulin in preventing memory impairments due to exposure to anaesthesia
\item
  In one experiment they recorded the ``percentage of freezing time'' in mice following an adverse stimulus (an electric shock) under 4 different exposures determined by insulin level and anaesthesia level
\end{itemize}

\includegraphics[width=0.6\linewidth]{./11_21}

\(^*\)From project report by Preeti Bhatt based on `Unpublished data, Dr.~Patricia Roque, Khoutorsky Lab, McGill University'

\includegraphics[width=0.5\linewidth]{./11_22}

\begin{itemize}
\tightlist
\item
  The four combinations of the exposure levels are:

  \begin{itemize}
  \tightlist
  \item
    Neither Anaesthesia nor insulin
  \item
    Anaesthesia only
  \item
    Insulin only
  \item
    Anaesthesia and insulin
  \end{itemize}
\item
  The adjacent graph plots the mean percentage of freezing time in the four groups
\item
  We can see that when the mice were exposed to insulin, the impact of anaesthesia was reduced
\end{itemize}

\hypertarget{example-2-way-anova-model-2}{%
\subsection{Example: 2-way ANOVA model}\label{example-2-way-anova-model-2}}

\begin{itemize}
\tightlist
\item
  When we suspect that the two factors interact, we can extend the ANOVA model to include an interaction term as follows
\end{itemize}

\[y_{ijk}=\mu+\tau_j+\beta_i+\gamma_{ij}+\epsilon_{ijk}\]

\begin{itemize}
\tightlist
\item
  The term \(\gamma_{ij}\) is the effect of the interaction between level j of the first factor (anaesthesia) and level I of the second factor (insulin)
\item
  If there are I levels of factor i and J levels of factor j, then the number of degrees of freedom associated with the interaction terms are (I-1) X (J-1)
\item
  The first hypothesis test of interest for is whether the interaction terms are non-zero, i.e.\\
  \(H_0:\gamma_{11}=\gamma_{12}=...=\gamma_{1J}=0\), vs.\\
  Ha: At least one interaction term is non-zero
\item
  The F-test statistic is once again obtained by dividing the mean interaction sum of squares by the mean residual sum of squares
\end{itemize}

\hypertarget{example-2-way-anova-table}{%
\subsection{Example: 2-way ANOVA table}\label{example-2-way-anova-table}}

\includegraphics[width=1\linewidth]{./11_25}

\begin{itemize}
\tightlist
\item
  The F-test statistic for the interaction terms is 8.875. When compared to the \(F_{1,95}\) distribution, the p-value is 0.00442, which leads us to conclude that the there is an interaction between Insulin and Anaesthesia
\item
  The remaining p-values for the effects of Insulin and Anaesthesia are not useful in this context. Instead, we have to carry out pairwise comparisons of each factor conditional on the other
\end{itemize}

\hypertarget{example-2-way-anova-table-in-r}{%
\subsection{Example: 2-way ANOVA table in R}\label{example-2-way-anova-table-in-r}}

\begin{verbatim}
m3=aov(Freezing~Anaesthesia+Insulin+Anaesthesia*Insulin,data=freezing)

> summary(m3)
                    Df Sum Sq Mean Sq F value  Pr(>F)   
Anaesthesia          1   1228  1228.4   9.119 0.00395 **
Insulin              1   1212  1212.3   9.000 0.00417 **
Anaesthesia:Insulin  1   1195  1195.5   8.875 0.00442 **
Residuals           51   6870   134.7                   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
\end{verbatim}

\hypertarget{example-pairwise-comparisons}{%
\subsection{Example: Pairwise comparisons}\label{example-pairwise-comparisons}}

\includegraphics[width=1\linewidth]{./11_27}

\hypertarget{assumptions-behind-2-way-anova}{%
\subsection{Assumptions behind 2-way ANOVA}\label{assumptions-behind-2-way-anova}}

\begin{itemize}
\tightlist
\item
  Independence of the residuals
\item
  Equality of the variances
\item
  Normality of the residuals
\item
  As in the case of 1-way ANOVA, we may use plots of the residuals to check if these assumptions are met
\end{itemize}

\hypertarget{correlation}{%
\section{Correlation}\label{correlation}}

\begin{itemize}
\tightlist
\item
  What can we conclude from the following two graphs regarding the \textbf{association} between age and blood pressure?
\end{itemize}

\includegraphics[width=0.7\linewidth]{./11_30}

\hypertarget{association-between-age-bp}{%
\subsection{Association between age \& BP}\label{association-between-age-bp}}

\begin{itemize}
\tightlist
\item
  The two plots on the previous page are based on the same data!
\item
  Changing the scale on the ``Age'' axis could falsely lead us to believing that the relation in the second graph was stronger than the first.
\item
  Therefore we need a more objective measure that we can use to supplement the information from a scatter plot.
\item
  The estimate of the Pearson correlation coefficient between age and blood pressure in our example is r=0.718.
\end{itemize}

\hypertarget{the-correlation-coefficient}{%
\subsection{The correlation coefficient}\label{the-correlation-coefficient}}

\begin{itemize}
\tightlist
\item
  The \textbf{correlation} is a numerical measure of the \textbf{direction} and \textbf{strength} of the \textbf{linear relation} between two quantitative variables
\item
  The most commonly used approach for measuring correlation is the \textbf{product-moment} or \textbf{Pearson's} correlation coefficient
\item
  There are other methods for calculating the correlation, e.g.~the \textbf{Spearman's} correlation coefficient, is a \textbf{non-parametric} equivalent that is based on the ranks of the observations rather than on the observations themselves
\end{itemize}

\hypertarget{pearsons-correlation-coefficient}{%
\subsection{Pearson's correlation coefficient}\label{pearsons-correlation-coefficient}}

\begin{itemize}
\tightlist
\item
  This coefficient is based on the assumption of a \textbf{bivariate random sampling model}, i.e.:

  \begin{itemize}
  \tightlist
  \item
    A model under which each pair \((x_i, y_i)\) is regarded as having been randomly sampled from a population of (X, Y) pairs
  \item
    This implies that the observed x's are a random sample, and the observed y's are also a random sample
  \end{itemize}
\item
  This assumption makes the correlation coefficient unsuitable for settings where the values of one of the variables is pre-determined,

  \begin{itemize}
  \tightlist
  \item
    e.g.~where X is the fixed value of the analyte concentrations defined by a researcher to fit a standard curve for an ELISA and Y is the observed optical densities
  \end{itemize}
\item
  As with other statistics we have studied we make a distinction between the population (or true) correlation coefficient and the sample correlation coefficient that we are able to observe
\item
  \textbf{Population correlation(\(\rho\))}
\end{itemize}

\[\rho=\frac{\sum_{all\space population}(X_i-\mu_x)(Y_i-\mu_y)}{\sqrt{\sum_{all\space population}(X_i-\mu_x)^2\sum_{all\space population}(Y_i-\mu_y)^2}},-1\leq\rho\leq1\]

\begin{itemize}
\tightlist
\item
  \textbf{Sample correlation (r)} is an estimate of the population correlation
\end{itemize}

\[r=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2\sum_{i=1}^n(y_i-\bar y)^2}},-1\leq r\leq 1\]

\hypertarget{properties-of-rho}{%
\subsection{\texorpdfstring{Properties of \(\rho\)}{Properties of \textbackslash rho}}\label{properties-of-rho}}

\begin{itemize}
\tightlist
\item
  Notice that the terms in the denominator can be expressed in terms of the sample standard deviations of x and y, namely\\
  \(r=\frac{1}{(n-1)}\sum_{i=1}^n\frac{(x_i-\bar x)}{s_x}\frac{(y_i-\bar y)}{s_y}\), where \(s_x=\sqrt{\frac{\sum_{i=1}^n(x_i- \bar x)^2}{n-1}}\) and \(s_y=\sqrt{\frac{\sum_{i=1}^n(y_i-\bar y)^2}{n-1}}\)
\item
  In other words, the correlation can be expressed in terms of the standardized values of X and Y
\item
  This implies that the correlation does not change when we change the units of measurement of X or Y or both. Hence, it is unitless, unlike the covariance.
\item
  The correlation always lies between --1 and +1. Values close to zero indicate a weak linear relationship. Values close to +1 indicate a strong, positive linear relationship, i.e.~X values tend to increase with Y values. Values close to --1 indicate a strong negative relationship, i.e.~as X values increase, Y values tend to decrease and vice-versa. The extreme values of --1 and +1 occur only when the points in a scatter plot lie on a perfect straight line.
\item
  Correlation only describes the strength of the linear relationship between two variables. It cannot detect non-linear relationships no matter how strong they are.
\item
  Like the mean and standard deviation, correlation is affected by outliers and must be used with caution.
\end{itemize}

\hypertarget{scatterplots-of-data-with-a-variety-of-sample-correlation-values}{%
\subsection{Scatterplots of data with a variety of sample correlation values}\label{scatterplots-of-data-with-a-variety-of-sample-correlation-values}}

\includegraphics[width=0.5\linewidth]{./11_37}

\hypertarget{example-length-and-weight-of-snakes}{%
\subsection{Example: Length and weight of snakes}\label{example-length-and-weight-of-snakes}}

\begin{itemize}
\tightlist
\item
  In a study of a free-living population of the snake \emph{Vipera bertis}, researchers caught and measured nine adult females. Their body lengths and weights are shown in the following table
\end{itemize}

\begin{tabular}{l|r|r}
\hline
  &  Length X in cm &  Weight Y in g\\
\hline
 & 60.0 & 136.0\\
\hline
 & 69.0 & 198.0\\
\hline
 & 66.0 & 194.0\\
\hline
 & 64.0 & 140.0\\
\hline
 & 54.0 & 93.0\\
\hline
 & 67.0 & 172.0\\
\hline
 & 59.0 & 116.0\\
\hline
 & 65.0 & 174.0\\
\hline
 & 63.0 & 145.0\\
\hline
Mean & 63.0 & 152.0\\
\hline
Standard Deviation & 4.6 & 35.3\\
\hline
\end{tabular}

\hypertarget{scatterplot-of-weight-vs.-length-of-snakes}{%
\subsection{Scatterplot of weight vs.~length of snakes}\label{scatterplot-of-weight-vs.-length-of-snakes}}

\includegraphics[width=0.5\linewidth]{./11_39}

\begin{itemize}
\tightlist
\item
  The scatterplot shows a clear upward trend. We say that weight shows a positive association with length, indicating that greater lengths are associated with greater weights.
\item
  Thus, snakes that are longer than the average length of \(\overline x=63\) tend to be heavier than the average weight of \(\overline y=152\)
\item
  Note that the line superimposed on the plot is called the least-squares line or fitted regression line of Y on X. We will learn how to compute and interpret the regression line later on
\end{itemize}

\hypertarget{how-strong-is-the-linear-relationship-between-snake-length-and-weight}{%
\subsection{How strong is the linear relationship between snake length and weight?}\label{how-strong-is-the-linear-relationship-between-snake-length-and-weight}}

\begin{itemize}
\tightlist
\item
  To understand how the correlation coefficient works, rather than plotting the original data the following figure plots the standardized values of x and y (commonly referred to as z-scores)
\item
  Note that this plot looks identical to the earlier scatterplot except the scales are unit-less.
\end{itemize}

\hypertarget{scatterplot-of-standardized-values-of-weight-vs.-length-of-snakes}{%
\subsection{Scatterplot of standardized values of weight vs.~length of snakes}\label{scatterplot-of-standardized-values-of-weight-vs.-length-of-snakes}}

\includegraphics[width=0.5\linewidth]{./11_42}

\begin{itemize}
\tightlist
\item
  Dividing the plot into quadrants based on the sign of the standardized score, we see that most of these points fall into the upper-right and lower-left quadrants.
\item
  Points falling in these quadrants will have standardized scores whose products are positive. Likewise, points falling in the upper-left and lower-right quadrants will have standardized score products that are negative. Computing the sum of these products provides a numeric measure of where our points fall (i.e., which quadrants are dominant).
\item
  In our case, since there is a positive association between length and weight, most points fall in the positive product quadrants; thus, the sum of the products of standardized scores is positive. If a negative relationship were present, most of the points would fall in the negative quadrants and the sum would be negative. And, if there were no linear relationship, the points would fall in evenly in all four quadrants so that the positive and negative products would balance and their sum would be zero.
\end{itemize}

\hypertarget{example-calculating-the-correlation-coefficient}{%
\subsection{Example: Calculating the correlation coefficient}\label{example-calculating-the-correlation-coefficient}}

\includegraphics[width=1\linewidth]{./11_44}

\begin{itemize}
\tightlist
\item
  The correlation coefficient is therefore given by \(r=\frac{7.549405}{9-1}\approx 0.94\) for our example
\end{itemize}

\hypertarget{inference-about-rho-test-for-a-zero-population-correlation}{%
\subsection{\texorpdfstring{Inference about \(\rho\) : Test For A Zero Population Correlation}{Inference about \textbackslash rho : Test For A Zero Population Correlation}}\label{inference-about-rho-test-for-a-zero-population-correlation}}

\begin{itemize}
\tightlist
\item
  In a sample of n (x,y) pairs we would like to know ``Is there evidence of a non-zero correlation between X and Y?''
\item
  To test this, we can set up a null hypothesis \(H_0\) that the correlation in the population is zero and measure whether our observed r is too discrepant from \(\rho=0\) to be just sampling fluctuation
\item
  To set up this test we assume that (X,Y) follow a bivariate normal distribution
\item
  We will need to: i) define the alternative hypothesis, ii) calculate the test statistic, iii) define the rejection region, iv) calculate a p-value
\item
  The test-statistic is measured as \(s_r=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}\)
\item
  The test statistic follows a t-distribution with n-2 degrees of freedom. Accordingly, the rejection region and p-value are obtained using this distribution
\item
  The following table summarizes the rejection region and the expression for the p-value for three different possibilities for the alternative hypothesis. \(t_{n-2, α}\) refers to the quantile of the t-distribution with n-2 degrees of freedom. \(T_{n-2}\) refers to the t-distribution with n-2 degrees of freedom
\end{itemize}

\includegraphics[width=0.7\linewidth]{./11_47}

\hypertarget{example-blood-pressure-and-platelet-calcium}{%
\subsection{Example: Blood pressure and platelet calcium}\label{example-blood-pressure-and-platelet-calcium}}

\includegraphics[width=0.5\linewidth]{./11_48}

\begin{itemize}
\tightlist
\item
  It is suspected that calcium in blood platelets may be related to blood pressure.
\item
  As part of a study of this relationship, researchers recruited 38 subjects whose blood pressure was normal (i.e., not abnormally elevated).
\item
  The data appear in the adjoining scatter plot, and are suggestive of a positive relationship
\item
  The observed correlation coefficient is r=0.5832
\item
  To determine if the observed correlation provides evidence of a linear relationship we will carry out a two-sided hypothesis test
\item
  The null and alternative hypotheses may be expressed verbally as
\end{itemize}

\[H_0:Platelet\space calcium\space is\space not\space linearly\space related\space to\space blood\space pressure\]
\[vs\]
\[H_A: Platelet\space calcium\space is\space linearly\space related\space to\space blood\space pressure\]

\begin{itemize}
\tightlist
\item
  The test statistic for our example is
\end{itemize}

\[s_r=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}=\frac{0.5832\sqrt{38-2}}{\sqrt{1-0.5832^2}}=4.308\]

\begin{itemize}
\tightlist
\item
  Setting the Type I error to α=0.05, the rejection region is the region below \(t_{38-2,0.05/2} = t_{36,0.025}\) or the region above \(t_{36,0.975}\)
\item
  These quantiles can be determined using R as follows
\end{itemize}

\begin{verbatim}
qt(0.025,36)
[1] -2.028094

qt(0.975,36)
[1] 2.028094
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Since the test statistic falls in the rejection region we can say reject the null hypothesis that the population correlation coefficient is 0
\item
  We can also report a p-value together with this conclusion
\item
  Using R we can calculate \(2P(T_{n-2} ≥ |s_r|)\) as follows
\end{itemize}

\begin{verbatim}
> 2*(1-pt(4.308,36))
[1] 0.0001215702
\end{verbatim}

\begin{itemize}
\tightlist
\item
  As expected the p-value, falls below our predetermined Type I error value of 0.05
\item
  Notice that to carry out the test or calculate the p-value we only used the value or r and the sample size. The actual values of x and y were not used
\end{itemize}

\hypertarget{inferences-on-rho-1-ux3b1-confidence-interval}{%
\subsection{\texorpdfstring{Inferences on \(\rho\): (1-α)\% confidence interval}{Inferences on \textbackslash rho: (1-α)\% confidence interval}}\label{inferences-on-rho-1-ux3b1-confidence-interval}}

\begin{itemize}
\tightlist
\item
  In order to derive the test-statistic in the previous example it was important to be able to assume that \(H_0:\rho=0\)
\item
  We will now cover a method that is relevant if this null hypothesis is not of interest of if you prefer to report a confidence interval instead
\item
  This method relies on the additional assumption that the sample size is large
\item
  The confidence interval is based on the Fisher transformation of the sample correlation coefficient, r, which is defined as
\end{itemize}

\[z_r=\frac{1}{2}ln\left[\frac{1+r}{1-r}\right]\]

where ln is the natural logarithm

\begin{itemize}
\tightlist
\item
  We apply this transformation because the sampling distribution of r is skewed, but that of \(z_r\) is approximately normal when the sample size is large
\item
  The expected (or mean) value of \(z_r\) is \(\frac{1}{2}ln\left[\frac{1+\rho}{1-\rho}\right]\) and its standard deviation is \(\sqrt{\frac{1}{n-3}}\)
\item
  A (1-α)\% confidence interval for r is obtained by solving the following inequalities
\end{itemize}

\[z_r-Z_{1-\alpha/2}\sqrt{\frac{1}{n-3}}\leq\frac{1}{2}ln\left[\frac{1+\rho}{1-\rho}\right]\leq z_r+Z_{1-\alpha/2}\sqrt{\frac{1}{n-3}}\]

\hypertarget{example-blood-pressure-and-platelet-calcium-1}{%
\subsection{Example: Blood pressure and platelet calcium}\label{example-blood-pressure-and-platelet-calcium-1}}

\begin{itemize}
\tightlist
\item
  The Fisher transformation of the sample correlation coefficient is
\end{itemize}

\[z_r=\frac{1}{2}ln\left[\frac{1+r}{1-r}\right]=\frac{1}{2}ln\left[\frac{1+0.5832}{1-0.5832}\right]=0.6673\]

\begin{itemize}
\tightlist
\item
  A 95\% confidence interval for \(z_r\) is given by
\end{itemize}

\[0.6673 \pm 1.96\sqrt{\frac{1}{38-3}}=0.6673\pm0.3313\]

which is (0.3360, 0.9986)

\begin{itemize}
\item
  Setting\\
  \(\frac{1}{2}ln\left[\frac{1+\rho}{1-\rho}\right]=0.3360\) gives \(\rho=\frac{e^{2(0.3360)}-1}{e^{2(0.3360)}+1}=0.32\)\\
  \(\frac{1}{2}ln\left[\frac{1+\rho}{1-\rho}\right]=0.9986\) gives \(\rho=\frac{e^{2(0.9986)}-1}{e^{2(0.9986)}+1}=0.76\)
\item
  Thus 95\% confidence interval for \(\rho\) is (0.32, 0.76)
\end{itemize}

\hypertarget{correlation-coefficient-in-r}{%
\subsection{Correlation coefficient in R}\label{correlation-coefficient-in-r}}

\begin{itemize}
\tightlist
\item
  You may wish to use a software program to calculate the correlation coefficient, particularly when the sample size is very large
\item
  The cor.test function in R can be used for obtaining the value of the correlation and also for carrying out a hypothesis test or obtaining a confidence interval
\end{itemize}

\begin{verbatim}
cor.test(x, ...)

## Default S3 method:
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, ...)
\end{verbatim}

In the following section on simple linear regression we will see an example where this function is used

\hypertarget{lecture-12-simple-and-multiple-linear-regression}{%
\chapter{Lecture 12: Simple and Multiple Linear Regression}\label{lecture-12-simple-and-multiple-linear-regression}}

\hypertarget{correlation-and-simple-linear-regression}{%
\section{Correlation and Simple Linear Regression}\label{correlation-and-simple-linear-regression}}

\begin{itemize}
\tightlist
\item
  Both are methods used for studying the association between two continuous variables

  \begin{itemize}
  \tightlist
  \item
    Correlation provides a measure of the strength of the association
  \item
    In addition, simple linear regression allows us to predict the `outcome' using the `predictor' variable
  \end{itemize}
\end{itemize}

\hypertarget{simple-and-multiple-linear-regression}{%
\subsection{Simple and Multiple Linear Regression}\label{simple-and-multiple-linear-regression}}

\begin{itemize}
\tightlist
\item
  This lecture will cover methods for

  \begin{itemize}
  \tightlist
  \item
    Estimation and inference for the best fitting straight line involving either one or two predictors
  \item
    Checking the assumptions of the linear regression model
  \item
    Using the linear regression model for prediction
  \end{itemize}
\end{itemize}

\hypertarget{research-question}{%
\subsection{Research question}\label{research-question}}

Is walking capacity at 3 months post-stroke associated with physical function at 3 months post-stroke?

\includegraphics[width=0.5\linewidth]{./12_4}

This question could be answered by calculating a correlation coefficient or fitting a simple linear regression model

Defining a theoretical model prior to setting up a statistical model can help determine which statistic is more appropriate

\hypertarget{terminologies}{%
\subsection{Terminologies}\label{terminologies}}

\begin{tabular}{l|l|l}
\hline
Discipline & Cause & Effect\\
\hline
Epidemiology & Exposure & Outcomes\\
\hline
Medical/clinical & Risk factor & Disease\\
\hline
Statistics & Independent & Dependent\\
\hline
Psychology & Stimulus & Response\\
\hline
Mathematical & X & Y\\
\hline
\end{tabular}

\hypertarget{finch-mayo-dataset}{%
\subsection{Finch-Mayo Dataset}\label{finch-mayo-dataset}}

\begin{itemize}
\tightlist
\item
  We will use the data from the Finch-Mayo study on rehabilitation after stroke
\item
  Who are these people:

  \begin{itemize}
  \tightlist
  \item
    Recruited within 72 hours of confirmed stroke using WHO definition
  \item
    Excluded: TIA, not confirmed stroke or non-vascular causes
  \item
    Cognitive or comprehension impairments (MMSE)
  \item
    Altered consciousness
  \end{itemize}
\item
  Data collection (time points)

  \begin{itemize}
  \tightlist
  \item
    Baseline (within 3 days): observed performance on specific tasks and rate their difficulty on those tasks
  \item
    Follow-up (at three months): same tasks + rating on activities and participation
  \end{itemize}
\end{itemize}

\hypertarget{knowing-the-type-of-outcome-measure-can-help-specify-the-theoretical-model}{%
\subsection{Knowing the type of outcome measure can help specify the theoretical model}\label{knowing-the-type-of-outcome-measure-can-help-specify-the-theoretical-model}}

\includegraphics[width=0.5\linewidth]{./12_7}

Ref: Terminology proposed to Measure What Matters in Health: Journal of Clin Rehab. Mayo NE et. al (2017)

\hypertarget{research-question-1}{%
\subsection{Research question}\label{research-question-1}}

Is walking capacity at 3 months post-stroke associated with physical function at 3 months post-stroke?

\includegraphics[width=0.5\linewidth]{./12_4}

This question could be answered by calculating a correlation coefficient or fitting a simple linear regression model

The variables X and Y are interchangeable when calculating the correlation. But they have specific roles in a regression model
* X is the \textbf{independent} or \textbf{predictor} or \textbf{explanatory} variable
* Y is the \textbf{dependent* or }outcome** or \textbf{response} variable

\hypertarget{measurement-scale}{%
\subsection{Measurement scale}\label{measurement-scale}}

\begin{itemize}
\tightlist
\item
  Physical Function (X): Physical Function Index of SF-36. Self-report questionnaire that asks limitations on certain activities (walking a block, climbing stairs etc.)
\end{itemize}

\includegraphics[width=0.7\linewidth]{./12_9}

\begin{itemize}
\tightlist
\item
  Walking Capacity (Y): Distance covered in 2 minutes
\end{itemize}

\hypertarget{walking-capacity---physical-function}{%
\subsection{Walking capacity \textless- Physical Function?}\label{walking-capacity---physical-function}}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Draw a rough scatter plot to visually examine the relation between Walking Capacity and Physical Function
\item
  Calculate the regression line of Walking Capacity on Physical Function
\item
  What is the estimate of \(\sigma\), the residual standard deviation? Calculate a 95\% confidence interval for the slope, \(\beta\) of the line in (b).
\item
  What difference in Walking Capacity do we expect between patients who differ by 1 standard deviation of Physical Function? Provide a 95\% confidence interval for this estimate.
\item
  Give your prediction for the average Walking Capacity in a group of stroke patients with Physical Function = 50.
\item
  What is the 95\% confidence interval around your answer in (e)?
\item
  What would be 95\% confidence interval for the predicted Walking Capacity of an individual stroke patient with Physical Function = 50?
\item
  Examine the graph of the residuals. Does a linear regression seem appropriate?
\end{enumerate}

\hypertarget{descriptive-statistics-and-graphs}{%
\subsection{Descriptive statistics and graphs}\label{descriptive-statistics-and-graphs}}

\begin{itemize}
\tightlist
\item
  We will begin by looking at the descriptive statistics and some descriptive graphs
\item
  225 patients have observations on both variables
\end{itemize}

\includegraphics[width=1\linewidth]{./12_12}

\hypertarget{histogram-of-walking-capacity}{%
\subsection{Histogram of walking capacity}\label{histogram-of-walking-capacity}}

\includegraphics[width=1\linewidth]{./12_13}

\hypertarget{walking-capacity-vs.-physical-function}{%
\subsection{Walking capacity vs.~Physical Function}\label{walking-capacity-vs.-physical-function}}

\includegraphics[width=1\linewidth]{./12_14}

\hypertarget{correlation-between-physical-function-and-walking-capacity}{%
\subsection{Correlation between physical function and walking capacity}\label{correlation-between-physical-function-and-walking-capacity}}

\begin{verbatim}
>a=read.csv("FinchMayoStrokeData.csv",header=T)
> cor.test(a$PFI,a$s3tmwt)

Pearson's product-moment correlation

data:  a$PFI and a$s3tmwt
t = 18.7, df = 223, p-value < 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.7246944 0.8276250
sample estimates:
      cor 
0.7814195 
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The sample correlation is approximately 0.78, implying there is a positive, linear relationship between physical function and walking capacity
\item
  The very low p-value tells us there is enough evidence to reject the null hypothesis that the population correlation coefficient is 0
\item
  The 95\% confidence interval ranges from 0.72 to 0.83, approximately
\end{itemize}

\hypertarget{results-of-simple-linear-regression-walking-capacity---pfi}{%
\subsection{Results of simple linear regression: Walking capacity \textless- PFI}\label{results-of-simple-linear-regression-walking-capacity---pfi}}

\hypertarget{output-from-r-1}{%
\subsubsection{Output from R}\label{output-from-r-1}}

\begin{verbatim}
Residuals:
     Min       1Q   Median       3Q      Max 
-124.630  -17.474   -3.306   21.877  138.863 

Coefficients:
                  Estimate   Std. Error t value Pr(>|t|)    
(Intercept)       17.09534    4.52145   3.781   0.000201 ***
a$PFI             1.43379     0.07667   18.700  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 37.61 on 223 degrees of freedom
  (10 observations deleted due to missingness)
Multiple R-squared: 0.6106,     Adjusted R-squared: 0.6089 
F-statistic: 349.7 on 1 and 223 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{simple-linear-regression}{%
\subsection{Simple Linear Regression}\label{simple-linear-regression}}

\begin{itemize}
\tightlist
\item
  Regression analysis is a statistical methodology for studying the relationship between a response variable, Y, and one or more explanatory variables
\item
  Simple linear regression is the special case when we have a single explanatory variable, X, and the functional relation between X and Y is linear.

  \begin{itemize}
  \tightlist
  \item
    When X is categorical, we have an ANOVA model
  \end{itemize}
\item
  A simple linear regression equation has a form:
\end{itemize}

\[Y_i=\alpha+\beta X_i+\epsilon_i\]

\begin{itemize}
\tightlist
\item
  model: the intercept (\(\alpha\)) and the slope (\(\beta\)).
\item
  The intercept (\(\alpha\)) is the value of the response variable when the predictor variable is exactly equal to zero. This parameter is of interest only when zero lies in the range of the predictor variable.
\item
  The slope (\(\beta\)) measures the increase in the response variable corresponding to a unit increase in the predictor variable.
\end{itemize}

\hypertarget{comparison-of-regression-lines}{%
\subsection{Comparison of Regression Lines}\label{comparison-of-regression-lines}}

\includegraphics[width=0.7\linewidth]{./12_19}

\hypertarget{simple-linear-regression-model}{%
\subsection{Simple Linear Regression Model}\label{simple-linear-regression-model}}

\begin{itemize}
\tightlist
\item
  In the simple linear regression model the response variable, \(Y_i\), is expressed as the sum of two components:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the \textbf{constant} term \(\alpha+\beta X_i\), and\\
\item
  the \textbf{random} error \(\epsilon_i\), where \(\epsilon_i\) \textasciitilde{} \(N(0,\sigma^2)\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Hence \(Y_i\) is a random variable. We express this as\\
  \(Y_i | X_i\) \textasciitilde{} \(N(\alpha+\beta X_i,\sigma^2)\)
\end{itemize}

In words, we say \(Y_i\) given \(X_i\) follows a normal distribution with\\
\(E(Y_i | X_i ) =\alpha+\beta X_i\) and \(Variance(Y_i|X_i)=\sigma^2\)

\hypertarget{illustration-of-assumptions-behind-simple-linear-regression}{%
\subsection{Illustration of assumptions behind Simple Linear Regression}\label{illustration-of-assumptions-behind-simple-linear-regression}}

\includegraphics[width=1\linewidth]{./12_21}

\hypertarget{assumptions-involved-in-simple-linear-regression}{%
\subsection{Assumptions involved in Simple Linear Regression}\label{assumptions-involved-in-simple-linear-regression}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each observed value of the response variable Y is independent of the other values of the response variable
\item
  There is an approximate linear relationship between the response variable and the explanatory variable
\item
  The standard deviation of the error random variable is exactly the same for each value of the explanatory variable
\item
  The probability distribution of the error random variable is normally distributed with mean 0 and standard deviation σ
\item
  The observed values of the explanatory variable X are measured without error
\end{enumerate}

\hypertarget{simple-linear-regression-1}{%
\subsection{Simple Linear Regression}\label{simple-linear-regression-1}}

\begin{itemize}
\tightlist
\item
  As usual we represent the ``population'' and ``sample'' regression lines using Greek and ordinary alphabets as follows\\
  True values: \(Y_i=\alpha+\beta X_i+\epsilon_i\)\\
  Fitted values: \(\hat{Y_i}=a+bX_i+e_i\)
\item
  By what criterion do we judge what is the best estimated line, i.e.~the best values for ``a'' and ``b''?
\item
  The \(i^{th}\) residual is \(e_i = Y_i-\hat{Y_i}\). If the line was a perfect fit all the residuals would be 0. Clearly, the best ``a'' and ``b'' would approach a line of perfect fit as far as possible.
\item
  Which of the following three criteria would be suitable for defining the ``best'' regression line?
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
  Line which minimizes \(\sum_{all\space points}(perpendicular\space distance\space to\space line)\)
\item
  Line which minimizes \(\sum_{all\space points}(vertical\space distance\space to\space line)\)
\item
  Line which minimizes \(\sum_{all\space points}(vertical\space distance\space to\space line)^2\)
\end{enumerate}

Method (c) is known as the \textbf{Method of Least Squares} and is widely used to estimate ``a'' and ``b''.

\hypertarget{residual}{%
\subsection{Residual}\label{residual}}

\includegraphics[width=0.5\linewidth]{./12_25}

\begin{itemize}
\tightlist
\item
  The figure above illustrates that the residual is the vertical distance between \(Y_i\) and the best fitting straight line
\item
  We do not use the perpendicular distance between \(Y_i\) and the line because that would be a function of X, and X is fixed
\item
  The sum of the vertical distances would give a line with a=1 and b=0, i.e.~a line where there is no association between x and y
\item
  Therefore, we work with the criterion that minimizes the sum of the squared vertical distances or residuals
\end{itemize}

\hypertarget{simple-linear-regression-2}{%
\subsection{Simple Linear Regression}\label{simple-linear-regression-2}}

\begin{itemize}
\tightlist
\item
  The following slides provide mathematical expressions for estimating the slope, intercept, the predicted value of the outcome.
\item
  Expressions for their confidence intervals are also provided
\item
  You can also obtain these results directly from R without performing any calculation by hand
\end{itemize}

\hypertarget{estimating-a-and-b-using-the-method-of-least-squares}{%
\subsection{Estimating a and b using the method of least squares}\label{estimating-a-and-b-using-the-method-of-least-squares}}

\begin{itemize}
\tightlist
\item
  Using calculus and algebra we can find ``a'' and ``b'' which minimize the residual sum of squares \(\sum_{i=1}^n(Y_i-a-bX_i)^2\) the solutions are as follows:
\end{itemize}

\(a=\bar Y-b\bar X\)

where

\(b = \frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^n(x_i-\bar x)^2}\) OR \(\frac{n\sum_{i=1}^nx_iy_i-(\sum_{i=1}^nx_i)(\sum_{i=1}^ny_i)}{n\sum_{i=1}^nx_i^2-(\sum_{i=1}^nx_i)^2}\)

\hypertarget{relation-between-slope-and-correlation-coefficient}{%
\subsection{Relation between slope and correlation coefficient}\label{relation-between-slope-and-correlation-coefficient}}

\begin{itemize}
\tightlist
\item
  Notice that the slope b can be expressed in terms of the correlation coefficient, \(r=b\frac{s_x}{s_y}\)
\item
  This implies that the sign of the slope is the same as that of the correlation
\end{itemize}

\hypertarget{estimating-the-error-variance}{%
\subsection{Estimating the error variance}\label{estimating-the-error-variance}}

\begin{itemize}
\tightlist
\item
  Having estimated ``a'' and ``b'' we can now estimate 2,the assumed common variance, as follows
\end{itemize}

\[\hat{\sigma}^2=\frac{Residual\space sum\space of\space squares}{n-2}=\frac{\sum_{i=1}^n(Y_i-a-bX_i)^2}{n-2}\]

\begin{itemize}
\tightlist
\item
  Standard errors of a and b are functions of \(\hat{\sigma}^2\). They can be used to define confidence intervals and hypothesis tests for the intercept and slope parameters, respectively
\end{itemize}

\hypertarget{expressions-for-standard-errors-and-confidence-intervals-for-parameter-estimates}{%
\subsection{Expressions for standard errors and confidence intervals for parameter estimates}\label{expressions-for-standard-errors-and-confidence-intervals-for-parameter-estimates}}

\begin{tabular}{l|l}
\hline
Standard error & Confidence interval\\
\hline
\$se(a)=\textbackslash{}hat\{\textbackslash{}sigma\}\textbackslash{}sqrt\{\textbackslash{}frac\{1\}\{n\}+\textbackslash{}frac\{\textbackslash{}bar x\textasciicircum{}2\}\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}n(x\_i-\textbackslash{}bar x)\textasciicircum{}2\}\}\$ & \$a\textbackslash{}pm t\_\{1-\textbackslash{}alpha/2,n-2\}\textbackslash{}times se(a)\$\\
\hline
\$se(b)=\textbackslash{}frac\{\textbackslash{}hat\{\textbackslash{}sigma\}\}\{\textbackslash{}sqrt\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}n(x\_i-\textbackslash{}bar x)\textasciicircum{}2\}\}\$ & \$b\textbackslash{}pm t\_\{1-\textbackslash{}alpha/2,n-2\}\textbackslash{}times se(b)\$\\
\hline
se(expected value of y at x)\$=\textbackslash{}hat\{\textbackslash{}sigma\}\textbackslash{}sqrt\{\textbackslash{}frac\{1\}\{n\}+\textbackslash{}frac\{(x-\textbackslash{}bar x)\textasciicircum{}2\}\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}n(x\_i-\textbackslash{}bar x)\textasciicircum{}2\}\}\$ & \$\textbackslash{}hat\{y\}\_i\textbackslash{}pm t\_\{1-\textbackslash{}alpha/2,n-2\}\$se(expected value of y at x)\\
\hline
se(value of y at x)\$=\textbackslash{}hat\{\textbackslash{}sigma\}\textbackslash{}sqrt\{1+\textbackslash{}frac\{1\}\{n\}+\textbackslash{}frac\{(x-\textbackslash{}bar x)\textasciicircum{}2\}\{\textbackslash{}sum\_\{i=1\}\textasciicircum{}n(x\_i-\textbackslash{}bar x)\textasciicircum{}2\}\}\$ & \$\textbackslash{}hat\{y\}\_i\textbackslash{}pm t\_\{1-\textbackslash{}alpha/2,n-2\}\$se(value of y at x)\\
\hline
\end{tabular}

\hypertarget{hypothesis-tests-for-regression-coefficients}{%
\subsection{Hypothesis tests for regression coefficients}\label{hypothesis-tests-for-regression-coefficients}}

\begin{itemize}
\tightlist
\item
  To test \(H_0:\alpha=\alpha_0\), use the fact that \(\frac{\alpha-\alpha_0}{se(a)}\) \textasciitilde{} \(t_{n-2}\)
\item
  Similarly for \(\beta\), to test \(H_0:\beta=\beta_0\), use the fact that \(\frac{\beta-\beta_0}{se(b)}\) \textasciitilde{} \(t_{n-2}\)
\end{itemize}

\hypertarget{estimating-the-regression-line}{%
\subsection{Estimating the regression line}\label{estimating-the-regression-line}}

Let X = PFI and Y = Walking Capacity\\
\(b=\frac{\sum_{i=1}^{225}(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^{225}(x_i-\bar x)^2}=\) 1.43m/unit change in PFI

\begin{itemize}
\tightlist
\item
  \(\bar y = 49.07\) and \(\bar x = 87.46\)\\
  \(\therefore a=\bar Y-b\bar X=49.07-1.43(87.46)=17.10\space m\)
\item
  Thus the regression line is: Walking Capacity = 17.10 + 1.43 Physical Function
\item
  An increase of 1 unit in Physical Function is associated with an increase of 1.43 metres in walking capacity
\end{itemize}

\hypertarget{best-fitting-straight-line}{%
\subsection{Best fitting straight line}\label{best-fitting-straight-line}}

\includegraphics[width=1\linewidth]{./12_33}

\hypertarget{confidence-interval-for-b}{%
\subsection{Confidence interval for b}\label{confidence-interval-for-b}}

\[\hat{\sigma}^2=\frac{\sum_{i=1}^{225}(Y_i-a-bX_i)^2}{n-2}=1414.51\space m^2\]
\[\rightarrow \hat{\sigma}=37.61\space m\]

95\% CI for the slope b is given by: \(b\pm t_{0.975,223}\times\frac{\hat{\sigma}}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}}\)

\(=1.43\pm(1.97)\frac{37.61}{\sqrt{240626.7}}\)

\(=1.43\pm0.15=\) (1.27m/PF-unit, 1.58m/PF-unit)

\hypertarget{change-in-walking-capacity-for-a-1-sd-change-in-physical-function}{%
\subsection{Change in walking capacity for a 1 SD change in physical function}\label{change-in-walking-capacity-for-a-1-sd-change-in-physical-function}}

\begin{itemize}
\tightlist
\item
  The expected difference in Walking Capacity between individuals whose physical function differs by 32.49 (i.e.~a difference of 1 standard deviation)\\
  = 1.43 × 32.49 = 46.46
\item
  Standard Error of this estimate\\
  = SE(b) × 32.49 = 0.08 × 32.49 = 2.60
\item
  The 95\% confidence interval for the difference is\\
  46.46 ± 1.97(2.60) = (41.34, 51.58)
\end{itemize}

\hypertarget{estimating-average-walking-capacity-for-patients-with-physical-function50}{%
\subsection{Estimating average Walking Capacity for patients with Physical Function=50}\label{estimating-average-walking-capacity-for-patients-with-physical-function50}}

\begin{itemize}
\tightlist
\item
  The average Walking Capacity among stroke patients with a Physical Function value of 50 can be estimated from the regression equation as:
\end{itemize}

\[Walking\space Capacity_{50}=17.10+1.43(50)=88.60\space m\]

\hypertarget{ci-for-average-walking-capacity-among-patients-with-pf50}{%
\subsection{95\% CI for average Walking Capacity among patients with PF=50}\label{ci-for-average-walking-capacity-among-patients-with-pf50}}

The 95\% CI for \(BP_{50}\) is given by:

\[\hat{y}_i\pm t_{1-\alpha/2,n-2}\times\sigma\sqrt{\frac{1}{n}+\frac{(x-\bar x)^2}{\sum_{i=1}^n(x_i-\bar x)^2}}\]
\[=88.60\pm t_{0.975,223}\times 37.61\sqrt{\frac{1}{225}+\frac{(50-49.03)^2}{240626.7}}\]
\[=88.60\pm4.94\]
\[=(83.66,93.54)\]

\hypertarget{ci-for-predicted-walking-capacity-in-an-individual-patient-with-pf50}{%
\subsection{95\% CI for predicted Walking Capacity in an individual patient with PF=50}\label{ci-for-predicted-walking-capacity-in-an-individual-patient-with-pf50}}

The 95\% CI for \(BP_{50}\) is given by:

\[\hat{y}_i\pm t_{1-\alpha/2,n-2}\times\sigma\sqrt{1+\frac{1}{n}+\frac{(x-\bar x)^2}{\sum_{i=1}^n(x_i-\bar x)^2}}\]
\[=88.60\pm t_{0.975,223}\times 37.61\sqrt{1+\frac{1}{225}+\frac{(50-49.03)^2}{240626.7}}\]
\[=88.60\pm74.25\]
\[=(14.34,162.86)\]

\hypertarget{regression-diagnostics-using-residuals}{%
\subsection{Regression diagnostics using residuals}\label{regression-diagnostics-using-residuals}}

\begin{itemize}
\tightlist
\item
  The appropriateness of the linear regression function can be checked informally using diagnostic plots of the residuals against X or the fitted values \((\hat{y})\). These plots can be used to see if\\
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  the residuals are randomly distributed about zero,\\
\item
  the error variance is constant for all observations,\\
\item
  an additional non-linear term would improve the fit of the model, and\\
\item
  there are any outliers.
\end{enumerate}

\hypertarget{plot-of-residuals-vs.-predicted-value-haty-or-exposure-x}{%
\subsection{\texorpdfstring{Plot of residuals vs.~predicted value \((\hat{y})\) or exposure (X)}{Plot of residuals vs.~predicted value (\textbackslash hat\{y\}) or exposure (X)}}\label{plot-of-residuals-vs.-predicted-value-haty-or-exposure-x}}

\begin{itemize}
\tightlist
\item
  If the assumptions of the simple linear regression model are met, the residual should be completely independent of X or of the predicted value.
\item
  If we see a pattern then it is an indication of a problem
\end{itemize}

\hypertarget{residual-plot-prototype-a-ideal-situation-no-apparent-pattern}{%
\subsection{Residual plot: Prototype (a), Ideal Situation: No apparent pattern}\label{residual-plot-prototype-a-ideal-situation-no-apparent-pattern}}

\includegraphics[width=1\linewidth]{./12_41}

\hypertarget{residual-plot-prototype-b-suggests-that-the-linear-model-is-inappropriate.-quadratic-model-with-x2-may-be-needed}{%
\subsection{\texorpdfstring{Residual plot: Prototype (b), Suggests that the linear model is inappropriate. Quadratic model (with \(x^2\)) may be needed}{Residual plot: Prototype (b), Suggests that the linear model is inappropriate. Quadratic model (with x\^{}2) may be needed}}\label{residual-plot-prototype-b-suggests-that-the-linear-model-is-inappropriate.-quadratic-model-with-x2-may-be-needed}}

\includegraphics[width=1\linewidth]{./12_42}

\hypertarget{how-does-the-non-linear-pattern-arise}{%
\subsection{How does the non-linear pattern arise?}\label{how-does-the-non-linear-pattern-arise}}

\begin{itemize}
\tightlist
\item
  If the true model is non-linear, for example:

  \begin{itemize}
  \tightlist
  \item
    True model: \(y_i=\alpha+\beta x_i+\gamma x_i^2+\epsilon_i\)
  \item
    Fitted (incorrect) model: \(\hat{y}_i=a+bx_i\)
  \end{itemize}
\item
  \(Residual=y_i-\hat{y}_i=(\alpha-a)+(\beta-b)x+\gamma x_i^2+\epsilon_i\)
\item
  Therefore, the residual is a non-linear function of x as can be seen from prototype (b).
\end{itemize}

\hypertarget{residual-plot-prototype-c-plot-suggests-variance-is-not-constant.-transformation-may-help.}{%
\subsection{Residual plot: Prototype (c), Plot suggests variance is not constant. Transformation may help.}\label{residual-plot-prototype-c-plot-suggests-variance-is-not-constant.-transformation-may-help.}}

\includegraphics[width=1\linewidth]{./12_44}

\hypertarget{residual-plot-for-walking-capacity-vs.-physical-function-model}{%
\subsection{Residual plot for Walking Capacity vs.~Physical Function model}\label{residual-plot-for-walking-capacity-vs.-physical-function-model}}

\includegraphics[width=1\linewidth]{./12_45}

\begin{itemize}
\tightlist
\item
  From the residual plot we can see that the residuals are randomly dispersed around zero. The absence of any pattern here indicates that the linear regression is an appropriate fit.
\item
  However, a plot of the standardized residuals indicates there may be a few outliers.
\end{itemize}

\hypertarget{what-is-the-impact-of-an-outlier}{%
\subsection{What is the impact of an outlier?}\label{what-is-the-impact-of-an-outlier}}

\begin{itemize}
\tightlist
\item
  The influence exerted by an outlier depends both on its X and Y values. As the sample size increases, its influence decreases
\item
  When an outlier has an X value far from the mean it primarily influences the slope. Outliers close to the mean of X influence the intercept
\end{itemize}

\hypertarget{what-should-we-do-with-an-outlier}{%
\subsection{What should we do with an outlier?}\label{what-should-we-do-with-an-outlier}}

\begin{itemize}
\tightlist
\item
  Check if it is a recording error
\item
  Fit model with and without it and examine its impact on intercept and slope estimates

  \begin{itemize}
  \tightlist
  \item
    If they do not change, the outlier is of no consequence
  \item
    If they do change, more must be done to resolve the problem
  \end{itemize}
\item
  E.g. Refit model of Walking Capacity \textless- Physical Function after removing observations
\end{itemize}

\hypertarget{qq-plot-to-see-if-residuals-are-normally-distributed}{%
\subsection{QQ-plot to see if residuals are normally distributed}\label{qq-plot-to-see-if-residuals-are-normally-distributed}}

\begin{itemize}
\tightlist
\item
  To see if the residuals are normally distributed we use a quantile-quantile (Q-Q) plot to compare the quantiles of the standardized residuals to those of the standard normal distribution

  \begin{itemize}
  \tightlist
  \item
    If the points lie along the reference line then we can reasonably assume that the residuals are normally distributed
  \item
    This is important to ensure validity of all inferences based on the model (confidence intervals, hypothesis tests, prediction intervals)
  \end{itemize}
\item
  A concave or convex pattern suggests the residuals follow a skewed distribution
\item
  An S-shaped pattern suggests that the residuals have either shorter tails of longer tails than the normal distribution
\item
  If there appears to be a severe deviation from normality:

  \begin{itemize}
  \tightlist
  \item
    A statistical test may be needed to test whether there is a statistically significant deviation
  \item
    a transformation may be required
  \end{itemize}
\end{itemize}

\hypertarget{q-q-plot-for-walking-capacity-vs.-physical-function-model}{%
\subsection{Q-Q plot for Walking Capacity vs.~Physical Function model}\label{q-q-plot-for-walking-capacity-vs.-physical-function-model}}

\includegraphics[width=1\linewidth]{./12_51}

\hypertarget{what-does-the-qq-plot-tell-us}{%
\subsection{What does the QQ plot tell us?}\label{what-does-the-qq-plot-tell-us}}

\begin{itemize}
\tightlist
\item
  For the model of Walking Capacity vs.~PFI model the QQ plot suggests that the tails of the residual distribution are wider than those of a standard normal distribution.

  \begin{itemize}
  \tightlist
  \item
    Due to the relatively large sample size we would not expect this to have much impact
  \end{itemize}
\end{itemize}

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of fit}\label{goodness-of-fit}}

\includegraphics[width=1\linewidth]{./12_53}

\begin{itemize}
\tightlist
\item
  Let's say that instead of estimating ``a'' and ``b'' we simply estimated Yi by \(\bar y\). This would be equivalent to assuming a=\(\bar y\) and b=0. In other words, there is no information gained from X on the distribution of Y. In that case the residual sum of squares would reduce to
\end{itemize}

\[\sum_{i=1}^n(Y_i-\hat{y})^2=\sum_{i=1}^n(Y_i-\bar{y})^2\]

which is called the \textbf{Total Sum of Squares}. This is the maximum possible value of the residual sum of squares.

\begin{itemize}
\tightlist
\item
  A commonly used method to estimate the utility of our linear regression line is to estimate the proportion of the total variation in Y that is ``explained'' by X. This can be estimated as follows:
\end{itemize}

\[R^2=1-\frac{Residual\space sum\space of\space squares}{Total\space sum\space of\space squares}\]

\begin{itemize}
\tightlist
\item
  If the Total Sum of Squares is much larger than the Residual Sum of Squares then we can say we have a ``Good Fit''. Note that Pearson's correlation (r) is related to \(R^2\) as follows: \(r=\pm\sqrt{R^2}\)
\item
  For our example we can find the value of \(R^2\) in the R output to be 0.6106
\item
  We can verify that the square root of 0.6106 is roughly 0.78 or the correlation coefficient
\end{itemize}

\hypertarget{multiple-linear-regression-conceptual-model}{%
\subsection{Multiple Linear Regression: Conceptual Model}\label{multiple-linear-regression-conceptual-model}}

\includegraphics[width=1\linewidth]{./12_56}

\hypertarget{self-perceived-health---physical-function-covariates}{%
\subsection{Self-Perceived Health \textless- Physical Function + covariates}\label{self-perceived-health---physical-function-covariates}}

\begin{itemize}
\tightlist
\item
  \textbf{Outcome}: Self-Perceived Health
\item
  \textbf{Principle explanatory variable}: Physical Function
\item
  \textbf{Confounding variables}: Walking capacity, Gender
\item
  \textbf{Effect modifier}: Mental Health
\item
  \textbf{Possibly Collinear variables}: Physical Function and Walking Capacity
\end{itemize}

\hypertarget{exploratory-analyses}{%
\subsection{Exploratory analyses}\label{exploratory-analyses}}

\begin{itemize}
\tightlist
\item
  Increasing, linear relation between Self-perceived health and Physical function
\item
  Possible confounding variable: Walking capacity
\item
  Possible effect modifier: Mental function
\item
  Possible multicollinearity problems: (physical function, walking capacity), (physical function, mental function), (walking capacity, mental function)
\end{itemize}

\hypertarget{multiple-linear-regression-model}{%
\subsection{Multiple Linear Regression Model}\label{multiple-linear-regression-model}}

\begin{verbatim}
Call:
lm(formula = a$eqtherm ~ PFI + MHI + PFI * MHI + s3tmwt + gender, 
    data = a, subset = -155)

Residuals:
    Min      1Q  Median      3Q     Max 
-54.524 -12.119   2.276  10.261  39.790 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 64.890092   6.342228  10.231  < 2e-16 ***
PFI         -0.291713   0.141446  -2.062  0.04045 *  
MHI          0.008999   0.092106   0.098  0.92227    
s3tmwt       0.024943   0.030055   0.830  0.40757    
gender      -2.797760   2.461413  -1.137  0.25703    
PFI:MHI      0.005343   0.001757   3.040  0.00267 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 16.49 on 203 degrees of freedom
  (25 observations deleted due to missingness)
Multiple R-squared: 0.2171,     Adjusted R-squared: 0.1978 
F-statistic: 11.26 on 5 and 203 DF,  p-value: 1.358e-09 
\end{verbatim}

\hypertarget{results-of-simple-and-multiple-linear-regression-models}{%
\subsection{\texorpdfstring{Results\(^*\) of simple and multiple linear regression models}{Results\^{}* of simple and multiple linear regression models}}\label{results-of-simple-and-multiple-linear-regression-models}}

\begin{tabular}{l|l|l}
\hline
Variable & Simple Linear Regression & Multiple Linear Regression\\
\hline
Physical Function & 0.2 (0.1, 0.3) & -0.29 (-0.57, -0.12)\\
\hline
Mental Health & 0.3 (0.2, 0.4) & 0.008 (-0.17, 0.19)\\
\hline
Walking Capacity & 0.1 (0.05, 0.13) & 0.02 (-0.03, 0.1)\\
\hline
Gender & -3.9 (-9.5, 0.5) & -2.8 (-7.7, 2.1)\\
\hline
Stroke Severity & 1.0 (0.03, 1.0) & --\\
\hline
Physical Function \$\textasciicircum{}*\$ Mental Health & -- & 0.005 (0, 0.008)\\
\hline
\end{tabular}

\(^*\)Slope and 95\% confidence interval

\hypertarget{interpretation-of-regression-coefficients-in-a-model-with-interaction-terms}{%
\subsection{Interpretation of regression coefficients in a model with interaction terms}\label{interpretation-of-regression-coefficients-in-a-model-with-interaction-terms}}

\begin{itemize}
\tightlist
\item
  Y = b0 + b1\(^*\)X1 + b2\(^*\)X2 + b3\(^*\)X1\(^*\)X2 + b4\(^*\)X3\\
  = b0 + (b1 + b3\(^*\)X2)\(^*\)X1 + b2\(^*\)X2 + b4\(^*\)X3\\
  = b0 + b1\(^*\)X1 + (b2 + b3\(^*\)X1)\(^*\)X2 + b4\(^*\)X3
\end{itemize}

Change in Y for unit change in X3 = b4

Change in Y for unit change in X1 = b1 + b3\(^*\)X2

Change in Y for unit change in X2 = b2 + b3\(^*\)X1

\begin{itemize}
\tightlist
\item
  Intercept is meaningful if all covariates can be simultaneously 0. If not, it only plays a role in calculating the predicted values
\item
  Slope = Change in outcome for a unit change in explanatory variable, while all other variables remain constant

  \begin{itemize}
  \tightlist
  \item
    e.g.~Increase in 1m on walking capacity test results in increase in 0.02 units in self-perceived health, holding other variables constant
  \item
    e.g.~Women score -2.8 units lower on the self-perceived health scale than men, holding other variables constant
  \end{itemize}
\end{itemize}

\hypertarget{interpretation-of-coefficients-of-interaction-terms}{%
\subsection{Interpretation of coefficients of interaction terms}\label{interpretation-of-coefficients-of-interaction-terms}}

\begin{itemize}
\tightlist
\item
  Interaction terms appear as products between explanatory variables. Needed when one explanatory variable modifies the effect of the other.

  \begin{itemize}
  \tightlist
  \item
    e.g.~Mental Health modifies the relation between Physical Function and Self-Perceived Health
  \end{itemize}
\item
  Therefore, we \textbf{cannot} say an increase in 1 unit of PFI results in a decrease in Self-Perceived Health of 0.29 units, all other variables remaining fixed
\item
  Instead, we talk of the slope of PFI at a given MHI
  e.g.~When MHI=10, self-perceived health changes by -0.29 + 0.005\emph{10 = -0.24 when PFI increases by 1 unit
  e.g.~When MHI=90, self-perceived health changes by -0.29 + 0.005}90 = 0.16 when PFI increases by 1 unit
\end{itemize}

\hypertarget{model-checking}{%
\subsection{Model checking}\label{model-checking}}

\begin{itemize}
\tightlist
\item
  Once again, we use plots and statistics based on the residuals, i.e.~Observed -- Predicted values
\item
  Your software program may produce the following statistics:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Variance Inflation Factor (VIF)}: Values greater than 10 indicate multicollinearity
  \item
    \textbf{DFFITS}: Indicates how much an observation influences predicted values
  \item
    \textbf{DFBETAS}: Indicates how much an observation influences the regression coefficients
  \item
    \textbf{Cook's distance}: Detects multivariate outliers that may not show up on scatter plot of a single explanatory variable. Values greater than 1 are problematic.
  \end{itemize}
\end{itemize}

\hypertarget{residuals-vs.-fitted-values-for-our-example}{%
\subsection{Residuals vs.~Fitted values for our example}\label{residuals-vs.-fitted-values-for-our-example}}

\includegraphics[width=1\linewidth]{./12_66}

\hypertarget{standardized-residuals-vs.-fitted-values}{%
\subsection{Standardized Residuals vs.~Fitted Values}\label{standardized-residuals-vs.-fitted-values}}

\includegraphics[width=1\linewidth]{./12_67}

\hypertarget{qq-normal-plot-of-standardized-residuals}{%
\subsection{QQ (normal)-plot of standardized residuals}\label{qq-normal-plot-of-standardized-residuals}}

\includegraphics[width=0.3\linewidth]{./12_68}

\hypertarget{vif-for-our-example}{%
\subsection{VIF for our example}\label{vif-for-our-example}}

\begin{verbatim}
> vif(m2)
      PFI                MHI      s3tmwt    gender      PFI:MHI 
15.756764  2.997654  2.437211  1.105772 19.009896
\end{verbatim}

\begin{itemize}
\tightlist
\item
  A remedy for multicollinearity, particularly for models with interaction or quadratic terms, is to centre the explanatory variables. For our example this helps:
\end{itemize}

\begin{verbatim}
> vif(m2)
     PFI      MHI a$s3tmwt a$gender  PFI:MHI 
2.711766 1.248527 2.437211 1.105772 1.066550 
\end{verbatim}

\hypertarget{model-with-centred-pfi-and-mhi-notice-change-in-slopes-of-these-variables}{%
\subsection{Model with centred PFI and MHI (notice change in slopes of these variables!)}\label{model-with-centred-pfi-and-mhi-notice-change-in-slopes-of-these-variables}}

\begin{verbatim}
Call:
lm(formula = a$eqtherm ~ PFI + MHI + PFI * MHI + a$s3tmwt + a$gender, 
    subset = -155)

Residuals:
    Min      1Q  Median      3Q     Max 
-54.524 -12.119   2.276  10.261  39.790 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 69.403873   3.100819  22.382  < 2e-16 ***
PFI          0.079324   0.058679   1.352  0.17793    
MHI          0.270968   0.059443   4.558  8.9e-06 ***
a$s3tmwt     0.024943   0.030055   0.830  0.40757    
a$gender    -2.797760   2.461413  -1.137  0.25703    
PFI:MHI      0.005343   0.001757   3.040  0.00267 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 16.49 on 203 degrees of freedom
  (25 observations deleted due to missingness)
Multiple R-squared: 0.2171,     Adjusted R-squared: 0.1978 
F-statistic: 11.26 on 5 and 203 DF,  p-value: 1.358e-09 
\end{verbatim}

NOTE: Residual plots were not much affected. Results not shown.

\hypertarget{results-of-multiple-linear-regression-model-before-and-after-centering}{%
\subsection{\texorpdfstring{Results\(^*\) of multiple linear regression model before and after centering}{Results\^{}* of multiple linear regression model before and after centering}}\label{results-of-multiple-linear-regression-model-before-and-after-centering}}

\begin{tabular}{l|l|l}
\hline
Variable & Multiple Linear Regression before centering & Multiple Linear Regression after centering\\
\hline
Physical Function & -0.29 (-0.57, -0.12) & 0.08 (-0.04, 0.19)\\
\hline
Mental Health & 0.008 (-0.17, 0.19) & 0.27 (0.15, 0.39)\\
\hline
Walking Capacity & 0.02 (-0.03, 0.1) & 0.02 (-0.03, 0.1)\\
\hline
Gender & -2.8 (-7.7, 2.1) & -2.8 (-7.7, 2.1)\\
\hline
Stroke Severity & -- & --\\
\hline
Physical Function \$\textasciicircum{}*\$ Mental Health & 0.005 (0, 0.008) & 0.005 (0.002, 0.009)\\
\hline
\end{tabular}

\(^*\)Slope and 95\% confidence interval

\hypertarget{revised-interpretation-of-interaction-terms}{%
\subsection{Revised interpretation of interaction terms}\label{revised-interpretation-of-interaction-terms}}

\begin{itemize}
\tightlist
\item
  e.g.~When MHI=10, self-perceived health changes by 0.079324 + (10-mean(MHI))*0.005 = -0.24 when PFI increases by 1 unit
\item
  e.g.~When MHI=90, self-perceived health changes 0.079324 + (90-mean(MHI))*0.005 = 0.19 when PFI increases by 1 unit
\item
  Notice that our interpretation has not changed much, though the regression coefficients did change.
\item
  Multicollinearity does not affect predictions made by the model or \(R^2\) but it can bias the standard errors of regression coefficients, affecting p-values and CIs
\end{itemize}

\hypertarget{interpretation-of-regression-coefficients-in-a-model-with-centred-explanatory-variables}{%
\subsection{Interpretation of regression coefficients in a model with centred explanatory variables}\label{interpretation-of-regression-coefficients-in-a-model-with-centred-explanatory-variables}}

\begin{itemize}
\tightlist
\item
  \(Y=b0+b1^*(X1-\overline{X1})+b2^*(X2-\overline{X2})+b3^*(X1-\overline{X1})^*(X2-\overline{X2})+b4^*X3\)\\
  \(=b0+(b1+b3^*(X2-\overline{X2}))^*X1+b2^*(X2-\overline{X2})+b4^*X3-b1^*\overline{X1}\)\\
  \(=b0+(b2+b3^*(X1-\overline{X1}))^*X2+b1^*(X1-\overline{X1})+b4^*X3-b2^*\overline{X2}\)
\end{itemize}

Change in Y for unit change in X3 = b4

Change in Y for unit change in \(X1 = b1 + b3^*(X2-\overline{X1})\)

Change in Y for unit change in \(X2 = b2 + b3^*(X1-\overline{X2})\)

Notice that when X2 is at its mean value, the slope of X1 is exactly equal to b1. Similarly, when X1 is at its mean value, the slope of X2 is b2.

\hypertarget{inference-for-slopes}{%
\subsection{Inference for slopes}\label{inference-for-slopes}}

\begin{itemize}
\tightlist
\item
  As in the case of simple linear regression, we can estimate the standard error of each regression coefficients.
\item
  For those variables that are not involved in an interaction term, confidence intervals or hypothesis tests can be constructed as we saw previously based on the t-distribution (with n-p-1 degrees of freedom, n=209 \& p=5 in our example)
\item
  For variables that include interaction terms calculations may need to be done by hand
\end{itemize}

\hypertarget{example-inference-for-slope-of-pfi-when-mhi90}{%
\subsection{Example: Inference for slope of PFI when MHI=90}\label{example-inference-for-slope-of-pfi-when-mhi90}}

\begin{itemize}
\tightlist
\item
  Obtain the variance-covariance matrix for the regression coefficients
\item
  se = Stderror of slope of PFI when MHI = 90\\
  \(= sqrt(variance(b_{PFI}) + variance((90-mean(MHI))*b_{PFI:MHI})\)\\
  \(+2*covariance(b_{PFI},(90-mean(MHI))*b_{PFI:MHI}))\)\\
  \(= sqrt(3.443211e-03 +(90-mean(MHI))^2* 3.088759e-06\)\\
  \(+2*(90-mean(MHI)) * (-1.203307e-05))\)\\
  \(= 0.06522543\)\\
  (Note that the \(variance(b_{PFI})\) can also be obtained as the square of its standard error)
\end{itemize}

Where:\\
\(b_{PFI}=\) regression coefficient of PFI\\
\(b_{PFI:MHI} =\) regression coefficient of interaction term

The 95\% CI is given by \(b_{PFI} + (90-mean(MHI))*b_{PFI:MHI}± t_{203,1-α/2}se\)\\
= (0.06, 0.31)

\hypertarget{anova-table-for-multiple-linear-regression-model}{%
\subsection{\texorpdfstring{ANOVA table for Multiple Linear Regression Model\(^*\)}{ANOVA table for Multiple Linear Regression Model\^{}*}}\label{anova-table-for-multiple-linear-regression-model}}

\begin{verbatim}
> summary(aov(b6))
                      Df Sum Sq   Mean Sq F value    Pr(>F)    
PFI                 1   8304      8304.5   30.5534 9.905e-08 ***
MHI               1   4110      4110.1   15.1216 0.0001365 ***
a$s3tmwt     1    139      139.3      0.5126 0.4748392    
a$gender      1    234      234.2      0.8616 0.3543937    
PFI:MHI         1   2513    2512.6    9.2441 0.0026738 ** 
Residuals   203  55176  271.8                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
25 observations deleted due to missingness
\end{verbatim}

\(^*\)sequential analysis of variance table comparable to linear regression models adding each variable one at a time. Hence, PFI is statistically significant here. The F-value is the square of the t-value from a model including PFI as the only covariate. The F-value for MHI is the square of the t-value from a model including both PFI and MHI and so on.

\hypertarget{adjusted-r-squared}{%
\subsection{Adjusted R-squared}\label{adjusted-r-squared}}

\begin{itemize}
\item
  \(R^2\) will increase steadily as p (the number of explanatory variables in the model) increases.\\
  For our example, \(R^2 =1-55175.77/70476 = 0.2171\)
\item
  Therefore, we need to penalize the model fit for variables that do not improve prediction. This is achieved by:

  \(R^2\) adjusted = 1 -- MSE/MSTOT\\
  = 1-(16.49*16.49)/(70476/(208)) = 0.1975,\\
  for our example
\end{itemize}

\(R^2\) adjusted can be negative. Higher values indicate better fit.

  \bibliography{book.bib,packages.bib}

\end{document}
